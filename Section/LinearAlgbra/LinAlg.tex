\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection*{Introduction}
\subsubsection*{Elementary row operation.} 
Rule as follows.
\begin{enumerate}
	\item Interchange two rows
	\item Multiply (or divide) a row by a (nonzero) constant
	\item Add a multiple of one row to another
\end{enumerate}

\subsubsection*{Rank.} 
Definition as follows.
\begin{quote}
	The number of nonzero rows remaining when a matrix has been row reduced is called the rank of the matrix.
\end{quote}
Or:
\begin{quote}
	The order of the largest nonzero determinant is the rank of the matrix.
\end{quote}

Consider $m$ equation with $n$ constants. We define matrix M and A, where M has $m$ rows and $n$ column--which corresponds to $m$ equation and $n$ unknown--while A has $m$ rows and $n+1$--which corresponds to unknown plus the constant. There are few possible cases.
\begin{enumerate}
	\item If (rank M) $<$ (rank A), the equations are inconsistent and there is no solution.
	\item If (rank M) $=$ (rank A) = $n$ (number of unknowns), there is one solution.
	\item If (rank M) = (rank A) = $R < n$, then $R$ unknowns can be found in terms of the remaining $n-R$ unknowns.
\end{enumerate}

\subsubsection*{Cramer's Rule.} The equations
\begin{align*}
	\begin{cases}
		a_1x+b_1y & =c_1 \\
		a_2x+b_2y & =c_2
	\end{cases}
\end{align*}
has the solution:
\begin{align*}
	x=\frac{1}{D}
	\begin{vmatrix}
		c_1 & b_1 \\
		c_2 & b_2
	\end{vmatrix} &  & \mathrm{and} &  &
	y=\frac{1}{D}
	\begin{vmatrix}
		a_1 & c_1 \\
		a_2 & c_2
	\end{vmatrix}
\end{align*}

\subsubsection*{Dot Product.}
\begin{align*}
	\A\cdot\B & =|\A|\B|\cos\theta    \\
	          & =A_xB_x+A_yB_y+A_zB_z
\end{align*}
The following applies if vector perpendicular:
\begin{align*}
	\A\cdot\B & =0
\end{align*}
The following applies if vector parallel:
\begin{align*}
	\frac{A_x}{B_x}=\frac{A_y}{B_y}=\frac{A_z}{B_z}
\end{align*}

\subsubsection*{Orthogonality.} 
Matrix, in context of linear transformation, that preserve the length of vector is said to be orthogonal.
Matrix M is orthogonal if
\begin{align*}
	M^{-1}=M^T
\end{align*}
with determinant
\begin{align*}
	\det M = \pm 1
\end{align*}
det $\text{M} = 1$ corresponds geometrically to a rotation, and det $\text{M} = -1$ means that a reflection is involved.

\subsubsection*{Cross Product.}
\begin{align*}
	\A\times\B & =|\A||\B|\sin\theta  \\
	           & =\det\begin{vmatrix}
		                  \x  & \y  & \z  \\
		                  A_x & A_y & A_z \\
		                  B_x & B_y & B_z
	                  \end{vmatrix}
\end{align*}
The following applies if vector parallel or antiparallel:
\begin{align*}
	\A\cdot\B & =0
\end{align*}

\subsubsection*{Homogeneous equations.} 
The definition is as follows.

\begin{quote}
	Sets of linear equations when the constants on the right-hand sides are all zero are called homogeneous equations.
\end{quote}

Homogeneous equations are never inconsistent; they always have the solution of zero--often called the trivial solution.
If the number of independent equations--that is, the rank of the matrix--is the same as the number of unknowns, this is the only solution.
If the rank of the matrix is less than the number of unknowns, there are infinitely many solutions.

Consider set of $n$ homogeneous equations in $n$ unknowns.
These equations have only the trivial solution unless the rank of the matrix is less than $n$.
This means that at least one row of the row reduced $n$ by $n$ matrix of the coefficients is a zero row.
Which mean that the determinant $D$ of the coefficients is zero.
This fact will be used in eigenvalue problem.

\begin{quote}
	A system of $n$ homogeneous equations in $n$ unknowns has solutions other than the trivial solution if and only if the determinant of the coefficients is zero.
\end{quote}

\subsection*{Vector in Braket Notation}
\subsubsection*{Vector space.}
Linear vector space $\mathbb{V}$ is a collection of vectors $\ket{1},\dots,\ket{n}$ for which there exists definitive rule for addition and multiplication.
Said rules are as follows.
\begin{enumerate}
	\item \textbf{Closure:} $\ket{V}+\ket{W}\in \mathbb{V}$
	\item \textbf{Distributive in the vector}: $a\left(\ket{V}+\ket{W}\right)=a\ket{V}+a \ket{W}$
	\item \textbf{Distributive in the scalar}: $(a+b)\ket{V}=a \ket{V}+b \ket{V}$
	\item \textbf{Associative in the scalar}: $a(b\ket{V}=ab\ket{V})$
	\item \textbf{Commutative in the addition}: $\ket{V}+\ket{W}+\ket{W}+\ket{V}$
	\item \textbf{Associative in the addition}: As follows.
	      \begin{equation*}
		      \ket{V}+\left(\ket{W}+\ket{P}\right)=\left(\ket{V}+\ket{W}\right)+\ket{P}
	      \end{equation*}
	\item \textbf{Null vector}: $\ket{V}+\ket{0}+\ket{V}$
	\item \textbf{Inverse under addition}: $\ket{-V}+\ket{V}=\ket{0}$
\end{enumerate}

Vector space has $n$ dimension if it can accommodate $n$ linear independent vectors.
We denote $\mathbb{V}^n(R)$ if the field--that is the scalar used to scale the vector--and $\mathbb{V}^n(C)$ if it is complex.

\subsubsection*{Dual space.}
Column vectors are concrete manifestations of an abstract vector $\ket{V}$ ket in a basis, while row vector are bra's $\bra{V}$.
They are adjoint of each other.
Thus, there are two vector space: space of KET $\ket{V}$ and dual space of bra $\bra{V}$.

\subsubsection*{Vector expansion in an orthonormal base.}
Suppose we are to expand vector $\ket{V}$ in an  orthonormal base.
First, we take the dot product of said vector with an orthonormal base $\bra{j}$
\begin{equation*}
	\braket{j|V}=\sum_i v_i\braket{j|i}=v_j
\end{equation*}
and obtain the $j$-th component of the vector.
Using this, we then write
\begin{equation*}
	\ket{V}=\sum_i \ket{i}\braket{i|V}
\end{equation*}

\subsection*{Lines and Plane}
Suppose we have vector $\A=a\;\x+b\;\y +c\;\z$ and vector $\mathbf{r}-\mathbf{r_0}=(x-x_0)\;\x+(y-y_0)\;\y +(z-z_0)\;\z$, which parallel to $\A$.
We can write:
\begin{align*}
	\frac{x-x_0}{a}=\frac{y-y_0}{b}=\frac{z-z_0}{c}
\end{align*}
which is the symmetric equations of a straight line.
Note that $\mathbf{r}$ and $\mathbf{r_0}$ is not necessarily parallel with $\A$, but $\mathbf{r}-\mathbf{r_0}$ do.
The parameter equation is:
\begin{align*}
	\mathbf{r}-\mathbf{r_0} & =\A t              \\
	\mathbf{r}              & =\mathbf{r_0}+\A t
\end{align*}
The previous equation is obtained by the dot identity of parallel vector
\begin{align*}
	a(x-x_0)+b(y-y_0)+c(z-z_0)=0
\end{align*}
also called the equation of plane.

\subsection*{Matrix Operation}
\subsubsection*{Multiplication.}
Matrix AB can be multiplied if they are conformable, that is if column A = row B.
Matrix multiplication in index notation is:
\begin{align*}
	(AB)_{ij}=\sum_{k}A_{ik}B_{kj}
\end{align*}
where $i$ denote row and $j$ denote column.
For $2\times2$ matrix
\begin{equation*}
	\begin{bmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{bmatrix}
	\begin{bmatrix}
		b_{11} & b_{12} \\
		b_{21} & b_{22}
	\end{bmatrix}=
	\begin{bmatrix}
		a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
		a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
\end{equation*}

\subsubsection*{Commutator.}
In general, matrix do not commute. We define the commutator of the matrices A and B by
\begin{align*}
	[A, B] = AB - BA
\end{align*}
Two identity involving commutators are
\begin{align*}
	[\Omega,\Lambda\theta] & =\Lambda[\Omega,\theta]+[\Omega,\Lambda]\theta \\
	[\Lambda\Omega,\Theta] & =\Lambda[\Omega,\theta]+[\Lambda,\theta]\Omega
\end{align*}

\subsubsection*{Inverse}
If a matrix has an inverse we say that it is invertible; if it doesn't have an inverse, it is called singular.
\begin{equation*}
	M^{-1}=\frac{1}{\det M}C^{T}
\end{equation*}
where $C_{ij}$ is cofactor of $m_{ij}$ or the checker thing you use on determining determinant.
The inverse of a product follows
\begin{equation*}
	(\Lambda\Omega)^{-1}=\Lambda^{-1}\Omega^{-1}
\end{equation*}
By thin we can obtain the desired result
\begin{equation*}
	(\Omega\Lambda)(\Omega\Lambda)^{-1}=\Omega\Lambda\Lambda^{-1}\Omega^{-1}=\Omega\Omega^{-1}=I
\end{equation*}

\subsubsection*{Derivative (with respect to parameter).}
Consider the function $\theta(\lambda)=e^{\lambda\Omega}$.
Its derivative with respect to parameter $\lambda$ is evaluated in the usual sense
\begin{equation*}
	\frac{d}{d\lambda}\theta(\lambda)=\Omega e^{\lambda\Omega}=e^{\lambda\Omega}\Omega=\theta(\lambda)\Omega
\end{equation*}
The second and third terms holds true because $[\Omega,e^{\Omega}]=0$.
If we are presented with the differential equation with this form, the solution is given by
\begin{equation*}
	\theta(\lambda)=ce^{\Omega \lambda}
\end{equation*}
with $c$ as operator constant of integration.

\paragraph*{Proof.} We can proof the evaluated method by writing the exponential function as an operator then applying the derivative operator
\begin{align*}
	\frac{d }{d\lambda}e^{\lambda\Omega}  & =\frac{d }{d\lambda}
	\begin{bmatrix}
		e^{\lambda\omega_1} &        &                     \\
		                    & \ddots &                     \\
		                    &        & e^{\lambda\omega_n}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\omega_1 e^{\lambda\omega_1} &        &                              \\
		                             & \ddots &                              \\
		                             &        & \omega_n e^{\lambda\omega_n}
	\end{bmatrix} \\
	\frac{d }{d\lambda }e^{\lambda\Omega} & =
	\begin{bmatrix}
		\omega_1 &        &          \\
		         & \ddots &          \\
		         &        & \omega_n \\
	\end{bmatrix}
	\begin{bmatrix}
		\lambda e^{\lambda\omega_1} &        &                             \\
		                            & \ddots &                             \\
		                            &        & \lambda e^{\lambda\omega_n}
	\end{bmatrix}
	=\Omega e^{\lambda \Omega}
\end{align*}
If $\Omega$ is not a hermitian however, we can produce the same result by working using the power series representation
\begin{equation*}
	\frac{d }{d\lambda }e^{\lambda\Omega}=\frac{d }{d\lambda} \sum_{m=0 }^{\infty }\frac{\lambda^n\Omega^n }{n! }= \sum_{m=1 }^{\infty }\frac{n\lambda^{n-1 }\Omega^{n}}{n!}=\Omega\sum_{m=1 }^{\infty}\frac{\lambda^{n-1} \Omega^{n-1}}{(n-1)!}
\end{equation*}
Shifting the index
\begin{equation*}
	\frac{d }{d\lambda }e^{\lambda \Omega}=\Omega\sum_{m=0 }^{\infty}\frac{\lambda^n \Omega^n }{n!}=\Omega e^{\lambda\Omega}
\end{equation*}

\subsection*{Determinant} For $2\times 2$ matrix:
\begin{align*}
	\det A=\begin{vmatrix}
		       a & b \\
		       c & d
	       \end{vmatrix}=ad-bc
\end{align*}

Here are some determinant rule.
\begin{align*}
	\det(kA) & =k^2\det A                      &  & (2\times2) \\
	\det(kA) & =k^3\det A                      &  & (3\times3) \\
	\det(AB) & =\det(BA)=\det(A)\times\det (B)
\end{align*}

\subsubsection*{Minor.}
Minor of element $a_{ij}$ is the determinant of submatrix order $(n-1)$ you get after crossing $i$-th row and $h$-th column from order $n$ matrix.
Consider the matrix
\begin{equation*}
	A=\begin{bmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33} \\
	\end{bmatrix}
\end{equation*}
Each minor is then
\begin{align*}
	M_{11} & = \begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}, \quad
	M_{12} = \begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix}, \quad
	M_{13} = \begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix},         \\
	M_{21} & = \begin{bmatrix} a_{12} & a_{13} \\ a_{32} & a_{33} \end{bmatrix}, \quad
	M_{22} = \begin{bmatrix} a_{11} & a_{13} \\ a_{31} & a_{33} \end{bmatrix}, \quad
	M_{23} = \begin{bmatrix} a_{11} & a_{12} \\ a_{31} & a_{32} \end{bmatrix},         \\
	M_{31} & = \begin{bmatrix} a_{12} & a_{13} \\ a_{22} & a_{23} \end{bmatrix}, \quad
	M_{32} = \begin{bmatrix} a_{11} & a_{13} \\ a_{21} & a_{23} \end{bmatrix}, \quad
	M_{33} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}
\end{align*}

\subsubsection*{Cofactor}
Cofactor is expressed is the minor that includes sign factors
\begin{equation*}
	\text{C}_{ij}=(-1)^{i+j}M_{ij}
\end{equation*}
As a mnemonic, use the sign pattern to check the sign of the cofactor
\begin{equation*}
	(-1)^{i+j}\sim
	\begin{bmatrix}
		+      & -      & +      & \cdots \\
		-      & +      & -      & \cdots \\
		+      & -      & +      & \cdots \\
		\vdots & \vdots & \vdots & \ddots \\
	\end{bmatrix}
\end{equation*}

\subsection*{Special Matrices and Operator}
\subsubsection*{Theorem.}
\begin{enumerate}
	\item $(ABC)^T=C^TB^TA^T$
	\item $(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$
	\item $Tr(ABC)=Tr(BCA)=Tr(CAB)$.  Trace is the sum of main diagonal. It is a theorem that the
	      trace of a product of matrices is not changed by permuting them in cyclic order.
	\item If $H$ is a Hermitian matrix, then $U = e^{iH}$ is a unitary matrix.
\end{enumerate}

\subsubsection*{Table of special matrices.} Consider this.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}llr@{}}
		\toprule
		Definition     & Condition                   \\
		\midrule
		Real           & $A=\bar{A}$                 \\
		Symmetric      & $A=A^T$                     \\
		Antisymmetric  & $A=-A^T$                    \\
		Orthogonal     & $A^{-1}=A^T$                \\
		Pure Imaginary & $A=-\bar{A}$                \\
		Hermitian      & $A=A^{\dagger}$             \\
		Antihermitian  & $A=-A^{\dagger}$            \\
		Unitary        & $A^{-1}=A^{\dagger}$        \\
		Normal         & $AA^{\dagger}=A^{\dagger}A$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Identity matrix.}
A matrix, who also acts as an operator, which leave the operated vector unchanged
\begin{equation*}
	I \ket{V}=\ket{V}\quad \bra{V}I=\bra{V}
\end{equation*}
Its element may be written as
\begin{equation*}
	I_{ij} =\braket{i|I|k}=\braket{i|j}=\delta_{ij}
\end{equation*}
or using the Kronecker delta.
Then $n\times n$, using the Kronecker delta, identity matrix may be written
\begin{equation*}
	I = \begin{bmatrix}
		\delta_{11} & \cdots & \delta_{1n} \\
		\vdots      & \ddots & \vdots      \\
		\delta_{n1} & \cdots & \delta_{nn}
	\end{bmatrix}
\end{equation*}

\subsubsection*{Projection Operator.}
The projection operator is defined as
\begin{equation*}
	\mathbb{P}_i=\ket{i}\bra{i}
\end{equation*}
This operator can be used to write the expansion of vector
\begin{equation*}
	\ket{V}=\sum_i \ket{i}\braket{i|V}=\sum_i \mathbb{P}_i \ket{V}
\end{equation*}
Or the identity matrix, which is called the completeness relation
\begin{equation*}
	{I}=\sum_i \ket{i}\bra{i}=\int_i \ket{x}\bra{i}\;dx=\sum_i \mathbb{P}_i
\end{equation*}
Its action on bra is all the same
\begin{equation*}
	\bra{V}\mathbb{P}_i=\braket{V|i}\bra{i}=v_i^*\bra{i}
\end{equation*}

Projection operator obey
\begin{equation*}
	\mathbb{P}_i\mathbb{P}_j=\delta_{ij}\mathbb{P}_j
\end{equation*}

\subsubsection*{Hermitian and anti Hermitian.}
An operator is called Hermitian fit satisfies $\Omega=\Omega ^\dagger$, in other hand an operator is called anti-Hermitian if $\Omega=-\Omega ^\dagger$.
In the world of linear algebra, Hermitian and anti-Hermitian play the role of pure real and pure imaginary number.
Just like how we can decompose every number into a sum of pure real and pure imaginary
\begin{align*}
	\alpha & =\frac{\alpha+\alpha ^*}{2}+\frac{\alpha-\alpha ^*}{2} \\
	       & =\frac{(a+ib)+(a-ib)}{2}+\frac{(a+ib)-(a-ib)}{2}       \\
	\alpha & =a+ib
\end{align*}
we can decompose every operator into its Hermitian and anti Hermitian
\begin{equation*}
	\Omega=\frac{\Omega+\Omega ^\dagger}{2}+\frac{\Omega-\Omega ^\dagger}{2}
\end{equation*}

\subsubsection*{Transformation matrix.} Matrix that rotate vector $\vec{r}=(x,y)$ into $\vec{R}=(X,Y)$ (in 2D)is
\begin{align*}
	\begin{pmatrix}
		X \\Y
	\end{pmatrix}=
	\begin{pmatrix}
		\cos \theta & -\sin \theta \\
		\sin \theta & \cos \theta
	\end{pmatrix}
	\begin{pmatrix}
		x \\y
	\end{pmatrix}
\end{align*}
and the one that rotate its axis instead
\begin{align*}
	\begin{pmatrix}
		x' \\y'
	\end{pmatrix}=
	\begin{pmatrix}
		\cos \theta  & \sin \theta \\
		-\sin \theta & \cos \theta
	\end{pmatrix}
	\begin{pmatrix}
		x \\y
	\end{pmatrix}
\end{align*}
in 3D
\begin{align*}
	R_x & =\begin{pmatrix}
		       1 & 0          & 0           \\
		       0 & \cos\theta & -\sin\theta \\
		       0 & \sin\theta & \cos\theta
	       \end{pmatrix}   \\
	R_y & =\begin{pmatrix}
		       \cos\theta  & 0 & \sin\theta \\
		       0           & 1 & 0          \\
		       -\sin\theta & 0 & \cos\theta
	       \end{pmatrix}   \\
	R_z & =\begin{pmatrix}
		       \cos \theta & -\sin \theta & 0 \\
		       \sin \theta & \cos \theta  & 0 \\
		       0           & 0            & 1
	       \end{pmatrix}
\end{align*}

\subsubsection*{Unitary operator.}
Unitary operator is defined as
\begin{equation*}
	UU ^\dagger=I
\end{equation*}
This definition also implies that $U$ and its Hermitian conjugate are inverse of each.
As a comparison with complex number, Unitary operator is a unit modulus $e^{i\theta}$, just as $e^{i\theta}e^{-i\theta}=1$.

\paragraph*{Inner product.} This theorem state that unitary operator preserves the inner product between the vectors they act on.
This can be proved by considering two vectors
\begin{equation*}
	\ket{V_1}=U \ket{W_1}\qquad \ket{V_2}=U \ket{W_2}
\end{equation*}
Then
\begin{equation*}
	\braket{V_2|V_1}=\bra{W_2}U ^\dagger U \ket{W_1}=\braket{W_2|W_2}
\end{equation*}
Unitary operator is the generalization of rotation operator from $\mathbb{V}^3(R)$ to $\mathbb{V}^n(C)$, for its preserves the inner product of the vector like the rotation matrix.

For $n\times n$ unitary operator, the column, or the row really, can be seen as the component of $n$ vector, just like operator in general.
These vector, then, is orthonormal to each other.
The reason for this is that the operator preserve the inner product, the transformed set of vector is also orthonormal.

\subsubsection*{Dirac delta.}
Under the integral sign, Dirac delta is defined as
\begin{equation*}
	\int \delta(x-x')f(x')\;dx=f(x)
\end{equation*}
where it samples the value of function $f(x')$ at one point $x$.
This expression also used to define continuous orthogonal basis
\begin{equation*}
	\braket{x|x'}=\delta(x-x')
\end{equation*}
Dirac delta is an even function proved by
\begin{equation*}
	\delta(x-x')=\braket{x|x'}=\braket{x'|x}^*=\delta(x'-x)^*=\delta(x'-x)
\end{equation*}
Dirac delta is normalized such that
\begin{equation*}
	\int_{-\infty}^{\infty}\delta(x-x')\;dx=1
\end{equation*}
Dirac delta is also defined by the limit of a Gaussian function
\begin{equation*}
	\delta (x-x')=\lim_{\epsilon \rightarrow 0}\frac{1}{\epsilon \sqrt{\pi}}\exp\left[-\frac{(x-x')^2 }{\epsilon^2}\right]
\end{equation*}

The derivative is defined as following
\begin{equation*}
	\delta'(x-x')=\frac{d }{dx}\delta(x-x')=-\frac{d }{dx}\delta(x'-x)
\end{equation*}
The action of this operator is
\begin{equation*}
	\int\delta'(x-x')f(x')\;dx=\frac{d }{dx} f(x)
\end{equation*}
which can be proved by
\begin{equation*}
	\int\delta'(x-x')f(x')\;dx=\frac{d }{dx }\int\delta(x-x')f(x')\;dx=\frac{d }{dx}f(x)
\end{equation*}
In general, the action of $n$-th order of derivative is
\begin{equation*}
	\delta^{(n)}(x-x')=\delta(x-x')\frac{d^n }{dx^n}
\end{equation*}

The integral representation of Dirac delta is
\begin{equation*}
	\delta(x'-x)=\frac{1 }{2\pi} \int_{-\infty}^{\infty} e^{ik(x'-x)}\;dk
\end{equation*}
This is obtained from Fourier transformation of given function
\begin{equation*}
	f(k)=\frac{1 }{\sqrt{2\pi }}\int_{-\infty}^{\infty} e^{-ikx}f(x)\;dx
\end{equation*}
and its inverse
\begin{equation*}
	f(x')=\frac{1 }{\sqrt{2\pi }}\int_{-\infty}^{\infty} e^{ikx'}f(k)\;dk
\end{equation*}
Substituting the transformation into the inverse
\begin{equation*}
	f(x')=\int_{-\infty}^{\infty} \left(\frac{1 }{2\pi}\int_{-\infty}^{\infty} e^{ik(x'-x)\;dk}\right)f(x)\;dx
\end{equation*}
The term inside parenthesis is the Dirac delta function.

\subsubsection*{Differential operator.}
The action of this operator is described by the following equation
\begin{equation*}
	\braket{x|D|f}=\left\langle x\bigg| \frac{df }{dx}\right\rangle=\frac{df(x)}{dx}
\end{equation*}

The operator also can act as the differential Dirac delta function under different basis
\begin{equation*}
	D_{xx'}=\braket{x|D|x'}=\delta'(x-x')=\delta(x-x')\frac{d }{dx'}
\end{equation*}
This can be seen by inserting the completion identity
\begin{equation*}
	\int \braket{x|D|x'}\braket{x'|f}\;dx'=\int D_{xx'}f(x')dx'=\frac{df(x)}{dx}
\end{equation*}
and comparing it with the action of the derivative of the Dirac delta
\begin{equation*}
	\int\delta'(x-x')f(x')\;dx=\frac{d }{dx} f(x)
\end{equation*}

The differential operator is not a Hermitian.
For $D$ to be a Hermitian, it must satisfy
\begin{equation*}
	D_{xx'}=D_{x'x}^\dagger\qquad\text{or}\qquad \braket{x|D|x'}=\left(\braket{x'|D|x}\right)^\dagger
\end{equation*}
The proof is as following
\begin{equation*}
	\braket{x|D|x'}=\delta'(x-x')
\end{equation*}
and
\begin{equation*}
	\left(\braket{x'|D|x}\right)^\dagger=\delta'(x'-x)^*=\delta'(x'-x)-\delta'(x-x')
\end{equation*}

\subsubsection*{Wave number operator.}
On the $x$ basis, $K$ acts as differential operator.
This is misleading, actually. 
The correct term is, if $K$ acts on a state $\ket{\psi}$, and then we project onto the position basis $\ket{x}$, the resulting expression is given by a differential operator acting on the projected state
\begin{equation*}
	\braket{x|K|\psi}=-i \frac{d }{dx}\psi(x)
\end{equation*}
On the other hand, $K$ acts multiplicatively at the $k$ basis
\begin{equation*}
	K\ket{k}=k \ket{k}
\end{equation*}
since $\ket{k}$ is an eigenstate of $K$.

Just like the differential operator, the wave number operator maybe expressed in terms of Dirac delta function
\begin{equation*}
	K_{xx'}=\braket{x|K|x'}=-i\delta'(x-x')
\end{equation*}
which is obtained the following expression with the definition of the Dirac delta function
\begin{equation*}
	\int \braket{x|K|x'}\braket{x'|f}\;dx'=\int K_{xx'}f(x')dx'=-i\frac{df(x)}{dx}
\end{equation*}

Let $\ket{f}$ and $\ket{g}$ be two kets in function space within $[a,b]$
Suppose we transform $\ket{f}$ by $K$ and project it onto $\bra{g}$.
In this basis, $K$ is a Hermitian if the surface term vanishes
\begin{equation*}
	-ig ^*(x)f(x)\bigg|_{a}^b=0
\end{equation*}
The expression is obtained by considering the Hermitian requirement
\begin{equation*}
	\braket{g|K|f}=\left(\braket{f|K|g}\right)^\dagger
\end{equation*}
For $K$ to be a Hermitian, it must also obey
\begin{multline*}
	\int_{a }^{b }\int_{ a }^{b }\braket{g|x}\braket{x|K|x'}\braket{x'|f}\;dx\;dx'
	=\\
	\left(	\int_{a }^{b }\int_{ a }^{b }\braket{f|x}\braket{x|K|x'}\braket{x'|g}\;dx\;dx'\right)^\dagger
\end{multline*}
Rewriting the left-hand side and using integration by part
\begin{align*}
	\int_{a}^{b}\int_{a }^{b}g ^*(x)K_{xx'}f(x')\;dx\;dx' & =\int_{a}^b g ^*(x)\left[-i \frac{d }{dx}f(x)\right]dx                 \\
	                                                      & =i\int_{a }^{b }f(x) \frac{d }{dx}g ^*(x)\;dx-ig ^*(x)f(x)\bigg|_{a}^b
\end{align*}
We are then equating it with the right-hand side, which we write as
\begin{align*}
	\left[\int_{a }^{b}\int_{a }^{b}f ^*(x)K_{xx'}f(x')\;dx\;dx'\right]^\dagger & =
	\left[\int_{a }^{b }\int_{a}^{b}f ^*(x)\left(-i \frac{d }{dx}g(x)\right)\;dx\right]^\dagger                                \\
	                                                                            & =i\int_{a }^{b }f(x)\frac{d }{dx}g ^*(x)\;dx
\end{align*}
Clearly, both side are equal if the surface term is zero.

The basis of $k$ has the form of 
\begin{equation*}
	\psi_k(x)=\frac{1 }{\sqrt{2\pi}}e^{ikx}
\end{equation*}
where $\psi_k(x)\equiv \braket{k|\psi}$ position representation of the abstract momentum eigenbasis $\ket{k}$.
This expression can be derived by considering the act of $K$ on the $k$ basis and project it on $x$ basis
\begin{align*}
	\braket{x|K|k}&=k\braket{x|k}\\
	\int \braket{x|K|x'}\braket{x'|k}\;dx'k &=\int K_{xx'}k(x')\;dx'=\psi_k(x)\\
	i\frac{d }{dx }\psi_k(x )&=k \psi_k(x)\\
\end{align*}
This differential equation is solved by simple integration
\begin{align*}
	\int \frac{1 }{\psi_k(x)}\;d\psi_k(x)&=\int ik\;dx\\
	\ln \psi_k(x)&=ik+A\\
	\psi_k(x)&=Ae^{ik}
\end{align*}
For continuous function, normalize any function into Dirac delta function.
So
\begin{equation*}
	\int_{-\infty}^{\infty} \psi_k ^*(x) \psi_k' (x)\;dx=\delta(k-k')
\end{equation*}
The integral gives 
\begin{align*}
	\int_{-\infty}^{\infty} A^2e^{i(k-k')x}\;dx=A^2\pi \delta(k-k')
\end{align*}
So the value of $A$ that normalize $\psi_k(x)$ is 
\begin{equation*}
	A=\frac{1 }{2\pi}\qquad\text{so that}\qquad \braket{x|k}=\psi_k(x)=\frac{1 }{\sqrt{2\pi}}e^{ikx}
\end{equation*}
On using this basis, the Fourier transforms is just the passage from one basis $\ket{x}$ to another $\ket{k}$
\begin{align*}
	f(k)&=\braket{k|f}=\int_{-\infty}^{\infty} \braket{k|x}\braket{x|f}\;dx=\frac{1 }{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-ikx} f(x)\;dx\\
	f(x)&=\braket{x|f}=\int_{-\infty}^{\infty} \braket{x|k}\braket{k|f}\;dk=\frac{1 }{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{ikx} f(k)\;dk\\
\end{align*}

\subsubsection*{Position operator.}
The action in $x$ basis is described as follows
\begin{equation*}
	X \ket{x}=x \ket{x}
\end{equation*}
while is the $k$ basis
\begin{equation*}
	\braket{k|X|\psi}=i \frac{d }{dk}\psi(k)
\end{equation*}

\subsection*{Inner Product}
\subsubsection*{Finite dimension.}
The inner product is defined in such way to obey the following requirement.
\begin{enumerate}
	\item \textbf{Skew-symmetry}: $\braket{V|W}=\braket{W|V}^*$
	\item \textbf{Positive semidefinitness}: $\braket{V|V}\geq 0$. If $\ket{V}=0\ket{0}$, then $\braket{V|V}=\ket{0}$
	\item \textbf{Linearity in ket} $\braket{V|(a \ket{W}+b \ket{P})}=a\braket{V|W}+b\braket{V|P}$
\end{enumerate}
In following this axiom, we arrive at the following definition for inner product
\begin{equation*}
	\braket{V|W}=\sum_{i,j}v_i^*w_i\braket{i|j}=\begin{bmatrix}
		v_1^* & \dots & v_n^*
	\end{bmatrix}
	\begin{bmatrix}
		w_1    \\
		\vdots \\
		w_n
	\end{bmatrix}
\end{equation*}

If we think function as element of vector space, two functions can be said to be orthogonal on $(a,b)$ if
\begin{equation*}
	\int_{a}^{b}A(x)B(x)\;dx=0
\end{equation*}

\subsubsection*{Infinite dimension.}
The inner product of two continuous function along some interval is defined as
\begin{equation*}
	\braket{f|g}=\int_{a}^{b}f ^*(x)g(x)\;dx
\end{equation*}
This definition comes after recalling the completeness relation for infinite space
\begin{equation*}
	\braket{f|g}=\bra{f}I \ket{g}=\int_{a }^{b}\braket{f|x}\braket{x|g}\;dx=\int_{a }^{b} f ^*(x)g(x)\;dx
\end{equation*}

\subsection*{Norm}
Norm of a vector is defined as
\begin{equation*}
	|V|\equiv\sqrt{\braket{V|V}}
\end{equation*}
Two vectors with zero inner product are said to be orthogonal.
If said vector has unit norm instead, it is referred as normalized.
Set of basis with orthogonal and normalized condition are called orthonormal basis.

\subsubsection*{Continuous function.}
The norm of a continuous function is defined as
\begin{equation*}
	\int_{a}^{b}A^*(x)A(x)\;dx=N^2
\end{equation*}
We also said the function $N^{-1}A(x)$ to be normalized and has the norm of one.

\subsection*{Adjoint Operation}
The following is a summary on how to perform adjoint in braket notation.
\begin{quotation}
	Reverse the order of all factors and make the substitutions $\Omega\leftrightarrow\Omega^\dagger,\ket{}\leftrightarrow \bra{}, a\leftrightarrow a^*$.
\end{quotation}
From this we also obtain the conjugate expressions
\begin{align*}
	\ket{aV}=a \ket{V}\quad            & \leftrightarrow \quad\bra{aV}=\bra{V}a^*                 \\
	\ket{\Omega V}=\Omega \ket{V}\quad & \leftrightarrow\quad\bra{\Omega V}=\bra{V}\Omega^\dagger
\end{align*}

\subsubsection*{Bra and ket.}
Here is how the method in action.
The following vector is an adjoint of each other
\begin{equation*}
	\ket{V}=\sum_iv_i \ket{i}\qquad \bra{V}=\sum_i \bra{i}v_i^*
\end{equation*}
We can also write it in terms of projection operation by recalling $v_i=\braket{i|V}$ and $v_i^*=\braket{V|i}$
\begin{equation*}
	\ket{V}=\sum_i \ket{i} \braket{i|V}\qquad \bra{V}=\sum_i \braket{V|i}\bra{i}
\end{equation*}

\subsubsection*{Operator.}
The matrix $\Omega^\dagger$--also called Hermitian adjoint--represent transpose conjugate of $\Omega$
\begin{equation*}
	\Omega^{\dagger}=(\Omega^*)^T=(\Omega^T)^*
\end{equation*}

Our general rule of adjoint state
\begin{equation*}
	(\Omega\Lambda)^\dagger=\Lambda ^\dagger \Omega ^\dagger
\end{equation*}
This can however be proved in another method.
Consider $\bra{\Omega \Lambda V}$. First treat $\Omega\Lambda$ as one operator
\begin{equation*}
	\bra{(\Omega\Lambda)V}=\bra{V}(\Omega\Lambda) ^\dagger
\end{equation*}
Then, treat both as separate operator and pull them out of the bra one by one
\begin{equation*}
	\bra{\Omega\Lambda V}=\bra{\Lambda V}\Omega^\dagger=\bra{V}\Lambda ^\dagger \Omega ^\dagger
\end{equation*}
Thus, we have
\begin{equation*}
	(\Omega\Lambda) ^\dagger=\Lambda ^\dagger\Omega ^\dagger
\end{equation*}

\subsubsection*{Equation.}
Suppose we have the equation involving
\begin{equation*}
	a_1 \ket{V_1}=a_2 \ket{V_2}+a_3 \ket{V_3}\braket{V_4|V_5}+a_4\Omega\Lambda \ket{V_6}
\end{equation*}
and we want to take its adjoint
\begin{equation*}
	\bra{V_1}a_1 ^*=\bra{V_2}a_2 ^*+\braket{V_4|V_5}\bra{V_3}a_3 ^*+\bra{V_6}\Lambda ^\dagger\Omega ^\dagger a_4 ^*
\end{equation*}

\subsection*{Gram-Schmidt Theorem}
The Gram-Schmidt procedure is used to convert linearly independent basis $\ket{I},\dots,\ket{N}$ into an orthonormal one $\ket{1},\dots,\ket{n}$.
We begin with the first basis vector and normalize it
\begin{equation*}
	\ket{1}=\frac{\ket{I}}{|I|}=\frac{\ket{I}}{\sqrt{\braket{I|I}}}
\end{equation*}
For our $i$-th basis, we create the projection along all $(i-1)$ vector
\begin{equation*}
	\ket{i'}=\ket{I_i}-\sum_{j=1}^{i-1}\ket{j}\braket{j|I_i}
\end{equation*}
and normalize it to obtain the orthonormal basis
\begin{equation*}
	\ket{i}=\frac{\ket{i'}}{|i'|}
\end{equation*}

\subsubsection*{Three basis.}
Let $\ket{I}, \ket{II}, \ket{III}$ be linearly independent basis.
The first orthonormal vector is
\begin{equation*}
	\ket{1}=\frac{\ket{I}}{|I|}
\end{equation*}
For the second basis
\begin{equation*}
	\ket{2'}=\ket{II}-\ket{1}\braket{1|II}
\end{equation*}
The second term is the projection of $\ket{II}$ along the first orthonormal basis.
By subtracting $\ket{II}$ by this, the only thing that remains is the perpendicular partial.
Then we normalize the vector
\begin{equation*}
	\ket{2}=\frac{\ket{2'}}{|2'|}
\end{equation*}
Same goes for the third basis.
We construct
\begin{equation*}
	\ket{3'} =\ket{II}-\ket{1}\braket{1|III}-\ket{2}\braket{2|III}
\end{equation*}
then normalize it
\begin{equation*}
	\ket{3}=\frac{\ket{3'}}{|3'|}
\end{equation*}

As a sanity check, we see that the projection of $\ket{1}$ along itself is norm
\begin{equation*}
	\braket{1|1}=\frac{I|I }{|I|^2}=1
\end{equation*}
and that the $\ket{2'}$ or $\ket{2}$ along the first is orthogonal
\begin{align*}
	\braket{1|2'} & =\braket{1|II}-\braket{1|1}\braket{1|II}=0 \\
	\braket{1|2}  & =\frac{1|2'}{|I||2'|}=0
\end{align*}

\subsection*{Schwarz Inequality}
Theorem that ensure the magnitude of inner product never exceed the product of vector magnitude
\begin{equation*}
	|\braket{V|W}|\leq|V||W|
\end{equation*}
Another related theorem is the triangle inequality, which state that the length of one side of a triangle is less than or equal to the sum of the lengths of the other two sides
\begin{equation*}
	|V+W|\leq|V|+|W|
\end{equation*}

\subsubsection*{Schwarz inequality proof.}
First we define
\begin{equation*}
	\ket{Z}=\ket{V}-\frac{\braket{W|V }}{|W|^2}\ket{W}
\end{equation*}
Its inner product is
\begin{align*}
	\braket{P|P} & =\bigg\langle V-\frac{\braket{W|V }}{|W|^2} W\bigg|V-\frac{\braket{W|V }}{|W|^2}W\bigg\rangle    \\
	             & =\braket{V|V} -\frac{\braket{W|V }\braket{V|W }}{|W|^2}-\frac{\braket{V|W }\braket{W|V }}{|W|^2} \\
	             & -\frac{\braket{V|W }\braket{W|V }\braket{W|W}}{|W^4|}                                            \\
	             & =\braket{V|V}-\frac{\braket{W|V }\braket{V|W }}{|W|^2}
\end{align*}
According to semidefinitness axiom
\begin{equation*}
	\braket{V|V}\geq \frac{\braket{W|V }\braket{ V|W}}{|W|^2}
\end{equation*}
Multiply by $|W|^2$ and taking the square root to obtain
\begin{align*}
	|V|^2|W|^2 & \geq|\braket{V|W}|^2 \\
	|V||W|     & \geq|\braket{V|W}|
\end{align*}

\subsection*{Subspace}
Given a vector space $\mathbb{V}$, a subset of its elements that form a vector space among themselves is called a subspace.
We will denote a particular subspace $i$ of dimensionality $n$ by $\mathbb{V}_i^n$.

Given two subspaces $\mathbb{V}_i^{n_i}$ and $\mathbb{V}_j^{m_j}$, we define their sum $\mathbb{V}_i^{n_i}\oplus\mathbb{V}_j^{m_j}$ as the set containing
\begin{enumerate}
	\item All vector in the $\mathbb{V}_i^{n_i}$.
	\item All vector in the $\mathbb{V}_j^{m_j}$.
	\item All linear combination of them.
\end{enumerate}

Vector along three spatial dimension is denoted as $\mathbb{V}_3(R)$, while its component is $\mathbb{V}_x^1$ on the $x$ direction and the same convention on other two.
All vector along the $xy$ plane is denoted $\mathbb{V}_{xy}^2$.
Adding the $x$ and $y$ subspace will result in the same subspace $\mathbb{V}_x^1\oplus \mathbb{V}_y^1=\mathbb{V}_{xy}^2$, which makes sense since the plane is a linear combination of two basis.

\subsection*{Linear Independence}
Linear combination of $\A$ and $\B$ means $a\A + b\B$ where $a$ and $b$ are scalars.

\subsubsection*{Vector.}
The vector $\mathbf{r}r = x\;\x+y\;\y+z\;\z$ with tail at the origin is a linear combination of the unit basis vectors $\x,\y,\z$
where $a$ is a scalar.
Set of vectors are said to independent if the only solution to the following equation is trivial
\begin{equation*}
	\sum_i a_i\ket{i}=\ket{0}
\end{equation*}
If the basis of the vector are independent, we can expand into
\begin{equation*}
	\ket{V}=\sum_i v_i \ket{i}
\end{equation*}

A function of a vector, say $f (r)$, is called linear if
\begin{align*}
	f(r_1+r_2)= f(r_1)+ f(r_2) &  & \mathrm{and} &  & f(ar)=a\;f(r)
\end{align*}

\subsubsection*{Operator.}
Linear operator obey the following rule
\begin{align*}
	\Omega \alpha \ket{V}                               & =\alpha\Omega \ket{V}                         \\
	\Omega\left[\alpha \ket{V_1}+\beta \ket{V_2}\right] & =\Omega\alpha \ket{V_1}+\beta\Omega \ket{V_2} \\
	\bra{V}\alpha\Omega                                 & =\bra{V}\Omega\alpha                          \\
	\left[\bra{V_1}\alpha+\bra{V_2}\beta\right]\Omega   & =\alpha \bra{V_1}\Omega +\beta\bra{V_2}\Omega
\end{align*}
$\Omega$ is a linear operator if
\begin{align*}
	\Omega(r_1+r_2)= \Omega(r_1)+ \Omega(r_2) &  & \mathrm{and} &  & \Omega(ar)=a\;\Omega(r)
\end{align*}

\subsubsection*{Complete function.} 
A set of function $f_n(x)$ is said to be complete if any other function $f(x)$ can be expressed as linear combination of them
\begin{equation*}
	f(y)=\sum_{n=1}^{\infty} C_nf(y)
\end{equation*}

\subsubsection*{Linear dependence.} 
$f_1(x), f_2(x),\cdots, f_n(x)$ have derivatives of order $n - 1$, and if the Determinant
\begin{align*}
	W=\begin{vmatrix}
		  f_1(x)       & f_2(x)       & \cdots & f_n(x)       \\
		  f_1'(x)      & f_2'(x)      & \cdots & f_n'(x)      \\
		  \vdots       & \vdots       & \ddots & \vdots       \\
		  f_1^{n-1}(x) & f_2^{n-1}(x) & \cdots & f_n^{n-1}(x) \\
	  \end{vmatrix}\not\equiv0
\end{align*}then the functions are linearly independent.

\subsection*{Eigenvalue Problem}
Consider linear operator $\Omega$ transforming non-trivial $\ket{V}$
\begin{equation*}
	\Omega 	\ket{V}=\ket{W}
\end{equation*}
Each operator has certain ket on which its transformation is simply recalling
\begin{equation*}
	\Omega \ket{V}=\omega \ket{V}
\end{equation*}
This is the eigenvalue equation which state that $\ket{V}$ is an eigenket of $\Omega$ with eigenvalue $\omega$.
We begin to solve the problem by writing the eigenvalue equation as
\begin{equation*}
	(\Omega-\omega I)\ket{V}=\ket{0}
\end{equation*}
Operating $(\Omega-\omega I)^{-1}$ on both side
\begin{equation*}
	\ket{V}=(\Omega-\omega I)^{-1}\ket{0}
\end{equation*}
Any operator acting on null vector can only give null vector, not arbitrary vector $\ket{V}$.
Therefore, our assumption that operator $(\Omega-\omega I)^{-1}$ exist is false.
Recalling the inverse of matrix $M$
\begin{equation*}
	M^{-1}=\frac{1}{\det M}\text{C}^T
\end{equation*}
we see that the condition for non-existent inverse is zero determinant.
Thus, the condition for non-zero eigenvector is
\begin{equation*}
	\det (\Omega-\omega I)^{-1}=0
\end{equation*}
In practice, this equation is enough to determine the eigenvalue and eigenvector.
For theoretical purpose however, we can determine what form does the solution take.
To do so, we project the equation with basis bra $\bra{i}$
\begin{equation*}
	\bra{i}\Omega-\omega I\ket{V}=0
\end{equation*}
or in summation form
\begin{equation*}
	\sum_j (\Omega_{ij}-\omega\delta_{ij})v_j=0
\end{equation*}
Setting the determinant to zero gives us the characteristic equation
\begin{equation*}
	\sum_{m=0}^{n}c_m\omega^{m}=0
\end{equation*}

Once the eigenvalue $\omega$ are found, we can move to the next step of determining the eigenvector.
We do this by substituting the value of $\omega$ into the equation
\begin{equation*}
	(\Omega-\omega I)\ket{V}=\ket{0}
\end{equation*}
Then we will obtain set of equations that the equation must obey.
It is conventional to normalize the eigenvector also.
In the case of degeneracy, we choose the eigenvector such that they are orthogonal to each other.

\subsection*{Propagator}
Propagator is an operator that propagate a state forward in space or time.
It can be used as a solution to a differential equation.
For an equation with the form
\begin{equation*}
	\ket{\ddot{x}}=\Omega \ket{x(t)}
\end{equation*}
have the solution written as
\begin{equation*}
	\ket{x(t)}=U(t)\ket{x(0)}
\end{equation*}
The propagator is constructed as
\begin{equation*}
	U(t)= \sum_i\ket{i} \bra{i}\cos \omega_i t
\end{equation*}
with $\ket{i }$ as the $i$-th eigenvector and $\omega_i$ as the $i$-th eigenvalue.

The steps of solving initial value problem are as follows.
\begin{enumerate}
	\item Consider the equation $\ket{\ddot{\psi}(t)}=\Omega\ket{\psi(0)}$
	\item Solve the eigenvalue of operator $\Omega$
	\item Find its eigenvector
	\item Find the propagator $U(t)$ in terms of the eigenvectors and eigenvalues
	      of $\Omega$
	      \begin{equation*}
		      U(t)= \sum_i\ket{i} \bra{i}\cos \omega_i t
	      \end{equation*}
	\item Write the solution
	      \begin{equation*}
		      \ket{\psi(t)}=U(t)\ket{\psi(0)}
	      \end{equation*}
\end{enumerate}

\subsection*{Function of Operator}
We shall consider the function, such as $e^x$, where $x$ is an operator and determine its meaning.
We restrict ourselves to those function that can be written as power series an operator that are hermitian.

\subsubsection*{Exponential function.}
Has the following form
\begin{equation*}
	e^\Omega=
	\begin{bmatrix}
		e^\omega_1 &        &            \\
		           & \ddots &            \\
		           &        & e^\omega_n
	\end{bmatrix}
\end{equation*}

\paragraph*{Proof.}
By going to the eigenbasis of hermitian operator and raising it to the power of $m$
\begin{equation*}
	\Omega=
	\begin{bmatrix}
		\omega_1 &        &          \\
		         & \ddots &          \\
		         &        & \omega_n
	\end{bmatrix},
	\qquad
	\Omega^m=
	\begin{bmatrix}
		\omega_1^m &        &            \\
		           & \ddots &            \\
		           &        & \omega_n^m
	\end{bmatrix}
\end{equation*}
Inserting this into the series representation of exponential function
\begin{equation*}
	e^\Omega=\sum_{m=0}^{\infty}\frac{\Omega^m}{m!}
\end{equation*}
we have
\begin{align*}
	e^\Omega & =\sum_{m=0}^{\infty}\frac{1}{m!}
	\begin{bmatrix}
		\omega_1^m &        &            \\
		           & \ddots &            \\
		           &        & \omega_n^m
	\end{bmatrix}
	         & =
	\begin{bmatrix}
		\sum_{m=0}^{\infty}\frac{\omega_1^m }{m!} &        &                                          \\
		                                          & \ddots &                                          \\
		                                          &        & \sum_{m=0}^{\infty}\frac{\omega_n^m}{m!}
	\end{bmatrix} \\
	e^\Omega & =
	\begin{bmatrix}
		e^{\omega_1} &        &              \\
		             & \ddots &              \\
		             &        & e^{\omega_n}
	\end{bmatrix}
\end{align*}

\subsubsection*{Geometric series.}
For hermitian $\Omega$
\begin{equation*}
	\sum_{m=0}^{\infty}\Omega^n=(1-\Omega)^{-1}
\end{equation*}

\paragraph*{Proof.}
We write the power series as
\begin{align*}
	f(\Omega) & =\sum_{m=0}^{\infty}
	\begin{bmatrix}
		\omega_1 &        &          \\
		         & \ddots &          \\
		         &        & \omega_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		\sum_{m=0}^{\infty}\omega_1^n &        &                               \\
		                              & \ddots &                               \\
		                              &        & \sum_{m=0}^{\infty}\omega_n^m
	\end{bmatrix} \\
	f(\Omega) & =
	\begin{bmatrix}
		(1-\omega_1)^{-1} &        &                   \\
		                  & \ddots &                   \\
		                  &        & (1-\omega_n)^{-1} \\
	\end{bmatrix}
	=(1-\Omega)^{-1}
\end{align*}

\end{document}