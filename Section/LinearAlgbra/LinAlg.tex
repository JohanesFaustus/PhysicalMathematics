\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection*{Introduction}
\subsubsection*{Elementary row operation.} Rule as follows.
\begin{enumerate}
	\item Interchange two rows
	\item Multiply (or divide) a row by a (nonzero) constant
	\item Add a multiple of one row to another
\end{enumerate}

\subsubsection*{Rank.} Definition as follows.
\begin{quote}
	The number of nonzero rows remaining when a matrix has been row reduced is called the rank of the matrix.
\end{quote}
Or:
\begin{quote}
	The order of the largest nonzero determinant is the rank of the matrix.
\end{quote}

Consider $m$ equation with $n$ constants. We define matrix M and A, where M has $m$ rows and $n$ column--which corresponds to $m$ equation and $n$ unknown--while A has $m$ rows and $n+1$--which corresponds to unknown plus the constant. There are few possible cases.
\begin{enumerate}
	\item If (rank M) $<$ (rank A), the equations are inconsistent and there is no solution.
	\item If (rank M) $=$ (rank A) = $n$ (number of unknowns), there is one solution.
	\item If (rank M) = (rank A) = $R < n$, then $R$ unknowns can be found in terms of the remaining $n-R$ unknowns.
\end{enumerate}

\subsubsection*{Cramer's Rule.} The equations
\begin{align*}
	\begin{cases}
		a_1x+b_1y & =c_1 \\
		a_2x+b_2y & =c_2
	\end{cases}
\end{align*}
has the solution:
\begin{align*}
	x=\frac{1}{D}
	\begin{vmatrix}
		c_1 & b_1 \\
		c_2 & b_2
	\end{vmatrix} &  & \mathrm{and} &  &
	y=\frac{1}{D}
	\begin{vmatrix}
		a_1 & c_1 \\
		a_2 & c_2
	\end{vmatrix}
\end{align*}

\subsubsection*{Dot Product.}
\begin{align*}
	\A\cdot\B & =|\A|\B|\cos\theta    \\
	          & =A_xB_x+A_yB_y+A_zB_z
\end{align*}
The following applies if vector perpendicular:
\begin{align*}
	\A\cdot\B & =0
\end{align*}
The following applies if vector parallel:
\begin{align*}
	\frac{A_x}{B_x}=\frac{A_y}{B_y}=\frac{A_z}{B_z}
\end{align*}

\subsubsection*{Cross Product.}
\begin{align*}
	\A\times\B & =|\A||\B|\sin\theta  \\
	           & =\det\begin{vmatrix}
		                  \x  & \y  & \z  \\
		                  A_x & A_y & A_z \\
		                  B_x & B_y & B_z
	                  \end{vmatrix}
\end{align*}
The following applies if vector parallel or antiparallel:
\begin{align*}
	\A\cdot\B & =0
\end{align*}

\subsection*{Vector in Braket Notation}
\subsubsection*{Vector space.}
Linear vector space $\mathbb{V}$ is a collection of vectors $\ket{1},\dots,\ket{n}$ for which there exists definitive rule for addition and multiplication.
Said rules are as follows.
\begin{enumerate}
	\item \textbf{Closure:} $\ket{V}+\ket{W}\in \mathbb{V}$
	\item \textbf{Distributive in the vector}: $a\left(\ket{V}+\ket{W}\right)=a\ket{V}+a \ket{W}$
	\item \textbf{Distributive in the scalar}: $(a+b)\ket{V}=a \ket{V}+b \ket{V}$
	\item \textbf{Associative in the scalar}: $a(b\ket{V}=ab\ket{V})$
	\item \textbf{Commutative in the addition}: $\ket{V}+\ket{W}+\ket{W}+\ket{V}$
	\item \textbf{Associative in the addition}: As follows.
	      \begin{equation*}
		      \ket{V}+\left(\ket{W}+\ket{P}\right)=\left(\ket{V}+\ket{W}\right)+\ket{P}
	      \end{equation*}
	\item \textbf{Null vector}: $\ket{V}+\ket{0}+\ket{V}$
	\item \textbf{Inverse under addition}: $\ket{-V}+\ket{V}=\ket{0}$
\end{enumerate}

Vector space has $n$ dimension if it can accommodate $n$ linear independent vectors.
We denote $\mathbb{V}^n(R)$ if the field--that is the scalar used to scale the vector--and $\mathbb{^n}(C)$ if it is complex.

\subsubsection*{Dual space.}
Column vectors are concrete manifestations of an abstract vector $\ket{V}$ ket in a basis, while row vector are bra's $\bra{V}$.
They are adjoint of each other.
Thus, there are two vector space: space of KET $\ket{V}$ and dual space of bra $\bra{V}$.

\subsubsection*{Vector expansion in an orthonormal base.}
Suppose we are to expand vector $\ket{V}$ in an  orthonormal base.
First, we take the dot product of said vector with an orthonormal base $\bra{j}$
\begin{equation*}
	\braket{j|V}=\sum_i v_i\braket{j|i}=v_j
\end{equation*}
and obtain the $j$-th component of the vector.
Using this, we then write
\begin{equation*}
	\ket{V}=\sum_i \ket{i}\braket{i|V}
\end{equation*}

\subsection*{Lines and Plane}
Suppose we have vector $\A=a\;\x+b\;\y +c\;\z$ and vector $\mathbf{r}-\mathbf{r_0}=(x-x_0)\;\x+(y-y_0)\;\y +(z-z_0)\;\z$, which parallel to $\A$.
We can write:
\begin{align*}
	\frac{x-x_0}{a}=\frac{y-y_0}{b}=\frac{z-z_0}{c}
\end{align*}
which is the symmetric equations of a straight line.
Note that $\mathbf{r}$ and $\mathbf{r_0}$ is not necessarily parallel with $\A$, but $\mathbf{r}-\mathbf{r_0}$ do.
The parameter equation is:
\begin{align*}
	\mathbf{r}-\mathbf{r_0} & =\A t              \\
	\mathbf{r}              & =\mathbf{r_0}+\A t
\end{align*}
The previous equation is obtained by the dot identity of parallel vector
\begin{align*}
	a(x-x_0)+b(y-y_0)+c(z-z_0)=0
\end{align*}
also called the equation of plane.

\subsection*{Matrix and Operation}
\subsubsection*{Multiplication.}
Matrix AB can be multiplied if they are conformable, that is if column A = row B.
Matrix multiplication in index notation is:
\begin{align*}
	(AB)_{ij}=\sum_{k}A_{ik}B_{kj}
\end{align*}
where $i$ denote row and $j$ denote column.
For $2\times2$ matrix
\begin{equation*}
	\begin{bmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}
	\end{bmatrix}
	\begin{bmatrix}
		b_{11} & b_{12} \\
		b_{21} & b_{22}
	\end{bmatrix}=
	\begin{bmatrix}
		a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
		a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
	\end{bmatrix}
\end{equation*}

\subsubsection*{Commutator.}
In general, matrix do not commute. We define the commutator of the matrices A and B by
\begin{align*}
	[A, B] = AB - BA
\end{align*}
Two identity involving commutators are
\begin{align*}
	[\Omega,\Lambda\theta] & =\Lambda[\Omega,\theta]+[\Omega,\Lambda]\theta \\
	[\Lambda\Omega,\Theta] & =\Lambda[\Omega,\theta]+[\Lambda,\theta]\Omega
\end{align*}

\subsubsection*{Inverse}
If a matrix has an inverse we say that it is invertible; if it doesn't have an inverse, it is called singular.
\begin{equation*}
	M^{-1}=\frac{1}{\det M}C^{T}
\end{equation*}
where $C_{ij}$ is cofactor of $m_{ij}$ or the checker thing you use on determining determinant.
The inverse of a product follows
\begin{equation*}
	(\Lambda\Omega)^{-1}=\Lambda^{-1}\Omega^{-1}
\end{equation*}
By thin we can obtain the desired result
\begin{equation*}
	(\Omega\Lambda)(\Omega\Lambda)^{-1}=\Omega\Lambda\Lambda^{-1}\Omega^{-1}=\Omega\Omega^{-1}=I
\end{equation*}

\subsection*{Determinant} For $2\times 2$ matrix:
\begin{align*}
	\det A=\begin{vmatrix}
		       a & b \\
		       c & d
	       \end{vmatrix}=ad-bc
\end{align*}

Here are some determinant rule.
\begin{align*}
	\det(kA) & =k^2\det A                      &  & (2\times2) \\
	\det(kA) & =k^3\det A                      &  & (3\times3) \\
	\det(AB) & =\det(BA)=\det(A)\times\det (B)
\end{align*}

\subsubsection*{Minor.}
Minor of element $a_{ij}$ is the determinant of submatrix order $(n-1)$ you get after crossing $i$-th row and $h$-th column from order $n$ matrix.
Consider the matrix
\begin{equation*}
	A=\begin{bmatrix}
		a_{11} & a_{12} & a_{13} \\
		a_{21} & a_{22} & a_{23} \\
		a_{31} & a_{32} & a_{33} \\
	\end{bmatrix}
\end{equation*}
Each minor is then
\begin{align*}
	M_{11} & = \begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}, \quad
	M_{12} = \begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix}, \quad
	M_{13} = \begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix},         \\
	M_{21} & = \begin{bmatrix} a_{12} & a_{13} \\ a_{32} & a_{33} \end{bmatrix}, \quad
	M_{22} = \begin{bmatrix} a_{11} & a_{13} \\ a_{31} & a_{33} \end{bmatrix}, \quad
	M_{23} = \begin{bmatrix} a_{11} & a_{12} \\ a_{31} & a_{32} \end{bmatrix},         \\
	M_{31} & = \begin{bmatrix} a_{12} & a_{13} \\ a_{22} & a_{23} \end{bmatrix}, \quad
	M_{32} = \begin{bmatrix} a_{11} & a_{13} \\ a_{21} & a_{23} \end{bmatrix}, \quad
	M_{33} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}
\end{align*}

\subsubsection*{Cofactor}
Cofactor is expressed is the minor that includes sign factors
\begin{equation*}
	\text{C}_{ij}=(-1)^{i+j}M_{ij}
\end{equation*}
As a mnemonic, use the sign pattern to check the sign of the cofactor
\begin{equation*}
	(-1)^{i+j}\sim
	\begin{bmatrix}
		+      & -      & +      & \cdots \\
		-      & +      & -      & \cdots \\
		+      & -      & +      & \cdots \\
		\vdots & \vdots & \vdots & \ddots \\
	\end{bmatrix}
\end{equation*}

\subsection*{Linear Algebra}
\subsubsection*{Orthogonality.} Matrix, in context of linear transformation, that preserve the length of vector is said to be orthogonal.
Matrix M is orthogonal if
\begin{align*}
	M^{-1}=M^T
\end{align*}
with determinant
\begin{align*}
	\det M = \pm 1
\end{align*}
det $\text{M} = 1$ corresponds geometrically to a rotation, and det $\text{M} = -1$ means that a reflection is involved.

\subsubsection*{Transformation matrix.} Matrix that rotate vector $\vec{r}=(x,y)$ into $\vec{R}=(X,Y)$ (in 2D)is
\begin{align*}
	\begin{pmatrix}
		X \\Y
	\end{pmatrix}=
	\begin{pmatrix}
		\cos \theta & -\sin \theta \\
		\sin \theta & \cos \theta
	\end{pmatrix}
	\begin{pmatrix}
		x \\y
	\end{pmatrix}
\end{align*}
and the one that rotate its axis instead
\begin{align*}
	\begin{pmatrix}
		x' \\y'
	\end{pmatrix}=
	\begin{pmatrix}
		\cos \theta  & \sin \theta \\
		-\sin \theta & \cos \theta
	\end{pmatrix}
	\begin{pmatrix}
		x \\y
	\end{pmatrix}
\end{align*}
in 3D
\begin{align*}
	R_x & =\begin{pmatrix}
		       1 & 0          & 0           \\
		       0 & \cos\theta & -\sin\theta \\
		       0 & \sin\theta & \cos\theta
	       \end{pmatrix}   \\
	R_y & =\begin{pmatrix}
		       \cos\theta  & 0 & \sin\theta \\
		       0           & 1 & 0          \\
		       -\sin\theta & 0 & \cos\theta
	       \end{pmatrix}   \\
	R_z & =\begin{pmatrix}
		       \cos \theta & -\sin \theta & 0 \\
		       \sin \theta & \cos \theta  & 0 \\
		       0           & 0            & 1
	       \end{pmatrix}
\end{align*}

\subsubsection*{Homogeneous equations.} The definition is as follows.

\begin{quote}
	Sets of linear equations when the constants on the right-hand sides are all zero are called homogeneous equations.
\end{quote}

Homogeneous equations are never inconsistent; they always have the solution of zero--often called the trivial solution.
If the number of independent equations--that is, the rank of the matrix--is the same as the number of unknowns, this is the only solution.
If the rank of the matrix is less than the number of unknowns, there are infinitely many solutions.

Consider set of $n$ homogeneous equations in $n$ unknowns.
These equations have only the trivial solution unless the rank of the matrix is less than $n$.
This means that at least one row of the row reduced $n$ by $n$ matrix of the coefficients is a zero row.
Which mean that the determinant $D$ of the coefficients is zero.
This fact will be used in eigenvalue problem.

\begin{quote}
	A system of $n$ homogeneous equations in $n$ unknowns has solutions other than the trivial solution if and only if the determinant of the coefficients is zero.
\end{quote}

\subsection*{Special Matrices}
\subsubsection*{Theorem.}
\begin{enumerate}
	\item $(ABC)^T=C^TB^TA^T$
	\item $(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$
	\item $Tr(ABC)=Tr(BCA)=Tr(CAB)$.  Trace is the sum of main diagonal. It is a theorem that the
	      trace of a product of matrices is not changed by permuting them in cyclic order.
	\item If $H$ is a Hermitian matrix, then $U = e^{iH}$ is a unitary matrix.
\end{enumerate}

\subsubsection*{Table of special matrices.} Consider this.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}llr@{}}
		\toprule
		Definition     & Condition                   \\
		\midrule
		Real           & $A=\bar{A}$                 \\
		Symmetric      & $A=A^T$                     \\
		Antisymmetric  & $A=-A^T$                    \\
		Orthogonal     & $A^{-1}=A^T$                \\
		Pure Imaginary & $A=-\bar{A}$                \\
		Hermitian      & $A=A^{\dagger}$             \\
		Antihermitian  & $A=-A^{\dagger}$            \\
		Unitary        & $A^{-1}=A^{\dagger}$        \\
		Normal         & $AA^{\dagger}=A^{\dagger}A$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection*{Identity matrix.}
A matrix, who also acts as an operator, which leave the operated vector unchanged
\begin{equation*}
	I \ket{V}=\ket{V}\quad \bra{V}I=\bra{V}
\end{equation*}
Its element may be written as
\begin{equation*}
	I_{ij} \braket{i|I|k}={i|j}=\delta_{ij}
\end{equation*}
or using the Kronecker delta.
Then $n\times n$, using the Kronecker delta, identity matrix may be written
\begin{equation*}
	I = \begin{bmatrix}
		\delta_{11} & \cdots & \delta_{1n} \\
		\vdots      & \ddots & \vdots      \\
		\delta_{n1} & \cdots & \delta_{nn}
	\end{bmatrix}
\end{equation*}

\subsection*{Inner Product}
The inner product is defined in such way to obey the following requirement.
\begin{enumerate}
	\item \textbf{Skew-symmetry}: $\braket{V|W}=\braket{W|V}^*$
	\item \textbf{Positive semidefinitness}: $\braket{V|V}\geq 0$. If $\ket{V}=0\ket{0}$, then $\braket{V|V}=\ket{0}$
	\item \textbf{Linearity in ket} $\braket{V|(a \ket{W}+b \ket{P})}=a\braket{V|W}+b\braket{V|P}$
\end{enumerate}
In following this axiom, we arrive at the following definition for inner product
\begin{equation*}
	\braket{V|W}=\sum_{i,j}v_i^*w_i\braket{i|j}=\begin{bmatrix}
		v_1^* & \dots & v_n^*
	\end{bmatrix}
	\begin{bmatrix}
		w_1    \\
		\vdots \\
		w_n
	\end{bmatrix}
\end{equation*}

If we think function as element of vector space, two functions can be said to be orthogonal on $(a,b)$ if
\begin{equation*}
	\int_{a}^{b}A(x)B(x)\;dx=0
\end{equation*}

\subsection*{Norm}
Norm of a vector is defined as
\begin{equation*}
	|V|\equiv\sqrt{\braket{V|V}}
\end{equation*}
Two vectors with zero inner product are said to be orthogonal.
If said vector has unit norm instead, it is referred as normalized.
Set of basis with orthogonal and normalized condition are called orthonormal basis.

\subsubsection*{Continuous function.}
The norm of a continuous function is defined as
\begin{equation*}
	\int_{a}^{b}A^*(x)A(x)\;dx=N^2
\end{equation*}
We also said the function $N^{-1}A(x)$ to be normalized and has the norm of one.

\subsection*{Adjoint Operation}
The following is a summary on how to perform adjoint in braket notation.
\begin{quotation}
	Reverse the order of all factors and make the substitutions $\Omega\leftrightarrow\Omega^\dagger,\ket{}\leftrightarrow \bra{}, a\leftrightarrow a^*$.
\end{quotation}
From this we also obtain the conjugate expressions
\begin{align*}
	\ket{aV}=a \ket{V}\quad            & \leftrightarrow \quad\bra{aV}=\bra{V}a^*                 \\
	\ket{\Omega V}=\Omega \ket{V}\quad & \leftrightarrow\quad\bra{\Omega V}=\bra{V}\Omega^\dagger
\end{align*}

\subsubsection*{Bra and ket.}
Here is how the method in action.
The following vector is an adjoint of each other
\begin{equation*}
	\ket{V}=\sum_iv_i \ket{i}\qquad \bra{V}=\sum_i \bra{i}v_i^*
\end{equation*}
We can also write it in terms of projection operation by recalling $v_i=\braket{i|V}$ and $v_i^*=\braket{V|i}$
\begin{equation*}
	\ket{V}=\sum_i \ket{i} \braket{i|V}\qquad \bra{V}=\sum_i \braket{V|i}\bra{i}
\end{equation*}

\subsubsection*{Operator.}
The matrix $\Omega^\dagger$--also called Hermitian adjoint--represent transpose conjugate of $\Omega$
\begin{equation*}
	\Omega^{\dagger}=(\Omega^*)^T=(\Omega^T)^*
\end{equation*}

Our general rule of adjoint state
\begin{equation*}
	(\Omega\Lambda)^\dagger=\Lambda ^\dagger \Omega ^\dagger
\end{equation*}
This can however be proved in another method.
Consider $\bra{\Omega \Lambda V}$. First treat $\Omega\Lambda$ as one operator
\begin{equation*}
	\bra{(\Omega\Lambda)V}=\bra{V}(\Omega\Lambda) ^\dagger
\end{equation*}
Then, treat both as separate operator and pull them out of the bra one by one
\begin{equation*}
	\bra{\Omega\Lambda V}=\bra{\Lambda V}\Omega^\dagger=\bra{V}\Lambda ^\dagger \Omega ^\dagger
\end{equation*}
Thus, we have
\begin{equation*}
	(\Omega\Lambda) ^\dagger=\Lambda ^\dagger\Omega ^\dagger
\end{equation*}

\subsubsection*{Equation.}
Suppose we have the equation involving
\begin{equation*}
	a_1 \ket{V_1}=a_2 \ket{V_2}+a_3 \ket{V_3}\braket{V_4|V_5}+a_4\Omega\Lambda \ket{V_6}
\end{equation*}
and we want to take its adjoint
\begin{equation*}
	\bra{V_1}a_1 ^*=\bra{V_2}a_2 ^*+\braket{V_4|V_5}\bra{V_3}a_3 ^*+\bra{V_6}\Lambda ^\dagger\Omega ^\dagger a_4 ^*
\end{equation*}

\subsection*{Gram-Schmidt Theorem}
The Gram-Schmidt procedure is used to convert linearly independent basis $\ket{I},\dots,\ket{N}$ into an orthonormal one $\ket{1},\dots,\ket{n}$.
We begin with the first basis vector and normalize it
\begin{equation*}
	\ket{1}=\frac{\ket{I}}{|I|}=\frac{\ket{I}}{\sqrt{\braket{I|I}}}
\end{equation*}
For our $i$-th basis, we create the projection along all $(i-1)$ vector
\begin{equation*}
	\ket{i'}=\ket{I_i}-\sum_{j=1}^{i-1}\ket{j}\braket{j|I_i}
\end{equation*}
and normalize it to obtain the orthonormal basis
\begin{equation*}
	\ket{i}=\frac{\ket{i'}}{|i'|}
\end{equation*}

\subsubsection*{Three basis.}
Let $\ket{I}, \ket{II}, \ket{III}$ be linearly independent basis.
The first orthonormal vector is
\begin{equation*}
	\ket{1}=\frac{\ket{I}}{|I|}
\end{equation*}
For the second basis
\begin{equation*}
	\ket{2'}=\ket{II}-\ket{1}\braket{1|II}
\end{equation*}
The second term is the projection of $\ket{II}$ along the first orthonormal basis.
By subtracting $\ket{II}$ by this, the only thing that remains is the perpendicular partial.
Then we normalize the vector
\begin{equation*}
	\ket{2}=\frac{\ket{2'}}{|2'|}
\end{equation*}
Same goes for the third basis.
We construct
\begin{equation*}
	\ket{3'} =\ket{II}-\ket{1}\braket{1|III}-\ket{2}\braket{2|III}
\end{equation*}
then normalize it
\begin{equation*}
	\ket{3}=\frac{\ket{3'}}{|3'|}
\end{equation*}

As a sanity check, we see that the projection of $\ket{1}$ along itself is norm
\begin{equation*}
	\braket{1|1}=\frac{I|I }{|I|^2}=1
\end{equation*}
and that the $\ket{2'}$ or $\ket{2}$ along the first is orthogonal
\begin{align*}
	\braket{1|2'} & =\braket{1|II}-\braket{1|1}\braket{1|II}=0 \\
	\braket{1|2}  & =\frac{1|2'}{|I||2'|}=0
\end{align*}

\subsection*{Schwarz Inequality}
Theorem that ensure the magnitude of inner product never exceed the product of vector magnitude
\begin{equation*}
	|\braket{V|W}|\leq|V||W|
\end{equation*}
Another related theorem is the triangle inequality, which state that the length of one side of a triangle is less than or equal to the sum of the lengths of the other two sides
\begin{equation*}
	|V+W|\leq|V|+|W|
\end{equation*}

\subsubsection*{Schwarz inequality proof.}
First we define
\begin{equation*}
	\ket{Z}=\ket{V}-\frac{\braket{W|V }}{|W|^2}\ket{W}
\end{equation*}
Its inner product is
\begin{align*}
	\braket{P|P} & =\bigg\langle V-\frac{\braket{W|V }}{|W|^2} W\bigg|V-\frac{\braket{W|V }}{|W|^2}W\bigg\rangle    \\
	             & =\braket{V|V} -\frac{\braket{W|V }\braket{V|W }}{|W|^2}-\frac{\braket{V|W }\braket{W|V }}{|W|^2} \\
	             & -\frac{\braket{V|W }\braket{W|V }\braket{W|W}}{|W^4|}                                            \\
	             & =\braket{V|V}-\frac{\braket{W|V }\braket{V|W }}{|W|^2}
\end{align*}
According to semidefinitness axiom
\begin{equation*}
	\braket{V|V}\geq \frac{\braket{W|V }\braket{ V|W}}{|W|^2}
\end{equation*}
Multiply by $|W|^2$ and taking the square root to obtain
\begin{align*}
	|V|^2|W|^2 & \geq|\braket{V|W}|^2 \\
	|V||W|     & \geq|\braket{V|W}|
\end{align*}

\subsection*{Subspace}
Given a vector space $\mathbb{V}$, a subset of its elements that form a vector space among themselves is called a subspace.
We will denote a particular subspace $i$ of dimensionality $n$ by $\mathbb{V}_i^n$.

Given two subspaces $\mathbb{V}_i^{n_i}$ and $\mathbb{v}_j^{m_j}$, we define their sum $\mathbb{V}_i^{n_i}\oplus\mathbb{V}_j^{m_j}$ as the set containing
\begin{enumerate}
	\item All vector in the $\mathbb{V}_i^{n_i}$.
	\item All vector in the $\mathbb{V}_j^{m_j}$.
	\item All linear combination of them.
\end{enumerate}

Vector along three spatial dimension is denoted as $\mathbb{V}_3(R)$, while its component is $\mathbb{V}_x^1$ on the $x$ direction and the same convention on other two.
All vector along the $xy$ plane is denoted $\mathbb{V}_{xy}^2$.
Adding the $x$ and $y$ subspace will result in the same subspace $\mathbb{V}_x^1\oplus \mathbb{V}_y^1=\mathbb{V}_{xy}^2$, which makes sense since the plane is a linear combination of two basis.

\subsection*{Linear Independence}
Linear combination of $\A$ and $\B$ means $a\A + b\B$ where $a$ and $b$ are scalars.

\subsubsection*{Vector.}
The vector $\mathbf{r}r = x\;\x+y\;\y+z\;\z$ with tail at the origin is a linear combination of the unit basis vectors $\x,\y,\z$
where $a$ is a scalar.
Set of vectors are said to independent if the only solution to the following equation is trivial
\begin{equation*}
	\sum_i a_i\ket{i}=\ket{0}
\end{equation*}
If the basis of the vector are independent, we can expand into
\begin{equation*}
	\ket{V}=\sum_i v_i \ket{i}
\end{equation*}

A function of a vector, say $f (r)$, is called linear if
\begin{align*}
	f(r_1+r_2)= f(r_1)+ f(r_2) &  & \mathrm{and} &  & f(ar)=a\;f(r)
\end{align*}

\subsubsection*{Operator.}
Linear operator obey the following rule
\begin{align*}
	\Omega \alpha \ket{V}                               & =\alpha\Omega \ket{V}                         \\
	\Omega\left[\alpha \ket{V_1}+\beta \ket{V_2}\right] & =\Omega\alpha \ket{V_1}+\beta\Omega \ket{V_2} \\
	\bra{V}\alpha\Omega                                 & =\bra{V}\Omega\alpha                          \\
	\left[\bra{V_1}\alpha+\bra{V_2}\beta\right]\Omega   & =\alpha \bra{V_1}\Omega +\beta\bra{V_2}\Omega
\end{align*}
$\Omega$ is a linear operator if
\begin{align*}
	\Omega(r_1+r_2)= \Omega(r_1)+ \Omega(r_2) &  & \mathrm{and} &  & \Omega(ar)=a\;\Omega(r)
\end{align*}

\subsubsection*{Complete function.} A set of function $f_n(x)$ is said to be complete if any other function $f(x)$ can be expressed as linear combination of them
\begin{equation*}
	f(y)=\sum_{n=1}^{\infty} C_nf(y)
\end{equation*}
\subsubsection*{Linear dependence}f $f_1(x), f_2(x),\cdots, f_n(x)$ have derivatives of order $n - 1$, and if the Determinant
\begin{align*}
	W=\begin{vmatrix}
		  f_1(x)       & f_2(x)       & \cdots & f_n(x)       \\
		  f_1'(x)      & f_2'(x)      & \cdots & f_n'(x)      \\
		  \vdots       & \vdots       & \ddots & \vdots       \\
		  f_1^{n-1}(x) & f_2^{n-1}(x) & \cdots & f_n^{n-1}(x) \\
	  \end{vmatrix}\not\equiv0
\end{align*}then the functions are linearly independent.

\end{document}