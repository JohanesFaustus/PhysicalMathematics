\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection*{Introduction}
Elementary row operation:
\begin{enumerate}
    \item Interchange two rows 
    \item Multiply (or divide) a row by a (nonzero) constant 
    \item Add a multiple of one row to another
\end{enumerate}

Rank definition:
\begin{definition}[Rank]
    The number of nonzero rows remaining when a matrix has been row reduced is called the rank of the matrix
\end{definition}
\begin{definition}[Rank]
    The order of the largest nonzero determinant is the rank of the matrix. 
\end{definition}

In a M matrix with m equation (rows) and n unknown (collumn), with and A has one more column (the constants):
\begin{enumerate}
    \item If (rank M) $<$ (rank A), the equations are inconsistent and there is no solution.
    \item f (rank M) $=$ (rank A) = n (number of unknowns), there is one solution.
    \item If (rank M) = (rank A) = $R < n$, then R unknowns can be found in terms of the remaining $n-R$ unknowns
\end{enumerate}

Determinant of $2\times 2$ matrix:
\begin{align*}
    \det A=\begin{vmatrix}
        a&b\\
        c&d
    \end{vmatrix}=ad-bc
\end{align*}
Some determinant rule:
\begin{align*}
    \det(kA)&=k^2\det A &&(2\times2)\\
    \det(kA)&=k^3\det A &&(3\times3)\\
    \det(AB)&=\det(BA)=\det(A)\times\det (B)
\end{align*}

Cramer's Rule:\\
the equations
\begin{align*}
    \begin{cases}
        a_1x+b_1y&=c_1\\
        a_2x+b_2y&=c_2
    \end{cases}
\end{align*}
has the solution:
\begin{align*}
    x=\frac{1}{D}
    \begin{vmatrix}
        c_1&b_1\\
        c_2&b_2
    \end{vmatrix}&&\mathrm{and}&&
    y=\frac{1}{D}
    \begin{vmatrix}
        a_1&c_1\\
        a_2&c_2
    \end{vmatrix}
\end{align*}

Scalar Product:
\begin{align*}
    \vec{A}\cdot\vec{B}&=|\vec{A}||\vec{B}|\cos\theta\\
    &=A_xB_x+A_yB_y+A_zB_z
\end{align*}
The following applies if vector perpendicular:
\begin{align*}
    \vec{A}\cdot\vec{B}&=0
\end{align*}
The following applies if vector parallel:
\begin{align*}
    \frac{A_x}{B_x}=\frac{A_y}{B_y}=\frac{A_z}{B_z}
\end{align*}

Vector Product:
\begin{align*}
    \vec{A}\cdot\vec{B}&=|\vec{A}||\vec{B}|\sin\theta\\
    &=\det\begin{vmatrix}
        \hat{i}&\hat{j}&\hat{k}\\
        A_x&A_y&A_z\\
        B_x&B_y&B_z
    \end{vmatrix}
\end{align*}
The following applies if vector parallel or antiparalel:
\begin{align*}
    \vec{A}\cdot\vec{B}&==0
\end{align*}

\subsection*{Lines and Plane}
Suppose we have vector $\vec{A}=a\hat{i}+b\hat{j}$ and vector $\vec{r}-\vec{r_0}=(x-x_0)\hat{i}+(y-y_0)\hat{j}$, which parallel to $\vec{A}$.
We can write:
\begin{align*}
    \frac{x-x_0}{a}=\frac{y-y_0}{b}=\frac{z-z_0}{c}
\end{align*}
which is the symmetric equations of a straight line. 
Note that $\vec{r}$ and $\vec{r_0}$ is not necessarily parallel with $\vec{A}$, but $\vec{r}-\vec{r_0}$ do.
The parameter equation is:
\begin{align*}
    \vec{r}-\vec{r_0}&=\vec{A}t\\
    \vec{r}&=\vec{r_0}+\vec{A}t
\end{align*}
Now suppose that $N=a\hat{i}+b\hat{j}+c\hat{k}$ is perpendicular with $\vec{r}-\vec{r_0}=(x-x_0)\hat{i}+(y-y_0)\hat{j}+(z-z_0)\hat{k}$.
We then have equation of plane:
\begin{align*}
    a(x-x_0)+b(y-y_0)+c(z-z_0)=0
\end{align*}

\subsection*{Matrix Operation}
Matrix AB can be multiplied if thay are conformable, that is if row A = row B. 
Matrix multiplication in index notation is:
\begin{align*}
    (AB)_{ij}=\sum_{k}A_{ik}B_{kj}
\end{align*}
Matrix in general do not commute. We define the commutator of the matrices A and B by
\begin{align*}
    [A, B] = AB - BA
\end{align*}
If a matrix has an inverse we say that it is invertible; if it doesn't have an inverse, it is called singular. 
\begin{align*}
    M^{-1}=\frac{1}{\det M}C^{T}&&\text{where $C_{ij}$ is cofactor of $m_{ij}$}
\end{align*}


\subsection*{Linear Algebra}
Holy shit! He said it! He said the section title! Truly one of the paper ever. 
Anyways, a linear combination of $\vec{A}$ and $\vec{B}$ means $a\vec{A} + b\vec{B}$ where a and b are scalars.
The vector $r = \hat{i}x + \hat{j}y + \hat{k}$z with tail at the origin is a linear combination of the unit basis vectors $\hat{i}, \hat{j}, \hat{k}$
A function of a vector, say $f (r)$, is called linear if
\begin{align*}
    f(r_1+r_2)= f(r_1)+ f(r_2)&&\mathrm{and}&&f(ar)=a\;f(r)
\end{align*}
where a is a scalar. O is a linear operator if
\begin{align*}
    O(r_1+r_2)= O(r_1)+ O(r_2)&&\mathrm{and}&&O(ar)=a\;O(r)
\end{align*}
Derivative, for example, is linear operator, while square root is not.

Matrix, in context of linear tarnsformation, that preserve the length of vector is said to be orthogonal.
Matrix M is orthogonal if
\begin{align*}
    M^{-1}=M^T
\end{align*} 
with determinant
\begin{align*}
    \det M = \pm 1
\end{align*}
det M = 1 corresponds geometrically to a rotation, and det M = -1 means that a reflection is involved.

Matrix that rotate vector $\vec{r}=(x,y)$ into $\vec{R}=(X,Y)$ (in 2D)is
\begin{align*}
    \begin{pmatrix}
        X\\Y
    \end{pmatrix}=
    \begin{pmatrix}
        \cos \theta&-\sin \theta\\
        \sin \theta& \cos \theta
    \end{pmatrix}
    \begin{pmatrix}
        x\\y
    \end{pmatrix}
\end{align*}
and the one that rotate its axis instead
\begin{align*}
    \begin{pmatrix}
        x'\\y'
    \end{pmatrix}=
    \begin{pmatrix}
        \cos \theta&\sin \theta\\
        -\sin \theta& \cos \theta
    \end{pmatrix}
    \begin{pmatrix}
        x\\y
    \end{pmatrix}
\end{align*}
in 3D 
\begin{align*}
    R_x&=\begin{pmatrix}
        1&0&0\\
        0&\cos\theta&-\sin\theta\\
        0&\sin\theta&\cos\theta
    \end{pmatrix}\\
    R_y&=\begin{pmatrix}
        \cos\theta&0&\sin\theta\\
        0&1&0\\
        -\sin\theta&0&\cos\theta
    \end{pmatrix}\\
    R_z&=\begin{pmatrix}
        \cos \theta&-\sin \theta&0\\
        \sin \theta& \cos \theta&0\\
        0&0&1
    \end{pmatrix}
\end{align*}
If $f_1(x), f_2(x),\cdots, f_n(x)$ have derivatives of order $n - 1$, and if the Determinant
\begin{align*}
    W=\begin{vmatrix}
        f_1(x)& f_2(x)&\cdots& f_n(x)\\
        f_1'(x)& f_2'(x)&\cdots& f_n'(x)\\
        \vdots&\vdots&\ddots& \vdots\\
        f_1^{n-1}(x)& f_2^{n-1}(x)&\cdots& f_n^{n-1}(x)\\
    \end{vmatrix}\not\equiv0
\end{align*}then the functions are linearly independent.

Homogeneous Equations
\begin{definition}[Homogeneous Equations]
    Sets of linear equations when the constants on the right hand sides are all zero are called homogeneous equations.
\end{definition}
\begin{quotation}
    Homogeneous equations are never inconsistent; they always have the solution “all unknowns = 0” (often called the “trivial solution”). 
If the number of independent equations (that is, the rank of the matrix) is the same as the number of unknowns, this is the only solution. 
If the rank of the matrix is less than the number of unknowns, there are infinitely many solutions.
\end{quotation}
Consider set of n homogeneous equations in n unknowns.
These equations have only the trivial solution unless the rank of the matrix is less than n. 
This means that at least one row of the row reduced n by n matrix of the coefficients is a zero row. 
Which mean that the determinant D of the coefficients is zero.
This fact will be used in eigenvalue problem.
\begin{quote}
    A system of n homogeneous equations in n unknowns has solutions
other than the trivial solution if and only if the determinant of the
coefficients is zero.
\end{quote}


\subsection*{Special Matrices}
Table of special matrices:
\begin{center}
    \begin{tabular}{ c c  }
     Definition & Condition  \\ 
     \hline
     Real & $A=\bar{A}$  \\  
     Symmetric & $A=A^T$ \\     
     Antisymmetric & $A=-A^T$ \\
     Orthogonal & $A^{-1}=A^T$ \\
     Pure Imaginary & $A=-\bar{A}$ \\
     Hermitian & $A=A^{\dagger}$ \\
     Antihermitian & $A=-A^{\dagger}$\\
     Unitary & $A^{-1}=A^{\dagger}$\\
     Normal & $AA^{\dagger}=A^{\dagger}A$
    \end{tabular}
\end{center}

Few theorem:
\begin{enumerate}
    \item $(ABC)^T=C^TB^TA^T$
    \item $(ABC)^{-1}=C^{-1}B^{-1}A^{-1}$
    \item $Tr(ABC)=Tr(BCA)=Tr(CAB)$.  Trace is the sum of main diagonal.It is a theorem that the
    trace of a product of matrices is not changed by permuting them in cyclic order.
    \item If $H$ is a Hermitian matrix, then $U = e^{iH}$ is a unitary matrix.
\end{enumerate}

\end{document}