\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection*{First Order}
\subsubsection*{First Order ODE.} Written in the form
\begin{equation*}
    y'+P(x)y=Q(x)
\end{equation*}
where $P$ and $Q$ are functions of $x$ has the solution
\begin{align*}
    ye^I&=\int Q e^I\;dx+c\\
    y&=e^{-I}\int Q e^I\;dx+ce^{-I}
\end{align*}
where\begin{equation*}
    I=\int P\;dx
\end{equation*}

\subsubsection*{Bernoulli Equation.} The differential equation
\begin{align*}
    y'+P(x)y=Q(x)y^n
\end{align*}
where $P$ and $Q$ are functions of $x$. It also can be written as
\begin{equation*}
    z'+(1-n)Pz=(1 - n)Q
\end{equation*}
where
\begin{align*}
    z = y^{1-n}
\end{align*}
This is now a first-order linear equation which we can solve as we did the linear equations above. 

\subsubsection*{Exact Equations.}  $P (x, y) dx+Q(x, y)dy$ is an exact differential [the differential of $F (x, y)$, or $P dx + Q dy = dF$] if
\begin{equation*}
    \pardf P=\pardf Q
\end{equation*}
and the solution is
\begin{equation*}
    F (x, y) = \text{constant}    
\end{equation*}
An equation which is not exact may often be made exact by multiplying it by
an appropriate factor.

\subsubsection*{Homogeneous Equations.} A homogeneous function of $x$ and $y$ of degree $n$ means a function which can be written as $x^nf (y/x)$. An equation in the form
\begin{equation*}
    P (x, y) dx + Q(x, y) dy = 0
\end{equation*}
where $P$ and $Q$ are homogeneous functions of the same degree is called homogeneous. Thus,
\begin{equation*}
    y'=\df y=-\frac{P (x, y)}{Q(x, y)}-f(\frac{y}{x})
\end{equation*}
This suggests that we solve homogeneous equations by
making the change of variables
\begin{align*}
    y=xv\quad\text{with}\quad v=\frac{y}{x}
\end{align*}

\subsection*{Second Order}
\subsubsection*{Second Order with Zero Right-Hand Side.} 
Equation of the form
\begin{equation*}
    (D-a)(D-b)y=0,\quad a \neq b
\end{equation*}
has the Solution
\begin{align*}
    y = c_1e^{ax} + c_2e^{bx}
\end{align*}

Equation of the form
\begin{equation*}
    (D-a)(D-a)y=0,\quad a \neq b
\end{equation*}
has the Solution
\begin{align*}
    y = (Ax + B)e^{ax}
\end{align*}

Now suppose the roots of the auxiliary equation are $\alpha \pm i\beta $.
The solution is now
\begin{align*}
    y&=Ae^{(\alpha + i\beta)}x + Be^{(\alpha - i\beta)}x \\
    &=e^{\alpha x}(Ae^{i\beta x} + Be^{-i\beta x})\\
    &= e^{\alpha x}(c_1 \sin \beta x + c_2 \cos \beta x)\\
    &= ce^{\alpha x} \sin(\beta x + \gamma)
\end{align*}
where $\alpha,\;\beta,\;\gamma,\;c,\;c_1,\;c_2$ are different constant.

\subsubsection*{Second Order with Nonzero Right-hand Side.} The equation
\begin{align*}
    a_2\seconddf y+a_1\df y+a_0y=f(x)\\
    \seconddf y+a_1\df y+a_0y=F(x)
\end{align*}
has the solution of the form
\begin{equation*}
    y = y_c + y_p
\end{equation*}
where the complementary function $y_c$ is the general solution of the homogeneous
equation (when right-hand side is equal to zero) and $y_p$ is a particular solution, that is when the right-hand side is equal to $f(x)$ or $F(x)$.  The simplest method solving them is by Inspection and Successive Integration of Two First-Order Equations.

\subsubsection*{Exponential Right-Hand Side.} 
Suppose we have $F (x) = ke^{cx}$, or
\begin{equation*}
    (D - a)(D - b)y =ke^{cx}
\end{equation*}
then, we find a particular solution by
assuming a solution of the form:
\begin{align*}
    y_p=
    \begin{cases}
        Ce^{cx} \quad&\text{if c is not equal to either a or b;}\\
        Cxe^{cx} \quad&\text{if c equals a or b, a  $\neq$ b;}\\
        Cx^2e^{cx} \quad&\text{if c = a = b.}
    \end{cases}
\end{align*}

\subsubsection*{Complex Exponential.} To find a particular solution of
\begin{equation*}
    (D - a)(D - b)y=\begin{cases}
        k\sin \alpha x,\\
        k\cos \alpha x,
    \end{cases}
\end{equation*}
first solve
\begin{equation*}
    (D - a)(D - b)y=ke^{i\alpha x}
\end{equation*}
then take the real or imaginary part. 

\subsubsection*{Method of Undetermined Coefficients.} To find a particular solution of
\begin{equation*}
    (D - a)(D - b)y=e^{cx}P_n(x)
\end{equation*}
where $P_n(x)$ is a polynomial of degree n is
\begin{align*}
    y_p=
    \begin{cases}
        e^{cx}Q_n(x) \quad&\text{if c is not equal to either a or b;}\\
        xe^{cx}Q_n(x) \quad&\text{if c equals a or b, a  $\neq$ b;}\\
        x^2e^{cx}Q_n(x) \quad&\text{if c = a = b.}
    \end{cases}
\end{align*}
where $Q_n(x)$ is a polynomial of the same degree as $P_n(x)$ with undetermined
coefficients to be found to satisfy the given differential equation.

\subsubsection*{Principle of Superposition.} The easiest way of handling a complicated right-hand side: Solve a separate equation for each different exponential and add the solutions. The fact that this is correct for a linear equation is often called the principle of superposition. 

Note that the principle holds only for linear equations.

\subsubsection*{Fourier Series.}
Suppose that the driving force $f(x)$ is periodic, we then can expand the function using Fourier Series. The equation
\begin{equation*}
    a_2\seconddf y+a_1\df y+a_0y=f(x)=\sum_{-\infty}^{\infty}c_ne_{inx}
\end{equation*}
can be solved by solving
\begin{equation*}
    a_2\seconddf y+a_1\df y+a_0y=c_ne_{inx}
\end{equation*}
then add the solutions for all $n$ (applying principle of superposition), and we have the solution of first the equation.

\subsection*{Laplace Transform}
We define $\mathcal{L}(f )$, the Laplace transform of $f (t)$ [also written $F (p)$ since it is a function of $p$], by the equation
\begin{equation*}
    \mathcal{L}(f)=F(P)=\int_{0}^{\infty}f(t)e^{-pt};dt
\end{equation*}

\subsubsection*{Laplace transform 101.} How 2 Laplace transform in 5 steps!
\begin{enumerate}
    \item Transform!
    \item Do algebra!
    \item Inverse!
    \item $\dots$
    \item Profit!
\end{enumerate}

\subsection*{Convolution}
\subsubsection*{Definition.} The integral
\begin{equation*}
    g*h=\int_{0}^{t}g(t-\tau)h(\tau)d(\tau)=\int_{0}^{t}g(\tau)h(t-\tau)d(\tau)
\end{equation*}
is called the convolution of g and h (or the resultant or the Faltung). Now suppose that we have
\begin{equation*}
    Ay' + By' + Cy = f (t), \quad y0 = y'0 = 0
\end{equation*}
take the Laplace transform of each term, substitute the initial conditions, and solve for Y
\begin{equation*}
    Y=\frac{F(p)}{A(p + a)(p + b)}=T(p)F(p)
\end{equation*}
Then $y$ the inverse transform of $Y$ in is the inverse transform of a product of two functions whose inverse transforms we know. Let $G(p)$ and $H(p)$ be the transforms of $g(t)$ and $h(t)$
\begin{equation*}
    G(p)H(p)=\mathcal{L}(g(t)\cdot h(t))=\mathcal{L}(g*h)
\end{equation*} 
Thus
\begin{equation*}
    y=\int_{0}^{t}g(t-\tau)h(\tau)d(\tau)
\end{equation*}
Observe from $\mathcal{L}34$ that we may use either $g(t - \tau )h(\tau )$ or $g(\tau )h(t - \tau )$ in the integral. It is well to choose whichever form is easier to integrate; it is best to put$ (t - \tau )$ in the simpler function.

\subsubsection*{Fourier Transform of a Convolution.} Let $g_1(\alpha)$ and $g_2(\alpha)$ be the Fourier transforms of $f_1(x)$ and $f_2(x)$
\begin{align*}
    g_1(\alpha)\cdot g_2(\alpha)&=\frac{1}{2\pi}\int_{-\infty}^{\infty}f_1(v)e^{-i\alpha v}dv\cdot \frac{1}{2\pi}\int_{-\infty}^{\infty}f_1(u)e^{-i\alpha u}du\\
    &=\biggl(\frac{1}{2\pi}\biggr)^2\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f_1(v)f_2(u)e^{-i\alpha (v+u)}dvdu
\end{align*}
Next we make the change of variables $x = v + u$, $dx = dv$, in the v integral
\begin{align*}
    g_1(\alpha)\cdot g_2(\alpha)&= \biggl(\frac{1}{2\pi}\biggr)^2\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} f_1(x-u)f_2(u)e^{-i\alpha x}dvdu\\
    &= \biggl(\frac{1}{2\pi}\biggr)^2\int_{-\infty}^{\infty}e^{-i\alpha x}\biggl[\int_{-\infty}^{\infty}f_1(x-u)f_2(u)\;du\biggr]dx
\end{align*}
if we define the term in the square parenthesis as convolution, we get 
\begin{align*}
    g_1(\alpha)\cdot g_2(\alpha)&=\frac{1}{2\pi}\biggl(\int_{-\infty}^{\infty}f_1*f_2e^{-i\alpha x}dx\biggr)\\
    &=\frac{1}{2\pi}\cdot\text{Fourier transform of }f_1*f_2
\end{align*}
In other words
\begin{equation*}
    g_1\cdot g_2\text{ and }f_1*f_2\text{ are a pair of Fourier transforms}
\end{equation*}
and by symmetry 
\begin{equation*}
    g_1* g_2\text{ and }f_1\cdot f_2\text{ are a pair of Fourier transforms}
\end{equation*}

\subsection*{Frobenius Method}
By using this method, we assume that the solution has the form of power series
\begin{equation*}
    y=\sum_{n=0}^{\infty} a_nx^{n+s}
\end{equation*}
We also assume that the first coefficient, that is $a_0$, is not zero. Computing the derivative of $y$, we obtain
\begin{align*}
    y&=\sum_{n=0}^{\infty} a_nx^{n+s} \\
    y'&=\sum_{n=0}^{\infty} (n+s) a_nx^{n+s-1}\\
    y''&=\sum_{n=0}^{\infty} (n+s) (n+s-1)a_nx^{n+s-2} 
\end{align*}

\subsubsection*{Frobenius 101.} How 2 solve differential equation using generalized power series in 5 steps!
\begin{enumerate}
    \item Tabulate!
    \item Find the column in terms of $x^{n+s}$ $x^s\rightarrow$!
    \item Factor the coefficients that contain $a_0\rightarrow$ and solve the indicial equation!
    \item Solve it in terms of $a_n=-a_{n-2}$! (not factorial!)
    \item As a check, put $n=2$ at $a_n$ not $n=0$! (also not factorial!)
\end{enumerate}

\subsection*{Bessel Function}
The first kind of Bessel function is written as 
\begin{align*}
    J_p(x)&=\sum_{n=0}^{\infty}\frac{(-1)^n(x/2)^{2n+p}}{\Gamma(n+1)\Gamma(n+p+1)}\\
    J_{-p} (x)&=\sum_{n=0}^{\infty}\frac{(-1)^n(x/2)^{2n-p}}{\Gamma(n+1)\Gamma(n-p+1)}
\end{align*}
While the second kind is 
\begin{equation*}
    N_p(x)=\frac{\cos(\pi p)J_p(x)-J_{-p}(x)}{\sin(\pi p)}
\end{equation*}

The Bessel function is used to solve the Bessel's equation of order $p$
\begin{equation*}
    x^2 y'' + xy' + (x^2- p^2 )y = 0
\end{equation*}
with the solution written as 
\begin{equation*}
    y=AJ_p(x)+BN_p(x)
\end{equation*}
Another form of Bessel's equation is 
\begin{equation*}
    x(xy')' + (K^2x^2- p^2 )y = 0
\end{equation*}
and the solution is 
\begin{equation*}
    y=AJ_p(Kx)+BN_p(Kx)
\end{equation*}
Another equation that can be solved by Bessel function
\begin{equation*}
    y''+ \frac{1-2a}{x}y'+ \left[(bcx^{c-1})^2+ \frac{a^2-p^2c^2}{x^2}\right]
\end{equation*}
The solution is 
\begin{equation*}
    y=x^a Z_p(bx^c)
\end{equation*}
where $a,\;b,\;c,\;p$ are constant and $Z$ denote $J$ or $N$ or any linear combination of them.

\subsubsection*{Derivation.} First we write the Bessel's equation as 
\begin{equation*}
    x(xy')'+ (x^2-p^2)y=0
\end{equation*}
By the Frobenius' method
\begin{align*}
    y&=\sum_{n=0}^{\infty} a_nx^{n+s}\\
    xy'&=\sum_{n=0}^{\infty} a_n(n+s) x^{n+s}\\
    (xy')'&=\sum_{n=0}^{\infty} a_n(n+s)^2 x^{n+s-1}
\end{align*}
and 
\begin{align*}
    x(xy')'&=\sum_{n=0}^{\infty} a_n(n+s)^2 x^{n+s}\\
    x^2y&=\sum_{n=0}^{\infty} a_n x^{n+s+2}\\
    -p^2y&=-\sum_{n=0}^{\infty} a_np^2x^{n+s}
\end{align*}
Tabulate them
\begin{center}
$\begin{array}{c || c c c}
  &x^{n+s}&x^s&x^{s+1}\\
  \hline\hline
  x(xy')' &a_n(n+s)^2 &a_0s^2 &a_1(s+1)^2\\
  x^2y &a_{n-2}&- &-\\
  -p^2y &-a_np^2&-a_0p^2&-a_1p^2
\end{array}$
\end{center}

From this we have the indicial equation
\begin{equation*}
    s^2-p^2=0\implies s=\pm p
\end{equation*}

And the general formula of the coefficient
\begin{equation*}
    a_n=-\frac{a_{n-2}}{(n+s)^2-p^2}
\end{equation*}
For $s=\pm p$ and odd $n$, the coefficient is zero; proved by 
\begin{equation*}
    a_1\left[(s+1)^2-p^2\right]=a_1\left[2p+1\right]=0\implies a_1=0 
\end{equation*} 

We begin first for the case $s=p$. The coefficient is given by 
\begin{equation*}
    a_n=-\frac{a_{n-2}}{(n+p)^2-p^2}=-\frac{a_{n-2}}{n^2-2np}=-\frac{a_{n-2}}{n(n+2p)}
\end{equation*}
For even $n$, we write
\begin{equation*}
    a_{2n}=-\frac{a_{2n-2}}{2n(2n+2p)}= -\frac{a_{2n-2}}{2^2n(n+p)}
\end{equation*}
The coefficients for few odd $n$ are as follows.
\begin{align*}
    a_2&=-\frac{a_0}{2^2(p+1)}=-\frac{a_0\Gamma(p+1)}{2^2\Gamma(p+2)}\\
    a_4&=-\frac{a_2}{2^22(p+2)}=-\frac{a_2\Gamma(p+2)}{2^22\Gamma(p+3)}=\frac{a_0\Gamma(p+1)}{2^42!\Gamma(p+3)}\\
    a_6&=-\frac{a_4}{2^23(p+3)}=-\frac{a_4\Gamma(p+3)}{2^23\Gamma(p+4)}=-\frac{a_0\Gamma(p+1)}{2^63!\Gamma(p+4)}
\end{align*}

The solution is written
\begin{align*}    
    y=&\;\sum_{n=0}^{\infty} a_nx^{n+p}=a_0x^p+a_2x^{p+2}+a_4 x^{p+4}+a_6x^{p+6}\\
    =&\;a_0x^p\Gamma(p+1)\bigg[\frac{1}{\Gamma(p+1)}- \frac{(x/2)^2}{\Gamma(p+2)}+\frac{(x/2)^4}{2!\Gamma(p+3)}\\
    &\quad- \frac{(x/2)^6}{3!\Gamma(p+4)}+\dots\bigg]\\
    =&\;a_02^p\Gamma(p+1)\left(\frac{x}{2}\right)^p\bigg[\frac{1}{\Gamma(1)\Gamma(p+1)}- \frac{(x/2)^2}{\Gamma(2)\Gamma(p+2)}+\frac{(x/2)^4}{\Gamma(3)\Gamma(p+3)}\\
    &\quad- \frac{(x/2)^6}{\Gamma(4)\Gamma(p+4)}+\dots\bigg]
\end{align*}
If we define 
\begin{equation*}
    a_0=\frac{1}{2p\Gamma(p+1)}
\end{equation*}
then the solution, which is defined as $J_p(x)$, is written
\begin{multline*}
    J_p(x)=\frac{(x/2)}{\Gamma(1)\Gamma(p+2)}-\frac{(x/2)^{p+2}}{\Gamma(3)\Gamma(p+3)} +\frac{(x/2)^{p+4}}{\Gamma(3)\Gamma(p+3)} \\
    -\frac{(x/2)^{p+6}}{\Gamma(4)\Gamma(p+4)} +\dots
\end{multline*}
or
\begin{equation*}
    J_p(x)=\sum_{n=0}^{\infty}\frac{(-1)^n(x/2)^{2n+p}}{\Gamma(n+1)\Gamma(n+p+1)}
\end{equation*}

Next we consider the solution for $s=-p$. Since the steps are the same, we only need to change the sign of $p$. The solution is written
\begin{equation*}
    J_{-p} (x)=\sum_{n=0}^{\infty}\frac{(-1)^n(x/2)^{2n-p}}{\Gamma(n+1)\Gamma(n-p+1)}
\end{equation*}

As an aside, for the Bessel equation written in the form 
\begin{equation*}
    x^2y'' +xy' + (K^2x^2- p^2 )y = 0
\end{equation*}
All the terms are unchanged except the term 
\begin{equation*}
    K^2x^2y=\sum_{n=0}^{\infty} a_n K^2x^{n+s+2}
\end{equation*}
This will result the change of argument in the Bessel equation from $Z(x)$ into $Z(Kx)$.

\subsubsection*{Recursion relation.} Here are few relations of Bessel function with its derivative.
\begin{flalign*}
    \begin{aligned}
        &\frac{d}{dx}[x^pJ_p(x)]=x^pJ_{p-1}(x)\\
        &\frac{d}{dx}[x^{-p}J_p(x)]=-x^{-p}_{p+1}(x)\\
        &J_{p-1}(x)+J_{p+1}(x)=\frac{2p}{x}J_p(x)\\
        &J_{p-1}(x)- J_{p+1}(x)=2J'(x)\\
        &J'_p(x)=-\frac{p}{x}J_p(x)+J_{p-1}(x)=\frac{p}{x}J_p(x)-J_{p+1}(x)
    \end{aligned}
\end{flalign*}
And bonus relation that only apply for integral $p$
\begin{equation*}
    J_{-p}(x)=(-1)^pJ_p{x},\quad J_p(-x)=(-1)^nJ_n(x)
\end{equation*}

\subsubsection*{Orthogonality.} Suppose $\alpha$ and $\beta$ are the zeros of the Bessel function order $p$. We can say that the function $\sqrt{x}J_p(\alpha x)$ is orthogonal with itself on $(0,1)$. We can also say that the functions $J_p^2(\alpha x)$ are orthogonal with respect with weight function $x$. Thus, we write
\begin{equation*}
    \int_{0}^{1}xJ_p(\alpha x)J_p(\beta x)\;dx=\begin{cases}
        0&\alpha\neq \beta\\
        \dfrac{1}{2}{J'_p}^2(\alpha)=\dfrac{1}{2}J_{p+1}^2(\alpha) =\dfrac{1}{2}J_{p-1}^2(\alpha)&\alpha=\beta
    \end{cases}
\end{equation*}
We can change the integration limit by subtituting $x=r/a$
\begin{equation*}
    \int_{0}^{a}rJ_p\left(\alpha \frac{r}{a}\right)J_p\left(\beta \frac{r}{a}\right)\;dr=\begin{cases}
        0\\
        \qquad \alpha\neq \beta\\
        \dfrac{a^2}{2}{J'_p}^2(\alpha)=\dfrac{a^2}{2}J_{p+1}^2(\alpha) =\dfrac{a^2}{2}J_{p-1}^2(\alpha)\\
        \qquad\alpha=\beta
    \end{cases}
\end{equation*}

To prove the relation orthoganality of the Bessel function on $(0,1)$ with respect to the weight function $x$, consider the equations 
\begin{align*}
    x(xy')'+(\alpha^2x^2-p^2)y&=0\\
    x(xy')'+(\beta^2x^2-p^2)y&=0
\end{align*}
which are solved by the functions $J_p(\alpha x)$ and $J_p(\beta x)$ respecttively. For breviety's sake, we define $J_p(\alpha x)\equiv u$,  $J_p(\beta x)\equiv v$ and we write
\begin{align*}
    x(xu')'+(\alpha^2x^2-p^2)u&=0\\
    x(xv')'+(\beta^2x^2-p^2)v&=0
\end{align*}
Multiplying the first equation with $u$ and the second with $v$
\begin{align*}
    xv(xu')'+(\alpha^2x^2-p^2)uv&=0\\
    xu(xv')'+(\beta^2x^2-p^2)vu&=0
\end{align*}
Subtracting them 
\begin{align*}
    xv(xu')'-xu(xv')'+(\alpha^2-p^2)x^2uv&=0\\
    v(xu')'-u(xv')'+(\alpha^2-p^2)xuv&=0
\end{align*}
Note that we can write the first two terms as
\begin{align*}
    \frac{d}{dx}\left(vxu'-uxv'\right)&=x'xu'+v(xu)'-u'xv'-u(xv')'\\
    &=v(xu')'-u(xv')'
\end{align*}
On integrating it within $(0,1)$
\begin{equation*}
    \left(vxu'-uxv'\right)\bigg|_0^1+\int_{0}^{1}(\alpha^2-p^2)xuv\;dx=0
\end{equation*}
By the definition
\begin{equation*}
    J_p(\beta)J_p'(\alpha) -J_p(\alpha)j_p'(\beta) +\int_{0}^{1}(\alpha^2-p^2)xJ_p(\alpha)j_p(\beta)\;dx=0
\end{equation*}
where the value at lower limit of those two terms are zero. Since both $\alpha$ and $\beta$ are the zeros of the Bessel functions
\begin{equation*}
    \int_{0}^{1}(\alpha^2-p^2)xJ_p(\alpha)J_p(\beta)\;dx=0
\end{equation*}
If $\alpha\neq\beta$, the terms inside parenthesis are not equal to zero. Hence,
\begin{equation*}
    \int_{0}^{1}xJ_p(\alpha)J_p(\beta)\;dx=0
\end{equation*}
If $\alpha\neq\beta$, the terms inside parenthesis are not equal to zero. Hence, we can simply divide both side by it and the integral is not zero. To finc its value, suppose that $\beta$ is not a zero, unlike $\alpha$. We can write 
\begin{equation*}
    \int_{0}^{1}xJ_p(\alpha)J_p(\beta)\;dx=\frac{\alpha J_p(\beta)J_p(\alpha)}{\beta^2-\alpha^2}
\end{equation*}
Now we let $\beta\rightarrow\alpha$ and evaluate the right term using  L'Hôpital's rule to find 
\begin{equation*}
    \lim_{\beta\rightarrow\alpha}\frac{\alpha J_p(\beta)J_p'(\alpha)}{\beta^2-\alpha^2} =\lim_{\beta\rightarrow\alpha}\frac{\alpha J_p'(\beta)J_p'(\alpha)}{2\beta}=\frac{1}{2}J_p'(\alpha)
\end{equation*}

\subsection*{Laplace's Equation}
Consider scalar function $u$, which may represent gravitational potential in a region containing no mass, the electrostatic potential in a charge-free region, the steady-state temperature in a region containing no sources of heat, or the velocity potential for an incompressible fluid with no vortices and no sources or sinks. Laplace's equation state that 
\begin{equation*}
    \nabla^2 u=0
\end{equation*}

\subsubsection*{Cylindrical domain.} Suppose we evaluate Laplace equation in cylindrical domain. The Laplacian reads
\begin{equation*}
    \nabla^2u= \frac{1}{r}\frac{\partial}{\partial r}\Biggl(r\frac{\partial u}{\partial r}\Biggr)+\frac{1}{r^2} \frac{\partial^2u}{\partial \phi^2}+ \frac{\partial^2u}{\partial z^2}
\end{equation*}
By the separation of variables, we assume the solution of $u=R(r)\Phi(\phi)Z(z)$. Thus, the Laplace's equation now reads
\begin{equation*}
    \frac{\Phi R}{r}\frac{\partial}{\partial r}\Biggl(r\frac{\partial R}{\partial r}\Biggr)
    +\frac{RZ}{r^2} \frac{\partial^2\Phi}{\partial \phi^2}
    + R\Phi\frac{\partial^2Z}{\partial z^2}=0
\end{equation*}
Then we divide by $u=R(r)\Phi(\phi)Z(z)$,
\begin{equation*}
    \frac{1}{Rr}\frac{\partial}{\partial r}\Biggl(r\frac{\partial R}{\partial r}\Biggr)
    +\frac{1}{\Phi r^2} \frac{\partial^2\Phi}{\partial \phi^2}
    + \frac{1}{Z}\frac{\partial^2Z}{\partial z^2}=0
\end{equation*}
Since only last term is the function of $z$ alone, we can safely say that it is a constant. Therefore, we define 
\begin{equation*}
    \frac{1}{Z}\frac{\partial^2Z}{\partial z^2}=K^2\implies Z=Ae^{Kz}+Be^{-KZ}
\end{equation*}
Subtituting this value and multiplying by $r^2$, we have
\begin{equation*}
    \frac{r}{R}\frac{\partial}{\partial r}\Biggl(r\frac{\partial R}{\partial r}\Biggr)
    +\frac{1}{\Phi } \frac{\partial^2\Phi}{\partial \phi^2}
    +K^2r^2=0
\end{equation*}
Now we see that the second term is constant. We define 
\begin{equation*}
    \frac{1}{\Phi}\frac{\partial^2 \Phi}{\partial \phi}=-n^2\implies \Phi=C\sin n\theta +D\cos n\theta
\end{equation*}
where $n$ is an interger. The reason for said separation constant is due to periodicity. For a position in polar coordinate, we denote them as $\theta+2n\pi$. Hence, for a most physical reason, the position $\theta$ and $\theta+2n\pi$ must give the same result, which is possible if the solution is periodic with period of $2\pi$. Finally, we substitute this and multiplying with $R$ to obtain the radial solution
\begin{equation*}
    r\frac{\partial}{\partial r}\Biggl(r\frac{\partial R}{\partial r}\Biggr)
    +(K^2r^2-n^2)R=0
\end{equation*}
This is Bessel function, in particular
\begin{equation*}
    R=\sum_{m=0}^{\infty}E_mJ_m(Kr)+F_mN_m(Kr)
\end{equation*}
Now, Putting it all together, we have the most general solution for the Laplace's equation in cylindrical domain 
\begin{multline*}
    u=\sum_{m=0}^{\infty}\left[A_me^{Kz}+B_me^{-KZ}\right]\left[C_m\sin n\theta +D_m\cos n\theta\right]\\
    \left[E_mJ_m(Kr)+F_mN_m(Kr)\right]
\end{multline*}

\subsection*{Poisson’s Equation}
Consider the same scalar function $u$ as the case of Laplace's equation, however we have a region containing mass, electric charge, or sources of heat or fluid denoted by $f(x,y,z)$. Poisson's equation is written as 
\begin{equation*}
    \nabla^2 u=f(x,y,z)
\end{equation*}

\subsection*{Heat Flow or Diffusion Equation}
Now suppose that the temperature is non-steady. The flow of temperature is governed by the equation 
\begin{equation*}
    \nabla^2 u=\frac{1}{\alpha}\frac{\partial u}{\partial t}
\end{equation*}
where $\alpha$ is a constant called the diﬀusivity.

\subsection*{Wave Equation}
Here $u$ may represent the displacement from equilibrium; in electricity it may be the current or potential along a transmission line; or it may be a component of \textbf{E} or \textbf{B} in an electromagnetic wave. The equation is written as 
\begin{equation*}
    \nabla^2u=\frac{1}{v^2}\frac{\partial^2u}{\partial t^2}
\end{equation*}

\subsection*{Helmholtz Equation}
Helmholtz's equation is the spatial part of either diffusion or wave equation
\begin{equation*}
    \nabla^2F+k^2F=0
\end{equation*}

\subsection*{Schrödinger's Equation}
Also known as the wavefunction equation of quantum mechanics
\begin{equation*}
    -\frac{\hbar^2}{2m}\nabla^2\Psi+V\Psi= i\hbar\frac{\partial \Psi}{\partial t}
\end{equation*}
\end{document}