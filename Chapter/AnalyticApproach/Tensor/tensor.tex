\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Introduction}
\subsubsection{Some mathematical shorthand.} As follows
\begin{flalign*}
       & \begin{array}{ll}
               \mathbb{R}      & \text{The set of real numbers}
               \\
               \mathbb{C}      & \text{The set of complex numbers}
               \\
               \mathbb{Z}      & \text{The set of positive and negative integers}
               \\
               \in             & \text{an element of }
               \\
               \notin          & \text{is not an element of}
               \\
               \forall         & \text{for all}
               \\
               \subset         & \text{is a subset of, a subset of}
               \\
               \equiv          & \text{Denotes a definition}
               \\
               f : A \to B     & \text{Denotes a map } f \text{ the set } A \text{ into set } B
               \\
                               & \text{ (describe the function).}
               \\
               f : a \mapsto b & \text{Map } f \text{ sends the element } a \text{ to the element } b.
               \\
                               & \text{ (describe the action).}
               \\
               \circ           & \text{Denotes a composition of maps}
               \\
               (g \circ f)(a)  & \equiv g(f(a))
               \\
               A \times B      & \text{The set } \{(a, b)\} \text{ of all ordered pairs where } a \in A, b \in B.
               \\
                               & \text{ Referred to as the cartesian product of sets A and B.}
               \\
               \mathbb{R}^n    & \underbrace{\mathbb{R} \times \cdots \times \mathbb{R} }_{n}
               \\
         \end{array} &
\end{flalign*}

\subsubsection{Einstein summation convention.}
The Einstein summation convention states that whenever an index is repeated in an expression, once as a superscript $v^i$ and once as a subscript $e_i$, then summation over that index is implied.
For example
\begin{equation*}
      v = \sum\limits_{i=1}^{n} v^{i} e_{i} \implies v = v^{i} e_{i}
\end{equation*}

Vector $v$ is defined as contravariant vector  due to the components obeying the opposite of the basis vector $e_i$.
Same reason is why the dual vector $f$ is said to be covariant vector: its components transform the same as basis vector
\begin{equation*}
      v^{i'} = A^{i'}_{j} v^{j}\quad
      e^{i} = A^{i}_{j'} e^{j'}    \qquad
      f_{i'} = A^{j}_{i'} f_{j}\quad
      e_{i'} = A_{i'}^{j} e_{j}
\end{equation*}

This is the reason why the indices is defined in such way
\begin{enumerate}
      \item We knew ahead of time that the components of dual vectors would transform like basis vectors, so we gave them both lower indices.
      \item We also knew that the components of vectors would
            transform like dual basis vectors, so we gave them both upper indices
\end{enumerate}

\subsubsection{Dirac notation comparison.}
Consider linear operator $H$, which is $(1,1)$ tensor, on a Hilbert space spanned by orthonormal vectors $\{e_i \}$.
In Dirac notation we would write the basis as $\{\ket{i}\}$, $H(e_i)$ as $H \ket{i}$, $\tilde{e}_j(e_i)=(e_j|e_i)$ as $\braket{j|i}$, and $(e_j|He_i)$ as $\braket{j|H|i}$.
The component of linear component $H$ is thus
\begin{equation*}
      H_i{}^j = H(e_i, e^j) = e^j (H e_i) = \langle j | H | i \rangle
\end{equation*}
In essence, we have
\begin{flalign*}
       & \begin{array}{ll}
               \text{Standard Notation }                                           & \text{Dirac Notation}
               \\
               \text{Basis } \{e_i\}                                               & \{\ket{i}\}
               \\
               \text{Vector } \psi \in \mathcal{H}                                 & |\psi\rangle
               \\
               \text{Dual Vector } L(\psi)                                         & \langle\psi|
               \\
               \text{Inner Product } (\psi, \phi) = \tilde{\psi}(\phi)=(\psi|\phi) & \langle\psi|\phi\rangle
               \\
               A(\psi), A \in \mathcal{L}(\mathcal{H})                             & A|\psi\rangle
               \\
               (\psi, A\phi)                                                       & \langle\psi|A\phi\rangle
               \\
               (1,1) \text{ tensor component }                                     & \langle j | H | i \rangle
               \\
               \quad H_i{}^j = H(e_i, e^j) = e^j (H e_i)                           &
               \\
               T_{ij} e^i \otimes e_j                                              & \sum_{i,j} T_{ij} |j\rangle\langle i|
               \\
               e_i \otimes e_j                                                     & |i\rangle |j\rangle \text{ or } |i,j\rangle
         \end{array} &
\end{flalign*}

\subsection{Definition}
Many older books define a tensor as a collection of objects which carry indices and which 'transform' in a particular way specified by those indices.
Modern  definition takes a tensor to be a function which eats a certain number of vectors (known as the rank $r$ of the tensor) and produces a number.
The distinguishing characteristic of a tensor is a special property called multilinearity
\begin{equation*}
      T(v_1^{i_1} e_{i_1}, ..., v_r^{i_r} e_{i_r}, f_{1j_1} e^{j_1}, ..., f_{sj_s} e^{j_s})
      =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T(e_{i_{1}}, ..., e_{i_{r}}, e^{j_{1}},..., e^{j_{s}}) \\
\end{equation*}
which enables us to express the value of the function on an arbitrary set of $r$ vectors in terms of the values of the function on $r$ basis vectors.
In older treatments, these are usually introduced as components of the tensor.

A tensor of type $(r, s)$ on a vector space V is a $\mathbb{C}$-valued function $T$ on $V^r \times  (V^{*}) ^s$,
\begin{equation*}
      T : \underbrace{V \times \cdots \times V}_{r}
      \times
      \underbrace{V^* \times \cdots \times V^*}_{s}
      \to \mathbb{C}
\end{equation*}
In this definition, vector $v: V \to (V^*)$ and its dual $f:V\to \mathbb{C}$ can be defined as $(0,1)$ and $(1,0)$ tensor respectively.
In other words
\begin{equation*}
      v(f)=f(v)
\end{equation*}
where $v \in V$ and $f \in V^*$.
We also view linear operator as $(1,1)$ tensor $oA(v,f)\equiv f(Av)$ and $(0,0)$ tensor as scalar.

In physics textbooks, tensors (usually of the second rank) are often represented as matrices
It is crucial to keep in mind, though, that this association between a tensor and a matrix depends entirely on a choice of basis, and that matrix $[T]$ is useful mainly as a computational tool, not a conceptual handle.
Tensor $T$ is best thought of abstractly as a multilinear function, and matrix $[T ]$ as its representation in a particular coordinate system.

\subsubsection{Multilinearity and tensor component.}
Linearity implies that dual vectors and linear operators are determined by their values on basis vectors
\begin{equation*}
      v \left(\sum_{i} a_{i} e_{i}\right) = \sum_{i} a_{i} v(e_{i}).
\end{equation*}
As such multilinearity implies the same thing for general tensors.

To see this, let  $\{e_i \}_{i=1,...,n}$ be a basis for $V$ and $\{e^i \}_{i=1,...,n}$ the corresponding dual basis.
We denote $i$-the of vector $v_p$ and $j$-th of the dual $f_q$ as
\begin{equation*}
      v_p  = v_p^i e_i\qquad
      f_q  = f_{pj} e^j
\end{equation*}
Then by repeated application of multilinearity, we have
\begin{align*}
      T(v_{1}, ..., v_{r}, f_{1}, ..., f_{s}) & =  T(v_1^{i_1} e_{i_1}, ..., v_r^{i_r} e_{i_r}, f_{1j_1} e^{j_1}, ..., f_{sj_s} e^{j_s})                        \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T(e_{i_{1}}, ..., e_{i_{r}}, e^{j_{1}},..., e^{j_{s}}) \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T_{i_{1},...,i_{r}}{}^{j_{1}...j_{s}}
\end{align*}
where, as before, the components of $T$ induced $\{e_i \}$ and its dual $\{e^i \}$ is
\begin{equation*}
      T_{i_{1}...i_{r}}{}^{j_{1}...j_{s}} \equiv T(e_{i_{1}},...,e_{i_{r}},e^{j_{1}},...,e^{j_{r}})
\end{equation*}
Here components are defined as the values of the tensor when evaluated on a given set of basis vectors.

\subsection{Tensor Transformation}
Suppose we have basis $\mathcal{B}$ and primed basis $\mathcal{B}$.
The transformation law that govern the change of basis from the primed basis $\mathcal{B}'$ into $\mathcal{B}$ is
\begin{equation*}
      T_{i'_{1}, \dots, i'_{r}}{}^{j'_{1} \dots j'_{s}}
      = A^{k_{1}}_{i'_{1}} \dots A^{k_{r}}_{i'_{r}} A^{l_{1}}_{j'_{1}} \dots A^{l_{s}}_{j'_{s}} T_{k_{1} \dots k_{r}}{}^{l_{1} \dots l_{s}}
\end{equation*}

\subsubsection{Derivation.}
We will derive the transformation law as a consequence of multilinearity, instead of take it as the deÔ¨Ånition of a tensor like in much of the physics literature.
First let us define the basis in both basis as $\mathcal{B}=\{e_i\}_{i=1,\dots,n}$ and $\mathcal{B}'=\{e_{i'}\}_{i=1,\dots,n}$.
With linearity, each basis can be written as
\begin{equation*}
      e_{i} = A_{i}^{j'} e_{j'}\qquad
      e_{i'} = A_{i'}^{j} e_{j}
\end{equation*}
Recall that the prime refer to the basis: unprimed subscript refer to unprimed basis and vice versa.
We then have
\begin{align*}
      e_{i} = A_{i}^{j'} A_{j'}^{k} e_{k} = \delta_{i}^{k} e_{k}
      \\
      e_{i'} = A_{i'}^{j} A_{j}^{k'} e_{k'} = \delta_{i'}^{k'} e_{k'}
\end{align*}
In a way, $A_{i}^{j'}$ and $A_{i'}^{j}$ are inverse of each other.
Note that inverse here means inverse as linear maps under composition
\begin{equation*}
      A : V \rightarrow V\qquad
      A^{-1} : V \rightarrow V
\end{equation*}
and
\begin{equation*}
      A \circ A^{-1} = I \qquad
      A^{-1} \circ A = I
\end{equation*}
with $(A)^{j'}_{i} = A^{j'}_{i}$ and $(A^{-1})^{j'}_{i} = A^{j'}_{i}$.
Of course, in index notation the identity refer to Kronecker delta.

Now define the dual basis $\mathcal{B}=\{e^i\}_{i=1,\dots,n}$ and $\mathcal{B}'=\{e^{i'}\}_{i=1,\dots,n}$.
The component of the basis are
\begin{align*}
      e^{i}(e_{j'}) & =  e^{i}\left( A^{k}_{j'}e_{k} \right)  = A^{k}_{j'} e^{i}\left( e_{k} \right) = A^{k}_{j'} \delta^{i}_{k} = A^{i}_{j'}       \\
      e^{i'}(e_{j}) & =  e^{i'}\left( A^{k'}_{j}e_{k'} \right)  = A^{k'}_{j} e^{i'}\left( e_{k'} \right) = A^{k'}_{j} \delta^{i'}_{k'} = A^{i'}_{j}
\end{align*}
That means
\begin{equation*}
      e^{i'} = A^{i'}_{j} e^{j} \qquad
      e^{i} = A^{i}_{j'} e^{j'}
\end{equation*}
Using repeated multilinearity once again
\begin{align*}
      T_{i'_{1},...,i'_{r}}{}^{j'_{1}...j'_{s}} & =  T(e_{i'_{1}}, ..., e_{i'_{r}}, e^{j'_{1}}, ..., e^{j'_{s}})                                                                             \\
                                                & =  T(A_{i'_{1}}^{k_{1}}e_{k_{1}}, ..., A_{i'_{r}}^{k_{r}}e_{k_{r}}, A_{l_{1}}^{j'_{1}}e^{l_{1}}, ..., A_{l_{s}}^{j'_{s}}e^{l_{s}})         \\
                                                & =  A_{i'_{1}}^{k_{1}}...A_{i'_{r}}^{k_{r}} A_{l_{1}}^{j'_{1}}...A_{l_{s}}^{j'_{s}} T(e_{k_{1}}, ..., e_{k_{r}}, e^{l_{1}}, ..., e^{l_{s}}) \\
                                                & =  A_{i'_{1}}^{k_{1}}...A_{i'_{r}}^{k_{r}} A_{l_{1}}^{j'_{1}}...A_{l_{s}}^{j'_{s}} T_{k_{1}...k_{r}}{}^{l_{1}...l_{s}}
\end{align*}
\end{document}