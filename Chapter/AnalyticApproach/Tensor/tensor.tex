\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Introduction}
\subsubsection{Some mathematical shorthand.} As follows
\begin{flalign*}
       & \begin{array}{ll}
               \mathbb{R}      & \text{The set of real numbers}
               \\
               \mathbb{C}      & \text{The set of complex numbers}
               \\
               \mathbb{Z}      & \text{The set of positive and negative integers}
               \\
               \in             & \text{an element of }
               \\
               \notin          & \text{is not an element of}
               \\
               \forall         & \text{for all}
               \\
               \subset         & \text{is a subset of, a subset of}
               \\
               \equiv          & \text{Denotes a definition}
               \\
               f : A \to B     & \text{Denotes a map } f \text{ the set } A \text{ into set } B
               \\
                               & \text{ (describe the function).}
               \\
               f : a \mapsto b & \text{Map } f \text{ sends the element } a \text{ to the element } b.
               \\
                               & \text{ (describe the action).}
               \\
               \circ           & \text{Denotes a composition of maps}
               \\
               (g \circ f)(a)  & \equiv g(f(a))
               \\
               A \times B      & \text{The set } \{(a, b)\} \text{ of all ordered pairs where } a \in A, b \in B.
               \\
                               & \text{ Referred to as the cartesian product of sets A and B.}
               \\
               \mathbb{R}^n    & \underbrace{\mathbb{R} \times \cdots \times \mathbb{R} }_{n}
               \\
         \end{array} &
\end{flalign*}

\subsubsection{Einstein summation convention.}
The Einstein summation convention states that whenever an index is repeated in an expression, once as a superscript $v^i$ and once as a subscript $e_i$, then summation over that index is implied.
For example
\begin{equation*}
      v = \sum\limits_{i=1}^{n} v^{i} e_{i} \implies v = v^{i} e_{i}
\end{equation*}

Vector $v$ is defined as contravariant vector  due to the components obeying the opposite of the basis vector $e_i$.
Same reason is why the dual vector $f$ is said to be covariant vector: its components transform the same as basis vector
\begin{equation*}
      v^{i'} = A^{i'}_{j} v^{j}\quad
      e^{i} = A^{i}_{j'} e^{j'}    \qquad
      f_{i'} = A^{j}_{i'} f_{j}\quad
      e_{i'} = A_{i'}^{j} e_{j}
\end{equation*}

This is the reason why the indices is defined in such way
\begin{enumerate}
      \item We knew ahead of time that the components of dual vectors would transform like basis vectors, so we gave them both lower indices.
      \item We also knew that the components of vectors would
            transform like dual basis vectors, so we gave them both upper indices
\end{enumerate}

\subsubsection{Dirac notation comparison.}
Consider linear operator $H$, which is $(1,1)$ tensor, on a Hilbert space spanned by orthonormal vectors $\{e_i \}$.
In Dirac notation we would write the basis as $\{\ket{i}\}$, $H(e_i)$ as $H \ket{i}$, $\tilde{e}_j(e_i)=(e_j|e_i)$ as $\braket{j|i}$, and $(e_j|He_i)$ as $\braket{j|H|i}$.
The component of linear component $H$ is thus
\begin{equation*}
      H_i{}^j = H(e_i, e^j) = e^j (H e_i) = \langle j | H | i \rangle
\end{equation*}
In essence, we have
\begin{flalign*}
       & \begin{array}{ll}
               \text{Standard Notation }                                           & \text{Dirac Notation}
               \\
               \text{Basis } \{e_i\}                                               & \{\ket{i}\}
               \\
               \text{Vector } \psi \in \mathcal{H}                                 & |\psi\rangle
               \\
               \text{Dual Vector } L(\psi)                                         & \langle\psi|
               \\
               \text{Inner Product } (\psi, \phi) = \tilde{\psi}(\phi)=(\psi|\phi) & \langle\psi|\phi\rangle
               \\
               A(\psi), A \in \mathcal{L}(\mathcal{H})                             & A|\psi\rangle
               \\
               (\psi, A\phi)                                                       & \langle\psi|A\phi\rangle
               \\
               (1,1) \text{ tensor component }                                     & \langle j | H | i \rangle
               \\
               \quad H_i{}^j = H(e_i, e^j) = e^j (H e_i)                           &
               \\
               T_{ij} e^i \otimes e_j                                              & \sum_{i,j} T_{ij} |j\rangle\langle i|
               \\
               e_i \otimes e_j                                                     & |i\rangle |j\rangle \text{ or } |i,j\rangle
         \end{array} &
\end{flalign*}

\subsubsection{Matrix notation comparison.}
The transformation of vectors and dual vectors
\begin{equation*}
      v^{i'} = A^{i'}_{j} v^{j}\qquad
      f_{i'} = A^{j}_{i'} f_{j}
\end{equation*}
can be written in terms of matrices as
\begin{equation*}
      [v]_{\mathcal{B}'} = A [v]_{\mathcal{B}}\qquad
      [f]_{\mathcal{B}'} = A^{-1 T} [f]_{\mathcal{B}}
\end{equation*}
we know that $f (v)=[f]_\mathcal{B}^T [v]_\mathcal{B}$ should be basis-independent.
This can be proven by
\begin{equation*}
      [f]_{\mathcal{B'}}^{T} [v]_{\mathcal{B'}} = (A^{-1T}[f]_{\mathcal{B}})^{T} A [v]_{\mathcal{B}}
            = [f]_{\mathcal{B}}^{T} A^{-1} A [v]_{\mathcal{B}}
            = [f]_{\mathcal{B}}^{T} [v]_{\mathcal{B}}
\end{equation*}

As a $(1,1)$ tensor, linear operator $T$ in $V$ can be written as
\begin{equation*}
      T_{i'}^{j'} = A_{i'}^{k} A_{l}^{j'} T_{k}^{l}
\end{equation*}
which in matrix form reads
\begin{equation*}
      [T]_{\mathcal{B}'} = A [T]_{\mathcal{B}} A^{-1}
\end{equation*}
with $[T]_\mathcal{B}=T_{k}^{l}$ and $[T]_{\mathbb{B'}'}T_{i'}^{j'}$.
This is nothing more than familiar similarity transformation of matrices.
From this, we can also define the trace of a tensor.
Given $T \in \mathcal{L}(V)$ and basis $\mathcal{B}$ for $V$, we define the trace of $T$ as
\begin{equation*}
      \mathrm{Tr}(T) \equiv \mathrm{Tr}([T]_{\mathcal{B}}).
\end{equation*}
which is basis independent.

\subsection{Definition}
Many older books define a tensor as a collection of objects which carry indices and which 'transform' in a particular way specified by those indices.
Modern  definition takes a tensor to be a function which eats a certain number of vectors (known as the rank $r$ of the tensor) and produces a number.
The distinguishing characteristic of a tensor is a special property called multilinearity
\begin{equation*}
      T(v_1^{i_1} e_{i_1}, ..., v_r^{i_r} e_{i_r})
      =  v_{1}^{i_{1}}...v_{r}^{i_{r}} T(e_{i_{1}}, ..., e_{i_{r}}) \\
\end{equation*}
which enables us to express the value of the function on an arbitrary set of $r$ vectors in terms of the values of the function on $r$ basis vectors.
In older treatments, these are usually introduced as components of the tensor.

A tensor of type $(r, s)$ on a vector space V is a $\mathbb{C}$-valued function $T$ on $V^r \times  (V^{*}) ^s$,
\begin{equation*}
      T : \underbrace{V \times \cdots \times V}_{r}
      \times
      \underbrace{V^* \times \cdots \times V^*}_{s}
      \to \mathbb{C}
\end{equation*}
In this definition, vector $v: V \to (V^*)$ and its dual $f:V\to \mathbb{C}$ can be defined as $(0,1)$ and $(1,0)$ tensor respectively.
In other words
\begin{equation*}
      v(f)=f(v)
\end{equation*}
where $v \in V$ and $f \in V^*$.
We also view linear operator as $(1,1)$ tensor $A(v,f)\equiv f(Av)$ and $(0,0)$ tensor as scalar.

In physics textbooks, tensors (usually of the second rank) are often represented as matrices
It is crucial to keep in mind, though, that this association between a tensor and a matrix depends entirely on a choice of basis, and that matrix $[T]$ is useful mainly as a computational tool, not a conceptual handle.
Tensor $T$ is best thought of abstractly as a multilinear function, and matrix $[T ]$ as its representation in a particular coordinate system.

\subsubsection{Multilinearity and tensor component.}
Linearity implies that dual vectors and linear operators are determined by their values on basis vectors
\begin{equation*}
      v \left(\sum_{i} a_{i} e_{i}\right) = \sum_{i} a_{i} v(e_{i}).
\end{equation*}
As such multilinearity implies the same thing for general tensors.

To see this, let  $\{e_i \}_{i=1,...,n}$ be a basis for $V$ and $\{e^i \}_{i=1,...,n}$ the corresponding dual basis.
We denote $i$-the of vector $v_p$ and $j$-th of the dual $f_q$ as
\begin{equation*}
      v_p  = v_p^i e_i\qquad
      f_q  = f_{pj} e^j
\end{equation*}
Then by repeated application of multilinearity, we have
\begin{align*}
      T(v_{1}, ..., v_{r}, f_{1}, ..., f_{s}) & =  T(v_1^{i_1} e_{i_1}, ..., v_r^{i_r} e_{i_r}, f_{1j_1} e^{j_1}, ..., f_{sj_s} e^{j_s})                        \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T(e_{i_{1}}, ..., e_{i_{r}}, e^{j_{1}},..., e^{j_{s}}) \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T_{i_{1},...,i_{r}}{}^{j_{1}...j_{s}}
\end{align*}
where, as before, the components of $T$ induced $\{e_i \}$ and its dual $\{e^i \}$ is
\begin{equation*}
      T_{i_{1}...i_{r}}{}^{j_{1}...j_{s}} \equiv T(e_{i_{1}},...,e_{i_{r}},e^{j_{1}},...,e^{j_{r}})
\end{equation*}
Here components are defined as the values of the tensor when evaluated on a given set of basis vectors.

\subsection{Tensor Transformation}
Suppose we have basis $\mathcal{B}$ and primed basis $\mathcal{B}$.
The transformation law that govern the change of basis from the primed basis $\mathcal{B}'$ into $\mathcal{B}$ is
\begin{equation*}
      T_{i'_{1}, \dots, i'_{r}}{}^{j'_{1} \dots j'_{s}}
      = A^{k_{1}}_{i'_{1}} \dots A^{k_{r}}_{i'_{r}} A^{l_{1}}_{j'_{1}} \dots A^{l_{s}}_{j'_{s}} T_{k_{1} \dots k_{r}}{}^{l_{1} \dots l_{s}}
\end{equation*}
The matrix representation of the transformation coefficients $A=A_{i}^{k'}$ and $A^{-1}=A_{j'}^{l}$ can be written as
\begin{equation*}
      A = \begin{pmatrix} A_{1}^{1'} & A_{2}^{1'} & \dots & A_{n}^{1'} \\ A_{1}^{2'} & A_{2}^{2'} & \dots & A_{n}^{2'} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1}^{n'} & A_{2}^{n'} & \dots & A_{n}^{n'} \end{pmatrix}
      \qquad
      A^{-1} = \begin{pmatrix} A_{1'}^{1} & A_{2'}^{1} & \dots & A_{n'}^{1} \\ A_{1'}^{2} & A_{2'}^{2} & \dots & A_{n'}^{2} \\ \vdots & \vdots & \ddots & \vdots \\ A_{1'}^{n} & A_{2'}^{n} & \dots & A_{n'}^{n} \end{pmatrix}
\end{equation*}
These matrices satisfy $AA^{-1}=A^{-1}A=I$.

\subsubsection{Derivation.}
We will derive the transformation law as a consequence of multilinearity, instead of take it as the deﬁnition of a tensor like in much of the physics literature.
First let us define the basis in both basis as $\mathcal{B}=\{e_i\}_{i=1,\dots,n}$ and $\mathcal{B}'=\{e_{i'}\}_{i=1,\dots,n}$.
With linearity, each basis can be written as
\begin{equation*}
      e_{i} = A_{i}^{j'} e_{j'}\qquad
      e_{i'} = A_{i'}^{j} e_{}
\end{equation*}
Recall that the prime refer to the basis: unprimed subscript refer to unprimed basis and vice versa.
We then have
\begin{align*}
      e_{i} = A_{i}^{j'} A_{j'}^{k} e_{k} = \delta_{i}^{k} e_{k}
      \\
      e_{i'} = A_{i'}^{j} A_{j}^{k'} e_{k'} = \delta_{i'}^{k'} e_{k'}
\end{align*}
In a way, $A_{i}^{j'}$ and $A_{i'}^{j}$ are inverse of each other.
Note that inverse here means inverse as linear maps under composition
\begin{equation*}
      A : V \rightarrow V\qquad
      A^{-1} : V \rightarrow V
\end{equation*}
and
\begin{equation*}
      A \circ A^{-1} = I \qquad
      A^{-1} \circ A = I
\end{equation*}
with $(A)^{j'}_{i} = A^{j'}_{i}$ and $(A^{-1})^{j'}_{i} = A^{j'}_{i}$.
Of course, in index notation the identity refer to Kronecker delta.

Now define the dual basis $\mathcal{B}=\{e^i\}_{i=1,\dots,n}$ and $\mathcal{B}'=\{e^{i'}\}_{i=1,\dots,n}$.
The component of the basis are
\begin{align*}
      e^{i}(e_{j'}) & =  e^{i}\left( A^{k}_{j'}e_{k} \right)  = A^{k}_{j'} e^{i}\left( e_{k} \right) = A^{k}_{j'} \delta^{i}_{k} = A^{i}_{j'}       \\
      e^{i'}(e_{j}) & =  e^{i'}\left( A^{k'}_{j}e_{k'} \right)  = A^{k'}_{j} e^{i'}\left( e_{k'} \right) = A^{k'}_{j} \delta^{i'}_{k'} = A^{i'}_{j}
\end{align*}
That means
\begin{equation*}
      e^{i'} = A^{i'}_{j} e^{j} \qquad
      e^{i} = A^{i}_{j'} e^{j'}
\end{equation*}
Using repeated multilinearity once again
\begin{align*}
      T_{i'_{1},...,i'_{r}}{}^{j'_{1}...j'_{s}} & =  T(e_{i'_{1}}, ..., e_{i'_{r}}, e^{j'_{1}}, ..., e^{j'_{s}})                                                                             \\
                                                & =  T(A_{i'_{1}}^{k_{1}}e_{k_{1}}, ..., A_{i'_{r}}^{k_{r}}e_{k_{r}}, A_{l_{1}}^{j'_{1}}e^{l_{1}}, ..., A_{l_{s}}^{j'_{s}}e^{l_{s}})         \\
                                                & =  A_{i'_{1}}^{k_{1}}...A_{i'_{r}}^{k_{r}} A_{l_{1}}^{j'_{1}}...A_{l_{s}}^{j'_{s}} T(e_{k_{1}}, ..., e_{k_{r}}, e^{l_{1}}, ..., e^{l_{s}}) \\
                                                & =  A_{i'_{1}}^{k_{1}}...A_{i'_{r}}^{k_{r}} A_{l_{1}}^{j'_{1}}...A_{l_{s}}^{j'_{s}} T_{k_{1}...k_{r}}{}^{l_{1}...l_{s}}
\end{align*}

\subsection{Active and Passive Transformation}
In active transformation, we use the matrix $A$ to change one vector $e_{i}$ into another $e_{i'}$
\begin{equation*}
      [v_{i'}]_{\mathcal{B}} = A[v_{i}]_{\mathcal{B}}
\end{equation*}
It is a change of vector $v \to v'$ in o other words.
This can be seen by considering the components of transformed vector $v'=A(v)$
\begin{equation*}
      v^{j'} e_{j} = A(v^{i} e_{i}) = v^{i} A(e_{i}) = v^{i} A_{i}^{j} e_{j}
\end{equation*}
And we obtain
\begin{equation*}
      v^{j'} = A_{i}^{j} v^{i}
\end{equation*}
We see that the change of component in active transformation.

In passive transformation, we use the matrix $A$ to change the basis $\mathcal{B}$ into another $\mathbb{B}'$, not the vector
\begin{equation*}
      \{e_i\}_{\mathcal{B}'} = A \left\{ e_i \right\} _{\mathcal{B}}
\end{equation*}
Here the basis and the components change, while the vector itself is fixed.
When $A$ is interpreted passively it corresponds to the opposite transformation as in the active case.
This can be understood by considering passive transformation definition of change in basis
\begin{equation*}
      v = v^{i} e_{i} = v^{i'} e_{i'}
\end{equation*}
The components must change in order for the definition remains true.
Inserting the change of basis
\begin{equation*}
      v^{i} e_{i} = v^{i'} A_{i}^{i} e_{i}
\end{equation*}
Hence, we obtain
\begin{equation*}
      v^{i} = A_{i'}^{i} v^{i'}\qquad
      v^{i'} = (A^{-1})_{i}^{i'} v^{i}
\end{equation*}

\subsubsection{Quantum mechanics.}
In the Schrödinger picture, one considers observables like the momentum or position operator as acting on the state ket while the basis kets remain fixed.
This is the active viewpoint.
In the Heisenberg picture, however, one considers the state ket to be fixed and considers the observables to be time-dependent.
Consider the time-evolution operator $U$, the evolution of quantum state is
\begin{align*}
      \langle \hat{x}(t) \rangle & =  \langle \psi | (U^{\dagger} \hat{x} U) | \psi \rangle                          \\
                                 & =  \left(\langle \psi | U^{\dagger}\right) \hat{x} \left(U | \psi \rangle\right),
\end{align*}
$U$ can be interpreted in two ways: in the active sense of (first line), in which the $U$ act on the vectors and change them into new vectors, and in the passive sense of (second line), where the $U$ act on the operator $\hat{x}$ by a similarity transformation to turn it into a new operator, $\hat{x}(t)$.
\end{document}