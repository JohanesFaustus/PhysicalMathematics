\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Introduction}
\subsubsection{Some mathematical shorthand.} As follows
\begin{equation*}
      \begin{array}{ll}
            \mathbb{R}      & \text{The set of real numbers}
            \\
            \mathbb{C}      & \text{The set of complex numbers}
            \\
            \mathbb{Z}      & \text{The set of positive and negative integers}
            \\
            \in             & \text{an element of }
            \\
            \notin          & \text{is not an element of}
            \\
            \forall         & \text{for all}
            \\
            \subset         & \text{is a subset of, a subset of}
            \\
            \equiv          & \text{Denotes a definition}
            \\
            f : A \to B     & \text{Denotes a map } f \text{ the set } A \text{ into set } B.
            \\
                            & \text{ Describe the function}
            \\
            f : a \mapsto b & \text{Map } f \text{ sends the element } a \text{ to the element } b.
            \\
                            & \text{ Describe the action}
            \\
            \circ           & \text{Denotes a composition of maps}
            \\
            (g \circ f)(a)  & \equiv g(f(a))
            \\
            A \times B      & \text{The set } \{(a, b)\} \text{ of all ordered pairs where } a \in A, b \in B.
            \\
                            & \text{Referred to as the cartesian product of sets A and B.}
            \\
            \mathbb{R}^n    & \mathbb{R} \times \cdots \times \mathbb{R}
            \\
      \end{array}
\end{equation*}

\subsubsection{Einstein summation convention.}
The Einstein summation convention states that whenever an index is repeated in an expression, once as a superscript $v^i$ and once as a subscript $e_i$, then summation over that index is implied.
For example
\begin{equation*}
      v = \sum\limits_{i=1}^{n} v^{i} e_{i} \implies v = v^{i} e_{i}
\end{equation*}


Vector $v$ is defined as contravariant vector  due to the components obeying the opposite of the basis vector $e_i$.
Same reason is why the dual vector $f$ is said to be covariant vector: its components transform the same as basis vector
\begin{equation*}
      v^{i'} = A^{i'}_{j} v^{j}\quad
      e^{i} = A^{i}_{j'} e^{j'}    \qquad
      f_{i'} = A^{j}_{i'} f_{j}\quad
      e_{i'} = A_{i'}^{j} e_{j}
\end{equation*}

This is the reason why the indices is defined in such way
\begin{enumerate}
      \item We knew ahead of time that the components of dual vectors would transform like basis vectors, so we gave them both lower indices.
      \item We also knew that the components of vectors would
            transform like dual basis vectors, so we gave them both upper indices
\end{enumerate}

\subsection{Definition}
Many older books define a tensor as a collection of objects which carry indices and which 'transform' in a particular way specified by those indices.
Modern  definition takes a tensor to be a function which eats a certain number of vectors (known as the rank $r$ of the tensor) and produces a number.
The distinguishing characteristic of a tensor is a special property called multilinearity
\begin{equation*}
      T(v_{1}, ..., v_{r}, f_{1}, ..., f_{s})
      = v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1}^{j_{1}}...f_{s}^{j_{s}} T_{i_{1},...,i_{r}}^{j_{1}...j_{s}}
\end{equation*}
which enables us to express the value of the function on an arbitrary set of $r$ vectors in terms of the values of the function on $r$ basis vectors.
In older treatments, these are usually introduced as components of the tensor.

In physics textbooks, tensors (usually of the second rank) are often represented as matrices
It is crucial to keep in mind, though, that this association between a tensor and a matrix depends entirely on a choice of basis, and that matrix $[T]$ is useful mainly as a computational tool, not a conceptual handle.
Tensor $T$ is best thought of abstractly as a multilinear function, and matrix $[T ]$ as its representation in a particular coordinate system.

A tensor of type $(r, s)$ on a vector space V is a $\mathbb{C}$-valued function $T$ on
\begin{equation*}
      V^r \times  (V^{*}) ^s
\end{equation*}
In this definition, vector $v: V \to (V^*)$ and its dual $f:V\to \mathbb{C}$ can be defined as $(0,1)$ and $(1,0)$ tensor respectively.
In other words
\begin{equation*}
      v(f)=f(v)
\end{equation*}
where $v \in V$ and $f \in V^*$.
We also view linear operator as $(1,1)$ tensor $oA(v,f)\equiv f(Av)$ and $(0,0)$ tensor as scalar.

\subsubsection{Multilinearity and tensor component.}
Linearity implies that dual vectors and linear operators are determined by their values on basis vectors
\begin{equation*}
      v \left(\sum_{i} a_{i} e_{i}\right) = \sum_{i} a_{i} v(e_{i}).
\end{equation*}
As such multilinearity implies the same thing for general tensors.

To see this, let  $\{e_i \}_{i=1,...,n}$ be a basis for $V$ and $\{e^i \}_{i=1,...,n}$ the corresponding dual basis.
We denote $i$-the of vector $v_p$ and $j$-th of the dual $f_q$ as
\begin{equation*}
      v_p  = v_p^i e_i\qquad
      f_q  = f_{pj} e^j
\end{equation*}
Then by repeated application of multilinearity, we have
\begin{align*}
      T(v_{1}, ..., v_{r}, f_{1}, ..., f_{s}) & =  T(v_1^{i_1} e_{i_1}, ..., v_r^{i_r} e_{i_r}, f_{1j_1} e^{j_1}, ..., f_{sj_s} e^{j_s})                        \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T(e_{i_{1}}, ..., e_{i_{r}}, e^{j_{1}},..., e^{j_{s}}) \\
                                              & =  v_{1}^{i_{1}}...v_{r}^{i_{r}} f_{1j_{1}}...f_{sj_{s}} T_{i_{1},...,i_{r}}^{j_{1}...j_{s}}
\end{align*}
where, as before, the components of $T$ induced $\{e_i \}$ and its dual $\{e^i \}$ is
\begin{equation*}
      T_{i_{1}...i_{r}}^{j_{1}...j_{s}} \equiv T(e_{i_{1}},...,e_{i_{r}},e^{j_{1}},...,e^{j_{r}})
\end{equation*}
\end{document}