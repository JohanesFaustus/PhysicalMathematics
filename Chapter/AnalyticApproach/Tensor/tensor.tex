\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Introduction}
\subsubsection{Some mathematical shorthand.} As follows
\begin{equation*}
    \begin{array}{ll}
        \mathbb{R}      & \text{The set of real numbers}
        \\
        \mathbb{C}      & \text{The set of complex numbers}
        \\
        \mathbb{Z}      & \text{The set of positive and negative integers}
        \\
        \in             & \text{an element of }
        \\
        \notin          & \text{``is not an element of''}
        \\
        \forall         & \text{``for all''}
        \\
        \subset         & \text{``is a subset of'', ``a subset of''}
        \\
        \equiv          & \text{Denotes a definition}
        \\
        f : A \to B     & \text{Denotes a map } f \text{ the set } A \text{ into set } B.
        \\
                        & \text{ Describe the function}
        \\
        f : a \mapsto b & \text{Map } f \text{ sends the element } a \text{ to the element } b.
        \\
                        & \text{ Describe the action}
        \\
        \circ           & \text{Denotes a composition of maps}
        \\
        (g \circ f)(a)  & \equiv g(f(a))
        \\
        A \times B      & \text{The set } \{(a, b)\} \text{ of all ordered pairs where } a \in A, b \in B.
        \\
                        & \text{Referred to as the cartesian product of sets A and B.}
        \\
        \mathbb{R}^n    & \mathbb{R} \times \cdots \times \mathbb{R}
        \\
    \end{array}
\end{equation*}

\subsubsection{Einstein summation convention.}
The Einstein summation convention states that whenever an index is repeated in an expression, once as a superscript $v^i$ and once as a subscript $e_i$, then summation over that index is implied.
For example
\begin{equation*}
    v = \sum\limits_{i=1}^{n} v^{i} e_{i} \implies v = v^{i} e_{i}
\end{equation*}


Vector $v$ is defined as contravariant vector  due to the components obeying the opposite of the basis vector $e_i$.
Same reason is why the dual vector $f$ is said to be covariant vector: its components transform the same as basis vector
\begin{equation*}
    v^{i'} = A^{i'}_{j} v^{j}\quad
    e^{i} = A^{i}_{j'} e^{j'}    \qquad
    f_{i'} = A^{j}_{i'} f_{j}\quad
    e_{i'} = A_{i'}^{j} e_{j}
\end{equation*}

This is the reason why the indices is defined in such way
\begin{enumerate}
    \item We knew ahead of time that the components of dual vectors would transform like basis vectors, so we gave them both lower indices.
    \item We also knew that the components of vectors would
          transform like dual basis vectors, so we gave them both upper indices
\end{enumerate}

\subsection{Definition}
Many older books define a tensor as a collection of objects which carry indices and which 'transform' in a particular way specified by those indices.
Modern  deÔ¨Ånition takes a tensor to be a function which eats a certain number of vectors (known as the rank $r$ of the tensor) and produces a number.
The distinguishing characteristic of a tensor is a special property called multilinearity, which enables us to express the value of the function on an arbitrary set of $r$ vectors in terms of the values of the function on $r$ basis vectors.
In older treatments, these are usually introduced as components of the tensor.

In physics textbooks, tensors (usually of the second rank) are often represented as matrices
It is crucial to keep in mind, though, that this association between a tensor and a matrix depends entirely on a choice of basis, and that matrix $[T]$ is useful mainly as a computational tool, not a conceptual handle.
Tensor $T$ is best thought of abstractly as a multilinear function, and matrix $[T ]$ as its representation in a particular coordinate system.

\end{document}