\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Courant Number}
The Courant number measures how far information (e.g., a fluid particle or wave front) travels across the grid during one time step relative to the grid spacing.
\begin{equation*}
  \sigma=\frac{u \Delta t}{\Delta x}
\end{equation*}
Numerical stability typically requires $\sigma<1$, and as such we can write the time steps $\Delta x$ as
\begin{equation*}
  \Delta t=\frac{\sigma \Delta x }{u}
\end{equation*}

For example, consider some loop at which time iteration is required.
In each iteration of our time loop, we use the existing data about our wave to estimate the speed of the wave in the subsequent time step.
What has happened is that over the time period $\Delta t$, the wave is travelling a distance which is greater than \verb|dx|.  The length \verb|dx| of each grid box is related to the number of total points \verb|nx|, so stability can be enforced if the $\Delta t$ step size is calculated with respect to the size of \verb|dx|.

\subsection{Finite Difference Method}
\subsubsection{Forward difference.}
The forward difference of a function $f(x)$ with step size $h$ is defined as
\begin{equation*}
  \Delta f(x)=f(x+h)-f(x)
\end{equation*}
The corresponding approximation of the first derivative is
\begin{equation*}
  f'(x)\approx \frac{f(x+h )-f(x )}{h}
\end{equation*}
Forward difference uses data at the current and next point.

\subsubsection{Backward difference.}
The backward difference of a function $f(x)$ with step size $h$ is defined as
\begin{equation*}
  \nabla f(x)=f(x)-f(x-h)
\end{equation*}
The corresponding approximation of the first derivative is
\begin{equation*}
  f'(x)\approx \frac{f(x )-f(x-h )}{h}
\end{equation*}
Backward difference uses data at the current and previous point.

\subsubsection{Central difference.}
Denoted by
\begin{equation*}
  \delta f(x)=f(x+h/2)-f(x-h/2)
\end{equation*}
with the first derivative as
\begin{equation*}
  f'(x)\approx \frac{f(x+h/2)-f(x-h/2)}{h}
\end{equation*}
or equivalently
\begin{equation*}
  f'(x)\approx \frac{f(x+h)-f(x-h)}{2h}
\end{equation*}

\subsubsection{Taylor series relation.}
Recall the Taylor expansion of $f(x)$
\begin{equation*}
  f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}
Now expand $f(x+h)$ with Taylor expansion by substituting $x=a$ with $a$ as the expansion point
\begin{equation*}
  f(x+h) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x+h-a)^n= \sum_{n=0}^{\infty} \frac{f^{(n)}(x)}{n!}h^n
\end{equation*}
Now do the same thing for $f(x-h)$ instead
\begin{equation*}
  f(x+h) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-h-a)^n= \sum_{n=0}^{\infty} (-1)^n\frac{f^{(n)}(x)}{n!}h^n
\end{equation*}
Explicitly, both equation can be written
\begin{align*}
  f(x+h) & =  f(x) + h f'(x) + \frac{h^2}{2} f''(x) + \frac{h^3}{6} f^{(3)}(x) + \cdots \\
  f(x-h) & =  f(x) - h f'(x) + \frac{h^2}{2} f''(x) - \frac{h^3}{6} f^{(3)}(x) + \cdots
\end{align*}

To obtain forward difference, subtract $f(x+h)$ by $f(x)$ and divide by $h$
\begin{equation*}
  \frac{f(x+h) - f(x)}{h} = f'(x) + \frac{h}{2} f''(x) + \frac{h^2}{6} f^{(3)}(x) +\cdots
\end{equation*}
Thus
\begin{equation*}
  f'(x) \approx \frac{f(x+h) - f(x)}{h}, \quad \text{error } = O(h)
\end{equation*}

In order to obtain backward difference, subtract $f(x-h)$ by $f(x)$ and divide by $h$
\begin{equation*}
  \frac{f(x+h) - f(x)}{h} = f'(x) + \frac{h}{2} f''(x) + \frac{h^2}{6} f^{(3)}(x) +\cdots
\end{equation*}
Thus
\begin{equation*}
  f'(x) \approx \frac{f(x) - f(x-h)}{h}, \quad \text{error } = O(h).
\end{equation*}

To obtain central difference, subtract $f(x+h)$ by $f(x-h)$ and divide by $h$
\begin{equation*}
  f(x+h) - f(x-h) = 2h f'(x) + \frac{h^3}{3} f^{(3)}(x) +\cdots
\end{equation*}
then divide by $2h$
\begin{equation*}
  \frac{f(x+h) - f(x-h)}{2h} = f'(x) + \frac{h^2}{6} f^{(3)}(x) +\cdots
\end{equation*}
Thus
\begin{equation*}
  f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}, \quad \text{error } = O(h^2)
\end{equation*}

\subsection{Euler Method}
The core idea of Euler method is to approximate derivatives by finite differences over small increments.
Consider a function $y(t)$ satisfying the differential equation
\begin{equation*}
  \frac{dy }{dt }=f(t,y),\qquad y(t_0)=y_0
\end{equation*}
Here, the derivative is effectively approximated as
\begin{equation*}
  \frac{dy}{dt}\Big|_{t=t_n} \approx \frac{y_{n+1} - y_n}{h}
\end{equation*}
The Euler method approximates $y$ at a discrete set of points $t_n$ with step size $h$ using the forward difference:
\begin{equation*}
  y_{n+1} = y_{n} + h f(t_{n}, y_{n})
\end{equation*}

\subsection{RK2: Heun Method}
The Heun method is a predictor-corrector method that improves upon the Euler method by using an average of slopes.
The algorithm is as follows.
\begin{enumerate}
  \item \textbf{Predictor (Euler step).} Estimate the next value using the Euler method
        \begin{equation*}
          y_{n+1}^{\text{pred}} = y_{n} + h f(t_{n}, y_{n})
        \end{equation*}
  \item \textbf{Corrector (average slope).} Refine the estimate using the average of the slope at the current point and the predicted point
        \begin{equation*}
          y_{n+1} = y_{n} + \frac{h}{2} [f(t_{n}, y_{n}) + f(t_{n+1}, y_{n+1}^{\text{pred}})]
        \end{equation*}
\end{enumerate}

\subsubsection{Implementation.} In python.
\begin{minted}[breaklines]{python}
def RK2(f, t0, y0, h, t_akh):
\end{minted}
The function \verb|f| is the derivative function \(f(t,y)\), \verb|t0| is the initial value of the independent variable \(t\), \verb|y0| is the initial value of the dependent variable \(y\), \verb|h| is the step size for the integration, and \verb|t_akh| is the final value of \(t\) at which the solution is desired.
\begin{minted}[breaklines]{python}
    t_rk2 = np.arange(t0, t_akh + h, h)
    y_rk2 = np.zeros(len(t_rk2))
    y_rk2[0] = y0
\end{minted}
Inside the function, \verb|t_rk2| generates an array of discrete time points from \verb|t0| to \verb|t_akh| spaced by \verb|h|, and \verb|y_rk2| initializes an array to store the numerical solution, with \verb|y_rk2[0]| set to the initial condition \verb|y0|.
\begin{minted}[breaklines]{python}
    for i in range(len(t_rk2) - 1):
        t_i = t_rk2[i]
        y_i = y_rk2[i]
        t_ip = t_rk2[i+1]
\end{minted}
The \verb|for| loop iterates over each time step. At each iteration, \verb|t_i| and \verb|y_i| are the current time and solution, and \verb|t_ip| is the next time point.
\begin{minted}[breaklines]{python}
        k1 = f(t_i, y_i)
        y_pred = y_i + h * k1
        k2 = f(t_ip, y_pred)
\end{minted}
\verb|k1 = f(t_i, y_i)| computes the slope at the beginning of the interval. The predictor step \verb|y_pred = y_i + h * k1| estimates the next value using a simple Euler step. Then, \verb|k2 = f(t_ip, y_pred)| evaluates the slope at the predicted point.
\begin{minted}[breaklines]{python}
        y_rk2[i+1] = y_i + 0.5 * h * (k1 + k2)
\end{minted}
The corrector step \verb|y_rk2[i+1] = y_i + 0.5 * h * (k1 + k2)| updates the solution by averaging the initial slope \verb|k1| and the predicted slope \verb|k2|, multiplied by the step size, giving the RK2 method its second-order accuracy.
\begin{minted}[breaklines]{python}
    return t_rk2, y_rk2
\end{minted}
Finally, \verb|return t_rk2, y_rk2| outputs the arrays of time points and the corresponding numerical solution.

\subsection{RK4}
The Runge窶適utta fourth-order method (RK4) is a widely used numerical integration technique for solving ordinary differential equations (ODEs) because it provides a good balance between accuracy and computational effort.
The algorithm is as follows.
\begin{enumerate}
  \item \textbf{First slope.} Compute the first slope at the beginning of the interval
        \begin{equation*}
          k_1 = f(t_n, y_n)
        \end{equation*}
  \item \textbf{Second slope.} Compute the slope at the midpoint using $k_1$
        \begin{equation*}
          k_2 = f\Big(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_1\Big)
        \end{equation*}
  \item \textbf{Third slope.} Compute another slope at the midpoint using $k_2$
        \begin{equation*}
          k_3 = f\Big(t_n + \frac{h}{2}, y_n + \frac{h}{2} k_2\Big)
        \end{equation*}
  \item \textbf{Fourth slope.} Compute the slope at the end of the interval using $k_3$
        \begin{equation*}
          k_4 = f(t_n + h, y_n + h k_3)
        \end{equation*}
  \item \textbf{Update.} Compute the next value of $y$ using a weighted average of the slopes:
        \begin{equation*}
          y_{n+1} = y_n + \frac{h}{6} \Big(k_1 + 2k_2 + 2k_3 + k_4 \Big)
        \end{equation*}
\end{enumerate}

\subsection{1D Linear Convection}
The equation is written as
\begin{equation*}
  \frac{\partial u}{\partial t} + c \frac{\partial u}{\partial x} = 0
\end{equation*}
Using forward difference for time and backward difference for spatial, our discrete equation is
\begin{equation*}
  \frac{u_i^{n+1}-u_i^n}{\Delta t} + c \frac{u_i^n - u_{i-1}^n}{\Delta x} = 0
\end{equation*}
Where $n$ and $n+1$ are two consecutive steps in time, while $i-1$ and $i$ are two neighboring points of the discretized $x$ coordinate.
If there are given initial conditions, then the only unknown in this discretization is $u_i^{n+1}$, which can be solved as
\begin{equation*}
  u_i^{n+1} = u_i^n - c \frac{\Delta t}{\Delta x}(u_i^n-u_{i-1}^n)
\end{equation*}

\subsubsection{Non-linear convection.}
The equation takes the form
\begin{equation*}
  \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = 0
\end{equation*}
Instead of a constant factor $c$ multiplying the second term, now we have the solution $u$ multiplying it, causing the equation to be non-linear.
As before, the discrete form is
\begin{equation*}
  \frac{u_i^{n+1}-u_i^n}{\Delta t} + u_i^n \frac{u_i^n-u_{i-1}^n}{\Delta x} = 0
\end{equation*}
or more practically written as
\begin{equation*}
  u_i^{n+1} = u_i^n - u_i^n \frac{\Delta t}{\Delta x} (u_i^n - u_{i-1}^n)
\end{equation*}

\subsubsection*{Implementation.}
Given the initial conditions, the solution can be written in python numerically, using the Courant number, as
\begin{minted}[breaklines]{python}
def linearconv(nx,nt,u):
    dx = 2 / (nx - 1)
    c = 1
    sigma = .5
    dt = sigma * dx

    for n in range(nt):  #iterate through time
        un = u.copy() ##copy the existing values of u into un
        for i in range(1, nx):
            u[i] = un[i] - c * dt / dx * (un[i] - un[i-1])

    return u
\end{minted}
and for the non-linear equation
\begin{minted}[breaklines]{python}
def nonlinear(nx,nt,u):
    dx = 2 / (nx - 1)
    c = 1
    sigma = .5
    dt = sigma * dx

    for n in range(nt):  #iterate through time
        un = u.copy() ##copy the existing values of u into un
        for i in range(1, nx):
           u[i] = un[i] - un[i] * dt / dx * (un[i] - un[i-1]) 
        
    return u
\end{minted}

As a bonus we can define the function where it begins with sinusoidal and after one period, it disappears
\begin{minted}[breaklines]{python}
def y(nx):
    u=numpy.zeros(nx)
    quarter=int(len(nx)/4)
    alpha = quarter / (2 * numpy.pi) 

    for i in range(0,quarter):
        u[i] = numpy.sin(i / alpha)
    return u
\end{minted}
Other is a hat function
\begin{minted}[breaklines]{python}
def uHat(nx):
    u = numpy.zeros(nx) 
    quarter=int(len(nx)/4)
    half=int(len(nx)/2)
    u[quarter:half] = 1
\end{minted}

\subsection{2D Linear Convection}
The PDE governing 2-D Linear Convection is written as
\begin{equation*}
  \frac{\partial u}{\partial t}+c\frac{\partial u}{\partial x} + c\frac{\partial u}{\partial y} = 0
\end{equation*}
This is the exact same form as with 1-D Linear Convection, except that we now have two spatial dimensions to account for as we step forward in time.
Again, the timestep will be discretized as a forward difference and both spatial steps will be discretized as backward differences
\begin{equation*}
  \frac{u_{i,j}^{n+1}-u_{i,j}^n}{\Delta t} + c\frac{u_{i, j}^n-u_{i-1,j}^n}{\Delta x} + c\frac{u_{i,j}^n-u_{i,j-1}^n}{\Delta y}=0
\end{equation*}

As before, solve for the only unknown:
\begin{equation*}
  u_{i,j}^{n+1} = u_{i,j}^n-c \frac{\Delta t}{\Delta x}(u_{i,j}^n-u_{i-1,j}^n)-c \frac{\Delta t}{\Delta y}(u_{i,j}^n-u_{i,j-1}^n)
\end{equation*}

\subsubsection{Nonlinear convection.}
2D Convection is represented by the pair of coupled partial differential equations
\begin{equation*}
  \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} + v \frac{\partial u}{\partial y} = 0,\qquad
  \frac{\partial v}{\partial t} + u \frac{\partial v}{\partial x} + v \frac{\partial v}{\partial y} = 0
\end{equation*}
Here $u(x,y,t)$ and $u(x,y,t)$ are the components of the velocity field $\mathbf{v}=(u,v)$ itself, where $u$ is the component in the $x$-direction and $v$ is the component in the $y$-direction.
If, instead, a scalar quantity $q(x,y,t)$ is advected by a given 2D velocity field $(u,v)$, then the 2D convection equation is not coupled
\begin{equation*}
  \frac{\partial q}{\partial t} + u \frac{\partial q}{\partial x} + v \frac{\partial q}{\partial y} = 0
\end{equation*}
Here $u$ and $v$ are prescribed, meaning they could be constant, vary linearly, or even follow some known function, but it is independent of $q$.

In any case, the discrete version reads
\begin{align*}
  \frac{u_{i,j}^{n+1}-u_{i,j}^n}{\Delta t} + u_{i,j}^n \frac{u_{i,j}^n-u_{i-1,j}^n}{\Delta x} + v_{i,j}^n \frac{u_{i,j}^n-u_{i,j-1}^n}{\Delta y} = 0 \\
  \frac{v_{i,j}^{n+1}-v_{i,j}^n}{\Delta t} + u_{i,j}^n \frac{v_{i,j}^n-v_{i-1,j}^n}{\Delta x} + v_{i,j}^n \frac{v_{i,j}^n-v_{i,j-1}^n}{\Delta y} = 0
\end{align*}

\subsubsection{Implementation.} For the linear
\begin{minted}[breaklines]{python}
def linearconv2D(u,nt,CFL):
    nx = len(u)
    dx = 2 / (nx - 1)
    dy = dx
    c = 1
    sigma = CFL
    dt = sigma * dx

    for n in range(nt + 1):  ##loop across number of time steps
        un = u.copy()
        u[1:, 1:] = (
            un[1:, 1:]
            - (c * dt / dx * (un[1:, 1:] - un[1:, :-1]))
            - (c * dt / dy * (un[1:, 1:] - un[:-1, 1:]))
        )
        u[0, :] = 1
        u[-1, :] = 1
        u[:, 0] = 1
        u[:, -1] = 1

    return u
\end{minted}
and for the coupled nonlinear
\begin{minted}[breaklines]{python}
def couplednonlinearconv2d(u,v,nt,CFL):
    nx = len(u)
    dx = 2 / (nx - 1)
    dy = dx
    c = 1
    sigma = CFL
    dt = sigma * dx

    for n in range(nt + 1): ##loop across number of time steps
        un = u.copy()
        vn = v.copy()
        u[1:, 1:] = (un[1:, 1:] - 
                    (un[1:, 1:] * c * dt / dx * (un[1:, 1:] - un[1:, :-1])) -
                    vn[1:, 1:] * c * dt / dy * (un[1:, 1:] - un[:-1, 1:]))
        v[1:, 1:] = (vn[1:, 1:] -
                    (un[1:, 1:] * c * dt / dx * (vn[1:, 1:] - vn[1:, :-1])) -
                    vn[1:, 1:] * c * dt / dy * (vn[1:, 1:] - vn[:-1, 1:]))
        
        u[0, :] = 1
        u[-1, :] = 1
        u[:, 0] = 1
        u[:, -1] = 1
        
        v[0, :] = 1
        v[-1, :] = 1
        v[:, 0] = 1
        v[:, -1] = 1

    return u,v
\end{minted}
Also consider the 2D hat function
\begin{minted}[breaklines]{python}
def uHat2D(nx):
    ny = nx
    dx = 2 / (nx - 1)
    dy = dx
    x = numpy.linspace(0, 2, nx)
    y = numpy.linspace(0, 2, ny)

    u = numpy.ones((ny, nx)) 
    u[int(.5 / dy):int(1 / dy + 1),int(.5 / dx):int(1 / dx + 1)] = 2 
    
    return u
\end{minted}


\subsubsection{Note on displaying surface plot.}
A two窶電imensional scalar field \(u(x,y)\) is represented numerically by sampling the field on a discrete Cartesian grid.
The coordinate arrays \(x\) and \(y\) contain the one窶電imensional locations along each axis but do not enumerate the full set of grid points in two dimensions.
The function \verb|X, Y = numpy.meshgrid(x, y)| constructs matrices \(X\) and \(Y\) such that
\begin{equation*}
  X_{ij}=x_i \text{ and } Y_{ij}=y_j
\end{equation*}
thereby generating the complete Cartesian product of the grid.
The scalar field is stored in a matrix \(u_{ij}\) of identical shape, where \(u_{ij}\approx u(x_i,y_j)\).
A surface plot requires the triplet \((X,Y,u)\). Each index pair $(i,j)$ defines a point in three-dimensional space
\begin{equation*}
  (X_{ij}, Y_{ij},u_{ij})
\end{equation*}
Hence, the construction for a three窶電imensional surface plot is performed by
\begin{minted}[breaklines]{python}
X, Y = numpy.meshgrid(x, y)
fig = pyplot.figure(figsize=(11, 7), dpi=100)
ax = fig.add_subplot(projection='3d')
surf2 = ax.plot_surface(X, Y, u[:], cmap=cm.viridis)
\end{minted}

\subsection{1D Diffusion Equation}
The one-dimensional diffusion equation is
\begin{equation*}
  \frac{\partial u}{\partial t}= \nu \frac{\partial^2 u}{\partial x^2}
\end{equation*}
The discrete version is
\begin{equation*}
  \frac{u_{i}^{n+1}-u_{i}^{n}}{\Delta t}=\nu\frac{u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n}}{\Delta x^2}
\end{equation*}
or
\begin{equation*}
  u_{i}^{n+1}=u_{i}^{n}+\frac{\nu\Delta t}{\Delta x^2}(u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n})
\end{equation*}

\subsubsection*{Derivation.}
Consider the Taylor expansion of $u_{i+1}$ and $u_{i-1}$ around $u_i$
\begin{align*}
  u_{i+1} & =  u_i + \Delta x \frac{\partial u}{\partial x}\bigg|_i + \frac{\Delta x^2}{2} \frac{\partial ^2 u}{\partial x^2}\bigg|_i + \frac{\Delta x^3}{3!} \frac{\partial ^3 u}{\partial x^3}\bigg|_i + O(\Delta x^4) \\
  u_{i-1} & =  u_i - \Delta x \frac{\partial u}{\partial x}\bigg|_i + \frac{\Delta x^2}{2} \frac{\partial ^2 u}{\partial x^2}\bigg|_i - \frac{\Delta x^3}{3!} \frac{\partial ^3 u}{\partial x^3}\bigg|_i + O(\Delta x^4)
\end{align*}
Add both
\begin{equation*}
  u_{i+1} + u_{i-1} = 2u_i+\Delta x^2 \frac{\partial ^2 u}{\partial x^2}\bigg|_i + O(\Delta x^4)
\end{equation*}
And rearrange thing
\begin{equation*}
  \frac{\partial ^2 u}{\partial x^2}=\frac{u_{i+1}-2u_{i}+u_{i-1}}{\Delta x^2} + O(\Delta x^2)
\end{equation*}
Now we can write the discretized diffusion equation
\begin{equation*}
  \frac{u_{i}^{n+1}-u_{i}^{n}}{\Delta t}=\nu\frac{u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n}}{\Delta x^2}
\end{equation*}
or
\begin{equation*}
  u_{i}^{n+1}=u_{i}^{n}+\frac{\nu\Delta t}{\Delta x^2}(u_{i+1}^{n}-2u_{i}^{n}+u_{i-1}^{n})
\end{equation*}

\subsubsection*{Implementiation.}
In python
\begin{minted}[breaklines]{python}
def diffus(u,nt,CFL):
    nx=len(u)
    dx = 2 / (nx - 1)
    nu = 0.3   
    sigma=CFL
    dt = sigma * dx**2 / nu 
    un=numpy.zeros(nx)
    for i in range(1,nt):
        un=u.copy()
        for j in range(1,nx-1):
            u[j]=un[j]+nu*dt/dx**2*(u[j+1]-2*u[j]+u[j-1])
    return u
\end{minted}

\subsection{2D Diffusion Equation}
In two-dimensional, we have
\begin{equation*}
  \frac{\partial u}{\partial t} = \nu \left(
  \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}
  \right)
\end{equation*}
Similar like the 1D counterpart, we only need to add the second spatial direction
\begin{equation*}
  \frac{u_{i,j}^{\,n+1} - u_{i,j}^{\,n}}{\Delta t}
  = \nu \left(
  \frac{u_{i+1,j}^{\,n} - 2 u_{i,j}^{\,n} + u_{i-1,j}^{\,n}}{\Delta x^2}
  + \frac{u_{i,j+1}^{\,n} - 2 u_{i,j}^{\,n} + u_{i,j-1}^{\,n}}{\Delta y^2}
  \right)
\end{equation*}
or
\begin{equation*}
  u_{i,j}^{\,n+1} = u_{i,j}^{\,n} + \nu \Delta t \left[
    \frac{u_{i+1,j}^{\,n} - 2 u_{i,j}^{\,n} + u_{i-1,j}^{\,n}}{\Delta x^2}
    + \frac{u_{i,j+1}^{\,n} - 2 u_{i,j}^{\,n} + u_{i,j-1}^{\,n}}{\Delta y^2}
    \right]
\end{equation*}
To make the computation easier, the diffusion constant is often used
\begin{equation*}
  \alpha_i =\nu \frac{\Delta t }{\Delta i^2}
\end{equation*}

\subsubsection{Implementiation.} As before.
\begin{minted}[breaklines]{python}
def diffuse2d(u,nt,CFL):
    nx = len(u)
    ny = len(u[0])
    dx = 2 / (nx - 1)
    dy = 2 / (ny - 1)

    nu = .05

    for n in range(nt + 1): 
        un = u.copy()
        u[1:-1, 1:-1] = (un[1:-1,1:-1] + 
                        nu * dt / dx**2 * 
                        (un[1:-1, 2:] - 2 * un[1:-1, 1:-1] + un[1:-1, 0:-2]) +
                        nu * dt / dy**2 * 
                        (un[2:,1: -1] - 2 * un[1:-1, 1:-1] + un[0:-2, 1:-1]))
        u[0, :] = 1
        u[-1, :] = 1
        u[:, 0] = 1
        u[:, -1] = 1
    return u

\end{minted}

\subsection{1D Burgers's Equation}
Burgers' equation is a combination of non-linear convection and diffusion, which in one spatial dimension looks like this
\begin{equation*}
  \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial ^2u}{\partial x^2}
\end{equation*}
The analytic solution is given by
\begin{gather*}
  u= -\frac{2 \nu}{\phi} \frac{\partial \phi}{\partial x} + 4 \\
  \phi = \exp \bigg(\frac{-(x-4t)^2}{4 \nu (t+1)} \bigg) + \exp \bigg(\frac{-(x-4t -2 \pi)^2}{4 \nu(t+1)} \bigg)
\end{gather*}
with initial conditions of
\begin{gather*}
  u = -\frac{2 \nu}{\phi} \frac{\partial \phi}{\partial x} + 4 \\
  \phi = \exp \bigg(\frac{-x^2}{4 \nu} \bigg) + \exp \bigg(\frac{-(x-2 \pi)^2}{4 \nu} \bigg)
\end{gather*}
and periodic boundary conditions of $u(0)=u(2\pi)$.

The discretized version is
\begin{equation*}
  \frac{u_i^{n+1}-u_i^n}{\Delta t} + u_i^n \frac{u_i^n - u_{i-1}^n}{\Delta x} = \nu \frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{\Delta x^2}
\end{equation*}
or
\begin{equation*}
  u_i^{n+1} = u_i^n - u_i^n \frac{\Delta t}{\Delta x} (u_i^n - u_{i-1}^n) + \nu \frac{\Delta t}{\Delta x^2}(u_{i+1}^n - 2u_i^n + u_{i-1}^n)
\end{equation*}

\subsubsection{Implementation.}
\begin{minted}[breaklines]{python}
def burger(u, nt, CFL):
    nx = len(u)
    dx = 2 * numpy.pi / (nx - 1)
    nu = 0.05
    dt = CFL * dx
    for n in range(nt):
        un = u.copy()
        for i in range(1, nx - 1):
            u[i] = (
                un[i]
                - un[i] * dt / dx * (un[i] - un[i - 1])
                + nu * dt / dx**2 * (un[i + 1] - 2 * un[i] + un[i - 1])
            )
        u[-1] = u[0]

    return u
\end{minted}

\subsection{2D Burger's Equation}
The coupled equations are as follows
$$
  \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} + v \frac{\partial u}{\partial y} = \nu \; \left(\frac{\partial ^2 u}{\partial x^2} + \frac{\partial ^2 u}{\partial y^2}\right)$$
$$
  \frac{\partial v}{\partial t} + u \frac{\partial v}{\partial x} + v \frac{\partial v}{\partial y} = \nu \; \left(\frac{\partial ^2 v}{\partial x^2} + \frac{\partial ^2 v}{\partial y^2}\right)$$
The discrete version is
\begin{multline*}
  \frac{u_{i,j}^{n+1} - u_{i,j}^n}{\Delta t} + u_{i,j}^n \frac{u_{i,j}^n-u_{i-1,j}^n}{\Delta x} + v_{i,j}^n \frac{u_{i,j}^n - u_{i,j-1}^n}{\Delta y}\\
  = \nu \left( \frac{u_{i+1,j}^n - 2u_{i,j}^n+u_{i-1,j}^n}{\Delta x^2} + \frac{u_{i,j+1}^n - 2u_{i,j}^n + u_{i,j-1}^n}{\Delta y^2} \right)
\end{multline*}
\begin{multline*}
  \frac{v_{i,j}^{n+1} - v_{i,j}^n}{\Delta t} + u_{i,j}^n \frac{v_{i,j}^n-v_{i-1,j}^n}{\Delta x} + v_{i,j}^n \frac{v_{i,j}^n - v_{i,j-1}^n}{\Delta y} = \\
  \nu \left( \frac{v_{i+1,j}^n - 2v_{i,j}^n+v_{i-1,j}^n}{\Delta x^2} + \frac{v_{i,j+1}^n - 2v_{i,j}^n + v_{i,j-1}^n}{\Delta y^2} \right)
\end{multline*}
And now, we will rearrange each of these equations for the only unknown: the two components $u,v$ of the solution at the next time step:
\begin{multline*}
  u_{i,j}^{n+1} = u_{i,j}^n - \frac{\Delta t}{\Delta x} u_{i,j}^n (u_{i,j}^n - u_{i-1,j}^n)  - \frac{\Delta t}{\Delta y} v_{i,j}^n (u_{i,j}^n - u_{i,j-1}^n) \\
  + \frac{\nu \Delta t}{\Delta x^2}(u_{i+1,j}^n-2u_{i,j}^n+u_{i-1,j}^n) + \frac{\nu \Delta t}{\Delta y^2} (u_{i,j+1}^n - 2u_{i,j}^n + u_{i,j-1}^n)
\end{multline*}
\begin{multline*}
  v_{i,j}^{n+1} =  v_{i,j}^n - \frac{\Delta t}{\Delta x} u_{i,j}^n (v_{i,j}^n - v_{i-1,j}^n) - \frac{\Delta t}{\Delta y} v_{i,j}^n (v_{i,j}^n - v_{i,j-1}^n) \\
  + \frac{\nu \Delta t}{\Delta x^2}(v_{i+1,j}^n-2v_{i,j}^n+v_{i-1,j}^n) + \frac{\nu \Delta t}{\Delta y^2} (v_{i,j+1}^n - 2v_{i,j}^n + v_{i,j-1}^n)
\end{multline*}

\subsubsection{Implementation}
\begin{minted}[breaklines]{python}
def burger2d(u,v,nt,CFL):
    nx = len(u)
    ny = nx
    c = 1
    dx = 2 / (nx - 1)
    dy = 2 / (ny - 1)
    nu = 0.01
    dt = CFL * dx * dy / nu

    for n in range(nt + 1): ##loop across number of time steps
        un = u.copy()
        vn = v.copy()

        u[1:-1, 1:-1] = (un[1:-1, 1:-1] -
                        dt / dx * un[1:-1, 1:-1] * 
                        (un[1:-1, 1:-1] - un[1:-1, 0:-2]) - 
                        dt / dy * vn[1:-1, 1:-1] * 
                        (un[1:-1, 1:-1] - un[0:-2, 1:-1]) + 
                        nu * dt / dx**2 * 
                        (un[1:-1,2:] - 2 * un[1:-1, 1:-1] + un[1:-1, 0:-2]) + 
                        nu * dt / dy**2 * 
                        (un[2:, 1:-1] - 2 * un[1:-1, 1:-1] + un[0:-2, 1:-1]))
        
        v[1:-1, 1:-1] = (vn[1:-1, 1:-1] - 
                        dt / dx * un[1:-1, 1:-1] *
                        (vn[1:-1, 1:-1] - vn[1:-1, 0:-2]) -
                        dt / dy * vn[1:-1, 1:-1] * 
                        (vn[1:-1, 1:-1] - vn[0:-2, 1:-1]) + 
                        nu * dt / dx**2 * 
                        (vn[1:-1, 2:] - 2 * vn[1:-1, 1:-1] + vn[1:-1, 0:-2]) +
                        nu * dt / dy**2 *
                        (vn[2:, 1:-1] - 2 * vn[1:-1, 1:-1] + vn[0:-2, 1:-1]))
        
        u[0, :] = 1
        u[-1, :] = 1
        u[:, 0] = 1
        u[:, -1] = 1
        
        v[0, :] = 1
        v[-1, :] = 1
        v[:, 0] = 1
        v[:, -1] = 1
    
    return u,v
\end{minted}

\subsection{2D Laplace Equation}
Here is Laplace's equation in 2D:
\begin{equation*}
  \frac{\partial ^2 p}{\partial x^2} + \frac{\partial ^2 p}{\partial y^2} = 0
\end{equation*}
Laplace's equation has the features typical of diffusion phenomena.
For this reason, it has to be discretized with central differences, so that the discretization is consistent with the physics we want to simulate.
Unlike convection equation, each iteration in a numerical solver does not correspond to any physical time evolution.
The iteration simply relaxes the solution toward the steady-state potential that satisfies the boundary conditions.

For the Laplace equation, the initial condition is entirely arbitrary.
Its only role is to provide a starting guess for the iterative solver.
The solver will converge to the unique steady-state solution dictated by the boundary conditions, regardless of the initial guess.
For comparison, in time-dependent equations like convection, the initial condition is physically meaningful because the solution evolves from it over time.

The discretized equation is:
\begin{equation*}
  \frac{p_{i+1, j}^n - 2p_{i,j}^n + p_{i-1,j}^n}{\Delta x^2} + \frac{p_{i,j+1}^n - 2p_{i,j}^n + p_{i, j-1}^n}{\Delta y^2} = 0
\end{equation*}
Let's rearrange the discretized equation, solving for $p_{i,j}^n$:
\begin{equation*}
  p_{i,j}^n = \frac{\Delta y^2(p_{i+1,j}^n+p_{i-1,j}^n)+\Delta x^2(p_{i,j+1}^n + p_{i,j-1}^n)}{2(\Delta x^2 + \Delta y^2)}
\end{equation*}
\end{document}