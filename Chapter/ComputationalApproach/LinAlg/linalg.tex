\documentclass[../../../main.tex]{subfiles}
\begin{document}
Many methods solving linear system here uses the definition of augmented matrix.
Consider the linear  equation in the form
\begin{equation*}
    \Omega x=W
\end{equation*}
with $\Omega$ as is the coefficient matrix, $x$ column vector of unknowns, and $W$ column vector of constants.
The augmented matrix $[\Omega|W]$ is simply the coefficient matrix $\Omega$ with the constant vector $W$ appended as an extra column.

For example a system in the form $\Omega x=W$
\begin{equation*}
    \begin{cases}
        \omega_{11}x_1 + \omega_{12}x_2 + \dots + \omega_{1n}x_n & = w_1, \\
                                                                 & \vdots \\
        \omega_{n1}x_1 + \omega_{n2}x_2 + \dots + \omega_{nn}x_n & = w_n.
    \end{cases}
\end{equation*}
Has the augmented matrix
\begin{equation*}
    [\Omega|W] =
    \begin{bmatrix}
        \omega_{11} & \omega_{12} & \cdots & \omega_{1n} & | & w_1    \\
        \omega_{21} & \omega_{22} & \cdots & \omega_{2n} & | & w_2    \\
        \vdots      & \vdots      & \ddots & \vdots      &   & \vdots \\
        \omega_{n1} & \omega_{n2} & \cdots & \omega_{nn} & | & w_n
    \end{bmatrix}
\end{equation*}

\subsection{Gaussian Elimination}
Gaussian elimination is an algorithm in linear algebra for solving systems of linear equations, computing the rank of a matrix, and finding the inverse of invertible matrices.
After forming the augmented matrix $A=[\Omega|W]$, the algorithm is as follows.

\begin{enumerate}
    \item \textbf{Pivoting.} Ensure that the pivot $a_{kk}$ is not zero.
    \item \textbf{Step 1: Pivot in column 1.}
          Choose $a_{11}$ as pivot (if $a_{11}=0$, swap with a lower row).
          For $i=2,\dots,n$, eliminate entries below the pivot.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{i1}}{a_{11}} R_1
          \end{equation*}
    \item \textbf{Step 2: Pivot in column 2.}
          The new pivot is $a_{22}^{(1)}$ (the updated entry in row 2, column 2).
          For $i=3,\dots,n$, eliminate the entries below.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{i2}^{(1)}}{a_{22}^{(1)}} R_2
          \end{equation*}
    \item \textbf{Step 3: Continue the process.}  At the $k$-th step, the pivot is $a_{kk}^{(k-1)}$.
          For each $i = k+1, \dots, n$.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{ik}^{(k-1)}}{a_{kk}^{(k-1)}} R_k
          \end{equation*}
    \item \textbf{Result:}
          After $n-1$ steps, the matrix is in row echelon (upper triangular) form:
          \[
              \begin{bmatrix}
                  \omega_{11}^{*} & \omega_{12}^{*} & \cdots & \omega_{1n}^{*} & w_1^{*} \\
                  0               & \omega_{22}^{*} & \cdots & \omega_{2n}^{*} & w_2^{*} \\
                  0               & 0               & \cdots & \omega_{3n}^{*} & w_3^{*} \\
                  \vdots          & \vdots          & \ddots & \vdots          & \vdots  \\
                  0               & 0               & \cdots & \omega_{nn}^{*} & w_n^{*}
              \end{bmatrix}
          \]

    \item \textbf{Back substitution:}
          The last row yields $\omega_{nn}^{*} x_n = w_n^{*}$, hence
          \[
              x_n = \frac{w_n^{*}}{\omega_{nn}^{*}}
          \]
          Substituting upward using
          \begin{equation*}
              x_i =\frac{\omega_i ^* -\sum_{j=i+i }^{n }w_{ij}^*x_j}{\omega_{ii }^*}
          \end{equation*}
          gives each preceding variable until $x_1$ is obtained.
          Thus, the solution vector $x$ is fully determined.
\end{enumerate}

\subsubsection{Implementation.}
In python.
\begin{minted}[breaklines]{python}
def gaussian_elimination(Omega, W):
\end{minted}
We define the function that takes \verb|Omega| the coefficient matrix $\Omega \in \mathbb{R}^{n \times n}$ and \verb|W| as the constant column vector $W \in \mathbb{R}^{n \times 1}$
\begin{minted}[breaklines]{python}
    W = W.reshape(-1, 1).astype(float)
\end{minted}
\verb|reshape(-1,1)| forces $W$ into shape $(n,1)$,  a column vector in other words.
\verb|.astype(float)| ensures floating-point arithmetic is used (required for divisions).
\begin{minted}[breaklines]{python}
    n = W.shape[0]
    A = np.hstack([Omega.astype(float), W]) 
\end{minted}
\verb|n| is the system size (number of rows).
\verb|np.hstack| horizontally concatenates the coefficient matrix and the constant column to form the augmented matrix $[\Omega|W]$.

Next for the pivot and forward elimination.
\begin{minted}[breaklines]{python}
    for k in range(n-1):
\end{minted}
This outer loop \verb|k| is the pivot row (from row 1 or index 0 to row \verb|n| or index \verb|n-1|).
\begin{minted}[breaklines]{python}
        pivot_row = np.argmax(np.abs(A[k:, k])) + k
\end{minted}
\verb|pivot_row| is the row containing the largest pivot candidate below or at the current diagonal position.
This is obtained by \verb|np.abs(A[k:,k])| taking the absolute value of column \verb|k|, starting form row \verb|k| to the last.
Then \verb|np.argmax()| return the index relative to the slice \verb|A[k:,k]|.
Lastly, \verb|+ k| is used to converts it to the correct row index in the full matrix.
\begin{minted}[breaklines]{python}
        if A[pivot_row, k] == 0:
            raise ValueError("Matrix is singular.")
\end{minted}
If the largest element in the column is zero, the pivot cannot be found, and the matrix is singular.
\begin{minted}[breaklines]{python}
        if pivot_row != k:
            A[[k, pivot_row]] = A[[pivot_row, k]]
\end{minted}
The row swapping \verb|A[[k, pivot_row]] = A[[pivot_row, k]]| is only performed when \verb|pivot_row| is not the same as \verb|k| using \verb|!=|.
\begin{minted}[breaklines]{python}
        pivot = A[k, k]
        for i in range(k+1, n):
            factor = A[i, k] / pivot
            A[i, k:] -= factor * A[k, k:] 
\end{minted}
\verb|pivot = A[k,k]| takes the pivot element $\omega_{kk}$ and the inner loop \verb|i| compute the factor
\begin{equation*}
    F=\frac{A[i,k]}{A[k,k]}=\frac{\omega_{ik }}{\omega_{kk}}
\end{equation*}
for each \verb|i|, the updates the row \verb|A[i,k]|.

Now we start the back substitution, since $A$ is in row echelon (upper triangular) form.
\begin{minted}[breaklines]{python}
    x = np.zeros((n,1))
    for i in range(n-1, -1, -1):
        x[i,0] = (A[i, -1] - np.dot(A[i, i+1:n], x[i+1:,0])) / A[i, i]
\end{minted}
Create solution vector \verb|x| as a column $(n,1)$, then iterate backwards from last row to first row.
For each row \verb|i|, use the back substitution formula
\begin{equation*}
    x_i =\frac{\omega_i ^* -\sum_{j=i+i }^{n }w_{ij}^*x_j}{\omega_{ii }^*}
\end{equation*}
Finally, simply return the solution vector \verb|x| and the augmented matrix in upper triangular form \verb|A|.
\begin{minted}[breaklines]{python}
    return x, A    
\end{minted}

\subsection{Gauss-Jordan Elimination}
If Gaussian elimination stops after forward elimination and relies on back substitution, Gauss–Jordan elimination extends the elimination upward (zeroing entries above pivots) and normalizes pivots to 1, so no back substitution is required.
The algorithm is similar with Gaussian elimination, however it diverges after the upper triangular augmented  matrix is obtained.
\begin{enumerate}
    \item \textbf{Result:} the matrix is now upper triangular like the case of Gaussian elimination
          \[
              \begin{bmatrix}
                  \omega_{11}^{*} & \omega_{12}^{*} & \cdots & \omega_{1n}^{*} & w_1^{*} \\
                  0               & \omega_{22}^{*} & \cdots & \omega_{2n}^{*} & w_2^{*} \\
                  0               & 0               & \cdots & \omega_{3n}^{*} & w_3^{*} \\
                  \vdots          & \vdots          & \ddots & \vdots          & \vdots  \\
                  0               & 0               & \cdots & \omega_{nn}^{*} & w_n^{*}
              \end{bmatrix}
          \]
          Also, make  sure  that the pivoting is really performed to obtain this matrix.
    \item \textbf{Step 4: Normalization of pivots.}
          Each pivot $a_{kk}$ is scaled to 1 with
          \begin{equation*}
              R_k \leftarrow \frac{1 }{a_{kk}}R_k
          \end{equation*}
    \item \textbf{Step 5: Elimination above the pivots.}
          The goal is to make the coefficient matrix into the identity.
          For each pivot column $k$, we remove nonzero entries above the pivot by subtracting suitable multiples of row $k$
          \begin{equation*}
              R_i \leftarrow R_i -a_{ik}R_k
          \end{equation*}
          where $a_{ik}$ is the entry currently in row $i$ column $k$.
\end{enumerate}

\subsubsection{Implementation.}
Roughly the same as Gaussian elimination, however we do not use the backwards substitution.
We use normalization and elimination above pivots
\begin{minted}[breaklines]{python}
    for k in range(n-1, -1, -1):  
        pivot = A[k, k]
        A[k, :] = A[k, :] / pivot 
        for i in range(k):
            factor = A[i, k]
            A[i, :] = A[i, :] - factor * A[k, :]
\end{minted}
This iterates over pivot positions starting from the last row $k=n-1$ up to the first row $k=0$.
This ensures we handle the bottom-most pivot first and move upward.
Then, current pivot is extracted from the diagonal entry \verb|A[k,k]| and being normalized.
The inner loop eliminates all entries above the pivot in column \verb|k|.
For each row \verb|i<k|, the multiplier (factor) is simply the entry \verb|A[i,k]|
Subtracting \verb|factor * A[k,:]| from row \verb|i| forces \verb|A[i,k]=0|.
\begin{minted}[breaklines]{python}
    sol = A[:, -1]
    return sol, A
\end{minted}
Finally, after finishing the loop, the left block of the augmented matrix is the identity matrix and the solution  with the augmented matrix are returned.

\subsection{LU Decomposition}
LU decomposition is essentially a formalization of Gaussian elimination without carrying along the right-hand side $W$.
Instead of modifying the augmented system, one factors the coefficient matrix $\Omega$ into the product of two triangular matrices
\begin{equation*}
    \Omega=LU
\end{equation*}
The element of $\Omega$ given by
\begin{equation*}
    (LU)_{ij}=\sum_{k=1 }^{n}l_{ik}u_{kj}
\end{equation*}
On rearranging, we can obtain the expression for $u_{ij}$ used in Doolittle method.
Here is the algorithm in Doolittle’s method.
\begin{enumerate}
    \item \textbf{Step 1: Compute $U$ and $L$ entries simultaneously.}
          Using
          \begin{equation*}
              u_{ij} = \omega_{ij} - \sum_{k=1}^{i-1} l_{ik} u_{kj}
          \end{equation*}
          Since $U$ is upper triangular, only $k \leq i$ terms contribute on the expression for the element of $\Omega$
          And using
          \begin{equation*}
              l_{ij} = \frac{1}{u_{jj}} \left( \omega_{ij} - \sum_{k=1}^{j-1} l_{ik} u_{kj} \right)
          \end{equation*}
          By the Doolittle method, we define $l_{ii}=1$ for all $i$
    \item \textbf{Step 2: Forward substitution.} Define $Ux=y$, then solve
          \begin{equation*}
              Ly=W
          \end{equation*}
          by forward substitution.
          For $i = 1,2,\ldots,n$,
          \begin{equation*}
              y_i = W_i - \sum_{j=1}^{i-1} l_{i,j} y_j,
          \end{equation*}
    \item \textbf{Step 3: Backwards substitution.} From the defined equation, we solve
          \begin{equation*}
              Ux=y
          \end{equation*}
          with backwards substitution.
          For $i = n, n-1, \ldots, 1$,
          \begin{equation*}
              x_i = \frac{1}{u_{i,i}} \left( y_i - \sum_{j=i+1}^{n} u_{i,j} x_j \right)
          \end{equation*}
\end{enumerate}

\subsubsection{Implementation.}
As usual, we write in python.
Here we create two code, first to create the decomposition and to actually solve it.
\begin{minted}[breaklines]{python}
def lu_decomposition(Omega):
\end{minted}
We take \verb|Omega|, a square matrix size $(n \times n)$.
\begin{minted}[breaklines]{python}
    n = Omega.shape[0]
    L = np.zeros((n, n))
    U = np.zeros((n, n))
\end{minted}
We determine the size of the square matrix \verb|Omega|.
Then we prepare two $(n \times  b)$ zero matrices \verb|L| and \verb|U| that will be filled progressively.
\begin{minted}[breaklines]{python}
    for j in range(i, n):
        U[i, j] = Omega[i, j] - sum(L[i, k] * U[k, j] for k in range(i))
\end{minted}
This computes row \verb|i| of \verb|U|.
\begin{minted}[breaklines]{python}
    for j in range(i, n):
        if i == j:
            L[i, i] = 1.0
        else:
            L[j, i] = (Omega[j, i] - sum(L[j, k] * U[k, i] for k in range(i))) / U[i, i]
\end{minted}
This computes column \verb|i| of \verb|L|.
If \verb|i==j|, $l_{ij}=1$ since it is the diagonal.
Else, we compute it normally.
\begin{minted}[breaklines]{python}
    return L, U
\end{minted}
Finally, we return the decomposed matrices \verb|L| and \verb|U|.

Next we move to the function to solve the linear system using the decomposed matrix and the right-hand constant.
\begin{minted}[breaklines]{python}
def lu_solver(L, U, W):
\end{minted}
Here \verb|L| and \verb|U| act as the decomposed matrix, while \verb|W| is right-hand side constant from $\Omega x=W$.
\begin{minted}[breaklines]{python}
    y = np.zeros((n, 1))
    for i in range(n):
        y[i, 0] = W[i, 0] - sum(L[i, j] * y[j, 0] for j in range(i))
\end{minted}
The forward substitution for the equation $Ly=W$.
It works by allocating \verb|y| as a vector of zeros, then for each row \verb|i|, compute \verb|y[i]|
\begin{equation*}
    y_i=W_i-\sum_{k=0 }^{i-1 }l_{ij}y_{j}
\end{equation*}
Because \verb|L| is lower triangular with unit diagonal, the diagonal entry \verb|L[i,i]| is 1, so it does not appear in the denominator.
\begin{minted}[breaklines]{python}
    x = np.zeros_like(W, dtype=float)
    for i in range(n - 1, -1, -1):
        x[i] = (y[i] - sum(U[i, j] * x[j] for j in range(i + 1, n))) / U[i, i]
\end{minted}
The backwards substitution for the equation $Ux=y$.
It works by allocating \verb|x| as a vector of zeros, then for each row \verb|i| in decreasing order from bottom row \verb|n-1| to top row 0, compute \verb|y[i]|
\begin{equation*}
    x_i=\frac{1 }{U_{ii}}\left( y_i-\sum_{k=i+1 }^{n-1 }u_{ik}x_{k} \right)
\end{equation*}
The backward order is necessary because each \verb|x[i]| depends on \verb|x[j]| for \verb|k > i|.
\begin{minted}[breaklines]{python}
    return x
\end{minted}
Finally, all that left is to return the solution \verb|x|.
\end{document}