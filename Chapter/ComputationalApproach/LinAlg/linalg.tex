\documentclass[../../../main.tex]{subfiles}
\begin{document}
Many methods solving linear system here uses the definition of augmented matrix.
Consider the linear  equation in the form
\begin{equation*}
    \Omega x=W
\end{equation*}
with $\Omega$ as is the coefficient matrix, $x$ column vector of unknowns, and $W$ column vector of constants.
The augmented matrix $[\Omega|W]$ is simply the coefficient matrix $\Omega$ with the constant vector $W$ appended as an extra column.

For example a system in the form $\Omega x=W$
\begin{equation*}
    \begin{cases}
        \omega_{11}x_1 + \omega_{12}x_2 + \dots + \omega_{1n}x_n & = w_1, \\
                                                                 & \vdots \\
        \omega_{n1}x_1 + \omega_{n2}x_2 + \dots + \omega_{nn}x_n & = w_n.
    \end{cases}
\end{equation*}
Has the augmented matrix
\begin{equation*}
    [\Omega|W] =
    \begin{bmatrix}
        \omega_{11} & \omega_{12} & \cdots & \omega_{1n} & | & w_1    \\
        \omega_{21} & \omega_{22} & \cdots & \omega_{2n} & | & w_2    \\
        \vdots      & \vdots      & \ddots & \vdots      &   & \vdots \\
        \omega_{n1} & \omega_{n2} & \cdots & \omega_{nn} & | & w_n
    \end{bmatrix}
\end{equation*}

\subsection{Gaussian Elimination}
Gaussian elimination is an algorithm in linear algebra for solving systems of linear equations, computing the rank of a matrix, and finding the inverse of invertible matrices.
After forming the augmented matrix $A=[\Omega|W]$, the algorithm is as follows.

\begin{enumerate}
    \item \textbf{Step 1: Pivot in column 1.}
          Choose $a_{11}$ as pivot (if $a_{11}=0$, swap with a lower row).
          For $i=2,\dots,n$, eliminate entries below the pivot.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{i1}}{a_{11}} R_1
          \end{equation*}
    \item \textbf{Step 2: Pivot in column 2.}
          The new pivot is $a_{22}^{(1)}$ (the updated entry in row 2, column 2).
          For $i=3,\dots,n$, eliminate the entries below.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{i2}^{(1)}}{a_{22}^{(1)}} R_2
          \end{equation*}
    \item \textbf{Step 3: Continue the process.}  At the $k$-th step, the pivot is $a_{kk}^{(k-1)}$.
          For each $i = k+1, \dots, n$.
          \begin{equation*}
              R_i \;\leftarrow\; R_i - \frac{a_{ik}^{(k-1)}}{a_{kk}^{(k-1)}} R_k
          \end{equation*}
    \item \textbf{Result:}
          After $n-1$ steps, the matrix is in row echelon (upper triangular) form:
          \[
              \begin{bmatrix}
                  \omega_{11}^{*} & \omega_{12}^{*} & \cdots & \omega_{1n}^{*} & w_1^{*} \\
                  0               & \omega_{22}^{*} & \cdots & \omega_{2n}^{*} & w_2^{*} \\
                  0               & 0               & \cdots & \omega_{3n}^{*} & w_3^{*} \\
                  \vdots          & \vdots          & \ddots & \vdots          & \vdots  \\
                  0               & 0               & \cdots & \omega_{nn}^{*} & w_n^{*}
              \end{bmatrix}
          \]

    \item \textbf{Back substitution:}
          The last row yields $\omega_{nn}^{*} x_n = w_n^{*}$, hence
          \[
              x_n = \frac{w_n^{*}}{\omega_{nn}^{*}}
          \]
          Substituting upward gives each preceding variable until $x_1$ is obtained.
          Thus the solution vector $x$ is fully determined.
\end{enumerate}

\subsubsection{Example.}
Let us consider the following $3 \times 3$ system in the form $\Omega x=W$
\begin{equation*}
    \begin{bmatrix}
        2  & 1  & -1 \\
        -3 & -1 & 2  \\
        -2 & 1  & 2
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
        8 \\ -11 \\ -3
    \end{bmatrix}
\end{equation*}

\text{The augmented matrix is:}\begin{equation*}
    \quad
    [\Omega|W] =
    \begin{bmatrix}
        2  & 1  & -1 & 8   \\
        -3 & -1 & 2  & -11 \\
        -2 & 1  & 2  & -3
    \end{bmatrix}
\end{equation*}
\begin{enumerate}
    \item \textbf{Pivot in column 1.}
          Using $\omega_{11} = 2$ as pivot, we eliminate below row.
          For row 2:
          \begin{equation*}
              R_2 \leftarrow R_2 - \left(\frac{-3}{2}\right)R_1 \quad \Rightarrow \quad
              R_2=
              \begin{bmatrix}
                  -3 \\-1\\2\\-1\\
              \end{bmatrix}
              - \frac{3 }{2}
              \begin{bmatrix}
                  2 \\1\\-1\\8
              \end{bmatrix}
              =
              \begin{bmatrix}
                  0 \\ 0.5 \\ 0.5 \\ 1
              \end{bmatrix}
          \end{equation*}
          For row 3:
          \begin{equation*}
              R_3 \leftarrow R_3 - \left(\frac{-2}{2}\right)R_1 \quad \Rightarrow \quad
              R_3=
              \begin{bmatrix}
                  -2 \\1\\2\\-3\\
              \end{bmatrix}
              +
              \begin{bmatrix}
                  2 \\1\\-1\\8
              \end{bmatrix}
              =
              \begin{bmatrix}
                  0 \\ 2 \\ 1 \\ 5
              \end{bmatrix}
          \end{equation*}
          So the matrix becomes:
          \begin{equation*}
              \begin{bmatrix}
                  2 & 1   & -1  & 8 \\
                  0 & 0.5 & 0.5 & 1 \\
                  0 & 2   & 1   & 5
              \end{bmatrix}
          \end{equation*}
    \item \textbf{Pivot in column 2.}
          Pivot is $\omega_{22}^{(1)} = 0.5$.
          Eliminate below, only row 3 though
          \begin{equation*}
              R_3 \leftarrow R_3 - \frac{2}{0.5} R_2 \quad \Rightarrow \quad
              R_3=
              \begin{bmatrix}
                  0 \\2\\1\\5\\
              \end{bmatrix}
              -4
              \begin{bmatrix}
                  - \\0.5\\0.5\\1
              \end{bmatrix}
              =
              \begin{bmatrix}
                  0 \\0\\-1\\1
              \end{bmatrix}
          \end{equation*}
          Thus:
          \begin{equation*}
              \begin{bmatrix}
                  2 & 1   & -1  & 8 \\
                  0 & 0.5 & 0.5 & 1 \\
                  0 & 0   & -1  & 1
              \end{bmatrix}
          \end{equation*}

    \item \textbf{Step 3: Pivot in column 3.}
          Pivot is $\omega_{33}^{(2)} = -1$. No elimination is needed because there are no rows below.
          The system is now in upper triangular form.
    \item \textbf{Back substitution:}
          From the last row:
          \begin{equation*}
              - x_3 = 1 \quad \Rightarrow \quad x_3 = -1.
          \end{equation*}

          From the second row:
          \begin{equation*}
              0.5x_2 + 0.5x_3 = 1 \quad \Rightarrow \quad 0.5x_2 - 0.5 = 1 \quad \Rightarrow \quad x_2 = 3.
          \end{equation*}

          From the first row:
          \begin{equation*}
              2x_1 + x_2 - x_3 = 8 \quad \Rightarrow \quad 2x_1 + 3 - (-1) = 8 \quad \Rightarrow \quad x_1 = 2.
          \end{equation*}
    \item \textbf{Final solution:}
          \begin{equation*}
              x =
              \begin{bmatrix}
                  2 \\ 3 \\ -1
              \end{bmatrix}
          \end{equation*}
\end{enumerate}

\subsubsection{Implementation.}
In python.
\begin{minted}[breaklines]{python}
def gaussian_elimination(Omega, W):
\end{minted}
We define the function that takes \verb|Omega| the coefficient matrix $\Omega \in \mathbb{R}^{n \times n}$ and \verb|W| as the constant column vector $W \in \mathbb{R}^{n \times 1}$
\begin{minted}[breaklines]{python}
    W = W.reshape(-1, 1).astype(float)
\end{minted}
\verb|reshape(-1,1)| forces $W$ into shape $(n,1)$,  a column vector in other words.
\verb|.astype(float)| ensures floating-point arithmetic is used (required for divisions).
\begin{minted}[breaklines]{python}
    n = W.shape[0]
    A = np.hstack([Omega.astype(float), W]) 
\end{minted}
\verb|n| is the system size (number of rows).
\verb|np.hstack| horizontally concatenates the coefficient matrix and the constant column to form the augmented matrix $[\Omega|W]$.
\begin{minted}[breaklines]{python}
    for k in range(n-1):
        pivot = A[k, k]
        for i in range(k+1, n):
            factor = A[i, k] / pivot
            A[i, k:] = A[i, k:] - factor * A[k, k:] 
\end{minted}
This loop creates zeros below the pivot.
Outer loop \verb|k| is the pivot row (from row 0 to row n-2).
\verb|pivot = A[k,k]| takes the pivot element $\omega_{kk}$.
Inner loop \verb|i| compute the factor
\begin{equation*}
    F=\frac{A[i,k]}{A[k,k]}=\frac{\omega_{ik }}{\omega_{kk}}
\end{equation*}
for each \verb|i|, the updates the row \verb|A[i,k]|.
After this phase, $A$ is in row echelon (upper triangular) form.
\begin{minted}[breaklines]{python}
    x = np.zeros((n,1))
    for i in range(n-1, -1, -1):
        x[i,0] = (A[i, -1] - np.dot(A[i, i+1:n], x[i+1:,0])) / A[i, i]
\end{minted}
Create solution vector \verb|x| as a column $(n,1)$, then iterate backwards from last row to first row.
For each row \verb|i|, use the back substitution formula
\begin{equation*}
    x_i =\frac{\omega_i ^* -\sum_{j=i+i }^{n }w_{ij}^*x_j}{\omega_{ii }^*}
\end{equation*}
Finally, simply return the solution vector \verb|x| and the augmented matrix in upper triangular form \verb|A|.
\begin{minted}[breaklines]{python}
    return x, A    
\end{minted}

\subsection{Gauss-Jordan Elimination}
If Gaussian elimination stops after forward elimination and relies on back substitution, Gaussâ€“Jordan elimination extends the elimination upward (zeroing entries above pivots) and normalizes pivots to 1, so no back substitution is required.
The algorithm is similar with Gaussian elimination, however it diverges after the upper triangular augmented  matrix is obtained.
\begin{enumerate}
    \item \textbf{Result:} the matrix is now upper triangular like the case of Gaussian elimination
          \[
              \begin{bmatrix}
                  \omega_{11}^{*} & \omega_{12}^{*} & \cdots & \omega_{1n}^{*} & w_1^{*} \\
                  0               & \omega_{22}^{*} & \cdots & \omega_{2n}^{*} & w_2^{*} \\
                  0               & 0               & \cdots & \omega_{3n}^{*} & w_3^{*} \\
                  \vdots          & \vdots          & \ddots & \vdots          & \vdots  \\
                  0               & 0               & \cdots & \omega_{nn}^{*} & w_n^{*}
              \end{bmatrix}
          \]
    \item \textbf{Step 4: Normalization of pivots.}
          Each pivot $a_{kk}$ is scaled to 1 with
          \begin{equation*}
              R_k \leftarrow \frac{1 }{a_{kk}}R_k
          \end{equation*}
    \item \textbf{Step 5: Elimination above the pivots.}
          The goal is to make the coefficient matrix into the identity.
          For each pivot column $k$, we remove nonzero entries above the pivot by subtracting suitable multiples of row $k$
          \begin{equation*}
              R_i \leftarrow R_i -a_{ik}R_k
          \end{equation*}
          where $a_{ik}$ is the entry currently in row $i$ column $k$.
\end{enumerate}

\subsubsection{Example.} Using previous example, the upper triangular form is
\begin{equation*}
    \begin{bmatrix}
        2 & 1   & -1  & 8 \\
        0 & 0.5 & 0.5 & 1 \\
        0 & 0   & -1  & 1
    \end{bmatrix}
\end{equation*}
\begin{enumerate}
    \item \textbf{Normalization.}
          For the first row,
          \begin{equation*}
              R_1 \leftarrow \frac{1 }{2 }R_1\qquad\Rightarrow\qquad R_1=
              \begin{bmatrix}
                  1 \\1/2\\-1/2\\4
              \end{bmatrix}
          \end{equation*}
          For the second row,
          \begin{equation*}
              R_2 \leftarrow 2R_2 \qquad\Rightarrow\qquad R_2=
              \begin{bmatrix}
                  0 \\1\\1\\2
              \end{bmatrix}
          \end{equation*}
          For the third row,
          \begin{equation*}
              R_3 \leftarrow -R_3 \qquad\Rightarrow\qquad R_3=
              \begin{bmatrix}
                  0 \\0\\1\\-1
              \end{bmatrix}
          \end{equation*}
          So the matrix becomes:
          \begin{equation*}
              \begin{bmatrix}
                  1 & 1/2 & -1/2 & 4 \\
                  0 & 1   & 1    & 2 \\
                  0 & 0   & 1    & 1
              \end{bmatrix}
          \end{equation*}
    \item \textbf{Elimination above the pivot.}
          For pivot in row 3
          \begin{equation*}
              \begin{cases}
                  R_2\leftarrow R_2-R_3 \\
                  R_1\leftarrow R_1-\frac{1 }{2}R_2
              \end{cases}
              \qquad \Rightarrow \qquad
              A=
              \begin{bmatrix}
                  1 & 1/2 & 0 & 3.5 \\
                  0 & 1   & 0 & 3   \\
                  0 & 0   & 1 & 1
              \end{bmatrix}
          \end{equation*}
          For pivot in row 2
          \begin{equation*}
              R_1 \leftarrow R_1-\frac{1 }{2 }R_2
              \qquad \Rightarrow \qquad
              A=
              \begin{bmatrix}
                  1 & 0 & 0 & 2 \\
                  0 & 1 & 0 & 3 \\
                  0 & 0 & 1 & 1
              \end{bmatrix}
          \end{equation*}
    \item \textbf{Final solution:} The same as Gaussian elimination
          \begin{equation*}
              x =
              \begin{bmatrix}
                  2 \\ 3 \\ -1
              \end{bmatrix}
          \end{equation*}
\end{enumerate}
\end{document}