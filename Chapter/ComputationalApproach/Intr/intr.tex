\documentclass[../../../main.tex]{subfiles}
\begin{document}
Interpolation is a method used to construct a function that exactly passes through a given set of data points. It assumes that the data are precise and free from measurement noise.
The primary goal is to reconstruct or estimate the values of the underlying function at intermediate points within the range of the observed data.
Unlike regression, which tolerates deviations between the model and the data due to noise, interpolation enforces an exact fit at each known point.

\subsection{Lagrange Interpolation}
Lagrange's interpolation is a polynomial interpolation method that constructs a unique polynomial of degree at most $n$ passing exactly through $n+1$ given data points \((x_i, F_i)\) for \(i = 1, \dots, N\).
The interpolating polynomial is written as
\[
    P_n(x) = \sum_{i=0}^{n} y_i L_i(x)
\]
where \( L_i(x) \) are the Lagrange basis polynomials defined by
\[
    L_i(x) = \prod_{\substack{j=0 \\ j \ne i}}^{n} \frac{x - x_j}{x_i - x_j}
\]

\subsubsection{Implementation.}
In python
\begin{minted}[breaklines]{python}
def LagInt(x_points, F_points):
\end{minted}
We define a function named LagInt that takes two arguments.
\begin{minted}[breaklines]{python}
    x = sp.Symbol("x")
    n = len(x_points)
    P = 0
\end{minted}
We Create a symbolic variable \verb|x| using SymPy, compute the number of data points, and initialize the polynomial accumulator \verb|P|.
\begin{minted}[breaklines]{python}
    for i in range(n):
        L = 1
        for j in range(n):
            if i != j:
                L *= (x - x_points[j]) / (x_points[i] - x_points[j])
        P += F_points[i] * L
    
    return sp.expand(P)
\end{minted}
The outer loop constructs each term, the inner loop builds the product for $L_i(x)$ and \verb|P| accumulates all terms into the complete interpolating polynomial.
Then returns the final Lagrange interpolating polynomial in expanded form.
The usage is as follows.
\begin{minted}[breaklines]{python}
x=sp.Symbol("x")
x_graph = np.linspace(min(x_i), max(x_i), 100)
inter= sp.lambdify(x,LagInt(x_i,F_i),"numpy")
\end{minted}

\subsection{Linear Interpolation}
Linear interpolation is a method for estimating the value of a function $f(x)$  at a point $ x$  between two known data points  $(x_0, y_0)$  and $ (x_1, y_1)$.
It assumes that the function varies linearly between these points.
The formula is derived from the equation of a straight line passing through the two points
\begin{equation*}
    y = y_0 + \frac{y_1 - y_0}{x_1 - x_0} (x - x_0)
\end{equation*}

The algorithm is as follows.
\begin{enumerate}
    \item \textbf{Input.} Two consecutive points $(x_0, y_0)$ and $(x_1, y_1)$ such that $x_i < x_{i+1}$. If $x$ lies outside the known points, interpolation is not possible without extrapolation.
    \item \textbf{Compute the gradient.} Use the following expression
          \begin{equation*}
              m= \frac{y_{i+1 }-x_i}{x_{i+1}-x_i}
          \end{equation*}
    \item \textbf{Return the value.} Return the value of $y$ with
          \begin{equation*}
              y(x)=y_i+m(x-x_i)
          \end{equation*}
\end{enumerate}

\end{document}