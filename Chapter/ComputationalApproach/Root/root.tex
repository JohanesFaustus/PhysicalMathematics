\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Bisection Method}

The bisection method is a numerical algorithm for approximating a root of a continuous function
\[
    f : \mathbb{R} \to \mathbb{R}.
\]
Convergence is guaranteed if $f$ is continuous and $f(a)f(b)<0$.
The convergence is linear, i.e., the error decreases by a factor of $\tfrac{1}{2}$ per iteration.
The method is robust but slower than Newton-Raphson.

\subsubsection{Preconditions.}
Let $f$ be continuous on the interval $[a,b]$.
If
\[
    f(a) \cdot f(b) < 0,
\]
then by the Intermediate Value Theorem, there exists at least one root $\xi \in (a,b)$ such that $f(\xi) = 0$.

Algorithm are as following.
\begin{enumerate}
    \item \textbf{Input.} Define function $f$ and two point $[x_1$, $x_2]$ as out initial guess. Also define $\varepsilon$, where $\varepsilon > 0$ is the prescribed tolerance.
    \item \textbf{Midpoint.} Compute the midpoint:
          \begin{equation*}
              x_t= \frac{x_1+x_2}{2}.
          \end{equation*}
    \item \textbf{Evaluation and loop.} Evaluate $f(x_t)$, then
          \begin{enumerate}
              \item If $f(x_t) = 0$, then $x_t$ is the root.
              \item If $f(x_1) f(x_t) < 0$, then set $x_2 \leftarrow x_t$. This condition implies that the root lies between $[x_1,x_t]$.
              \item Otherwise, set $x_1 \leftarrow x_t$, since $f(x_1)f(x_t)>0$ imply that the root lies on $[x_t,x_2]$.
          \end{enumerate}
          Repeat until the interval length is sufficiently small:
          \begin{equation*}
              |x_2 - x_1| < \varepsilon
          \end{equation*}
          or until
          \begin{equation*}
              |f(x_t)| < \varepsilon,
          \end{equation*}
\end{enumerate}

\subsubsection{Implementation.}
In python

\begin{minted}{python}
def bisec(f,x1,x2):
    loop=0
    xt=(x1+x2)/2
    eps=10**(-17)
    if f(x1)*f(x2)>=0:
        raise ValueError("no root within this range")
    else:
        while np.abs(f(xt))>eps and np.abs(x1-x2)>eps:
            if f(x1)*f(xt)<0:
                x2=xt 
            else:
                x1=xt 
            xt=(x1+x2)/2
            loop+=1
        print( xt,loop)
\end{minted}

\subsection{Newton–Raphson Method}
Recall the Taylor series for continuous function
\begin{align*}
    f(x) & =         \frac{1}{n!}(x-a)^n f^{n}(a)              \\
         & =   f(a)+(x-a)f'(a)+\frac{1}{2!}(x-a)^2f''(a)+\dots
\end{align*}
If we use $x=x_r$
\begin{align*}
    f(x_r) & =  f(a)+(x_r-a)f'(a)+\frac{(x_r-a)^2}{2!}f''(a)+\dots=0
\end{align*}
and $a \approx x_r$
\begin{equation*}
    f(a)+(x_r-a)f'(a)=0
\end{equation*}
We obtain the Newton–Raphson iteration
\begin{equation*}
    x_r=a-\frac{f(a )}{f'(a)}
\end{equation*}

\subsubsection{Implementation.}
As shrimple as this
\begin{minted}{python}
def NR(f,df,a,eps=10**(-17)):
    loop=0 
    xr=0
    while np.abs(f(a))>eps:
        xr=a-f(a)/df(a)
        a=xr 
        loop+=1
    return xr,loop
\end{minted}

\subsection{Hybrid Newton-Raphson Bisection Method}
This method combine the Newton-Raphson with Bisection method, it is designed to exploit the rapid convergence of Newton–Raphson while preserving the guaranteed convergence of bisection.
The bisection method serves as a safeguard to guarantee the existence of a root within the chosen interval.
Once such an interval is established, the Newton–Raphson method is applied to accelerate convergence toward the root.

\subsubsection*{Implementation.} As always, in python
\begin{minted}{python}
def BNR(f,dfdx,xi,xf,dx,eps=10**(-6)):
    roots=[]
    loops=[]
    a=xi
    while a<xf:
        b=a+dx 
        if b>xf:
            break
        if f(a)*f(b)<=0:
            xr=(a+b)/2
            loop=0 
            while np.abs(f(xr))>eps and loop<100:
                xr=xr-f(xr)/dfdx(f,xr)
                loop+=1
            roots.append(float(xr))
            loops.append(loop)
        a=b
    if not roots:
        raise ValueError("No root found in interval")
    print( roots,loops)
\end{minted}

As a bonus, this function can be used to find the root of the Bessel function, which can be written as
\begin{minted}[breaklines]{python}
    def Jn(x, n, eps=10**(-6)):
    xo2 = x / 2.0
    x2 = xo2 * xo2
    a0 = 1.0

    for i in range(1, n+1):
        a0 = a0 * xo2 / i
    
    ss = a0
    error = a0
    k = 1
    
    while error > eps:
        a0 = -a0 * x2 / ((n + k) * k)
        ss = ss + a0
        error = abs(a0)
        k += 1
    
    return ss
\end{minted}
To explain the code, first recall the Bessel function
\begin{equation*}
    J_n(x) = \sum_{k=0}^{\infty} \frac{(-1)^k}{k! \, (n+k)!} \left( \frac{x}{2} \right)^{2k+n}
\end{equation*}
The $k$-th term then may be written
\begin{equation*}
    a_k = \frac{(-1)^k}{k! \, (n+k)!} \left( \frac{x}{2} \right)^{2k+n}
\end{equation*}
To avoid using factorial, we write the $k+1$-term in terms of $k$-th term
\begin{align*}
    a_{k+1} & = \frac{(-1)^{k+1}(x/2)^{2k+n+2}}{(k+1)!(n+k+1)!}          \\
            & = -\frac{(-1)^k(x/2)^2(x/2)^{2k+n }}{(k+1)k!(n+k+1)(n+k)!} \\
    a_{k+1} & = -a_k\frac{(x/2)^2}{(k+1)(n+k+1)}
\end{align*}
Using this, we initialize the first term as
\begin{equation*}
    a_0=\frac{1 }{n!}\left( \frac{x }{2 } \right) ^n
\end{equation*}
\end{document}