\documentclass[../../../main.tex]{subfiles}
\begin{document}
\subsection{Bisection Method}

The bisection method is a numerical algorithm for approximating a root of a continuous function
\[
    f : \mathbb{R} \to \mathbb{R}.
\]
Convergence is guaranteed if $f$ is continuous and $f(a)f(b)<0$.
The convergence is linear, i.e., the error decreases by a factor of $\tfrac{1}{2}$ per iteration.
The method is robust but slower than Newton-Raphson.

\subsubsection{Preconditions.}
Let $f$ be continuous on the interval $[a,b]$.
If
\[
    f(a) \cdot f(b) < 0,
\]
then by the Intermediate Value Theorem, there exists at least one root $\xi \in (a,b)$ such that $f(\xi) = 0$.

Algorithm are as following.
\begin{enumerate}
    \item \textbf{Input.} Define function $f$ and two point $[x_1$, $x_2]$ as out initial guess. Also define $\varepsilon$, where $\varepsilon > 0$ is the prescribed tolerance.
    \item \textbf{Midpoint.} Compute the midpoint:
          \begin{equation*}
              x_t= \frac{x_1+x_2}{2}.
          \end{equation*}
    \item \textbf{Evaluation and loop.} Evaluate $f(x_t)$, then
          \begin{enumerate}
              \item If $f(x_t) = 0$, then $x_t$ is the root.
              \item If $f(x_1) f(x_t) < 0$, then set $x_2 \leftarrow x_t$. This condition implies that the root lies between $[x_1,x_t]$.
              \item Otherwise, set $x_1 \leftarrow x_t$, since $f(x_1)f(x_t)>0$ imply that the root lies on $[x_t,x_2]$.
          \end{enumerate}
          Repeat until the interval length is sufficiently small:
          \begin{equation*}
              |x_2 - x_1| < \varepsilon
          \end{equation*}
          or until
          \begin{equation*}
              |f(x_t)| < \varepsilon,
          \end{equation*}
\end{enumerate}

\subsubsection{Implementation.}
In python
\begin{minted}{python}
def bisec(f,x1,x2):
\end{minted}
We define function to accept the polynomials \verb|f| and the interval \verb|x1,x2|.
\begin{minted}[breaklines]{python}
    loop=0
    xt=(x1+x2)/2
    eps=10**(-17)
\end{minted}
Initializing the nessecarry variable.
\begin{minted}[breaklines]{python}
    if f(x1)*f(x2)>=0:
        raise ValueError("no root within this range")
\end{minted}
Initial check to determine that there exists a root within interval \verb|x1,x2|.
\begin{minted}[breaklines]{python}
    else:
        while np.abs(f(xt)) > eps and np.abs(x1 - x2) > eps:
\end{minted}
Begins an iterative process that continues until $|f(x_t)|$ is smaller than the $\epsilon$, or the distance between x1 and x2 becomes smaller than the tolerance.
\begin{minted}[breaklines]{python}
            if f(x1) * f(xt) < 0:
                x2 = xt
            else:
                x1 = xt
\end{minted}
If $f(x_1) f(x_t) < 0$, then set $x_2 \leftarrow x_t$, otherwise set $x_1 \leftarrow x_t$.
\begin{minted}[breaklines]{python}
            xt = (x1 + x2) / 2
            loop += 1
\end{minted}
Updates the midpoint after the interval has been reduced and increments the iteration count.
\begin{minted}[breaklines]{python}
        return xt, loop
\end{minted}
Returns the last midpoint and total number of iterations.

\subsection{Newton–Raphson Method}
Recall the Taylor series for continuous function
\begin{align*}
    f(x) & =         \frac{1}{n!}(x-a)^n f^{n}(a)              \\
         & =   f(a)+(x-a)f'(a)+\frac{1}{2!}(x-a)^2f''(a)+\dots
\end{align*}
If we use $x=x_r$
\begin{align*}
    f(x_r) & =  f(a)+(x_r-a)f'(a)+\frac{(x_r-a)^2}{2!}f''(a)+\dots=0
\end{align*}
and $a \approx x_r$
\begin{equation*}
    f(a)+(x_r-a)f'(a)=0
\end{equation*}
We obtain the Newton–Raphson iteration
\begin{equation*}
    x_r=a-\frac{f(a )}{f'(a)}
\end{equation*}

\subsubsection{Implementation.}
As shrimple as this
\begin{minted}{python}
def NR(f, df, a):
\end{minted}
We define function to accept the polynomials \verb|f|, the derivative \verb|df| and the initial guess \verb|a|.
\begin{minted}[breaklines]{python}
    eps = 10 ** (-17)
    loop = 0
    xr = 0
\end{minted}
Sets the convergence tolerance \verb|eps|, initializes iteration counter \verb|loop| and result variable \verb|xr|.
\begin{minted}[breaklines]{python}
    while np.abs(f(a)) > eps:
        xr = a - f(a) / df(f, a)
        a = xr
        loop += 1
\end{minted}
The loop continues until the function value is sufficiently close to zero.
Then it /implements the Newton–Raphson update rule and  increase the \verb|loop| count.
\begin{minted}[breaklines]{python}
    return xr, loop
\end{minted}
Finally, it returns the root \verb|xr| and the iteration count \verb|loop|.

The derivative can be computed using the central finite difference
\begin{minted}[breaklines]{python}
def dfdx(f, x):
    h = 10 ** (-5)
    return (f(x + h) - f(x - h)) / (2 * h)
\end{minted}

\subsection{Hybrid Newton-Raphson Bisection Method}
This method combine the Newton-Raphson with Bisection method, it is designed to exploit the rapid convergence of Newton–Raphson while preserving the guaranteed convergence of bisection.
The bisection method serves as a safeguard to guarantee the existence of a root within the chosen interval.
Once such an interval is established, the Newton–Raphson method is applied to accelerate convergence toward the root.

\subsubsection*{Implementation.} As always, in python
\begin{minted}{python}
def BNR(f,dfdx,xi,xf,dx,eps=10**(-6)):
    roots=[]
    loops=[]
    a=xi
    while a<xf:
        b=a+dx 
        if b>xf:
            break
        if f(a)*f(b)<=0:
            xr=(a+b)/2
            loop=0 
            while np.abs(f(xr))>eps and loop<100:
                xr=xr-f(xr)/dfdx(f,xr)
                loop+=1
            roots.append(float(xr))
            loops.append(loop)
        a=b
    if not roots:
        raise ValueError("No root found in interval")
    print( roots,loops)
\end{minted}

As a bonus, this function can be used to find the root of the Bessel function, which can be written as
\begin{minted}[breaklines]{python}
    def Jn(x, n, eps=10**(-6)):
    xo2 = x / 2.0
    x2 = xo2 * xo2
    a0 = 1.0

    for i in range(1, n+1):
        a0 = a0 * xo2 / i
    
    ss = a0
    error = a0
    k = 1
    
    while error > eps:
        a0 = -a0 * x2 / ((n + k) * k)
        ss = ss + a0
        error = abs(a0)
        k += 1
    
    return ss
\end{minted}
To explain the code, first recall the Bessel function
\begin{equation*}
    J_n(x) = \sum_{k=0}^{\infty} \frac{(-1)^k}{k! \, (n+k)!} \left( \frac{x}{2} \right)^{2k+n}
\end{equation*}
The $k$-th term then may be written
\begin{equation*}
    a_k = \frac{(-1)^k}{k! \, (n+k)!} \left( \frac{x}{2} \right)^{2k+n}
\end{equation*}
To avoid using factorial, we write the $k+1$-term in terms of $k$-th term
\begin{align*}
    a_{k+1} & = \frac{(-1)^{k+1}(x/2)^{2k+n+2}}{(k+1)!(n+k+1)!}          \\
            & = -\frac{(-1)^k(x/2)^2(x/2)^{2k+n }}{(k+1)k!(n+k+1)(n+k)!} \\
    a_{k+1} & = -a_k\frac{(x/2)^2}{(k+1)(n+k+1)}
\end{align*}
Using this, we initialize the first term as
\begin{equation*}
    a_0=\frac{1 }{n!}\left( \frac{x }{2 } \right) ^n
\end{equation*}
\end{document}